{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindquantum/simulator/__init__.py:17: UserWarning: Unable import mqvector gpu backend due to: cannot import name '_mq_vector_gpu' from partially initialized module 'mindquantum' (most likely due to a circular import) (/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindquantum/__init__.py)\n",
      "  from .available_simulator import SUPPORTED_SIMULATOR\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mindquantum.algorithm.nisq import HardwareEfficientAnsatz,RYFull\n",
    "from mindquantum.core.parameterresolver import  PRGenerator\n",
    "import numpy as np\n",
    "from mindquantum.core.gates import RX, RY, RZ, H, X, Y, Z, I,CNOT\n",
    "from mindquantum.core.circuit import Circuit,UN\n",
    "import mindspore as ms\n",
    "from mindquantum.core.parameterresolver import PRGenerator\n",
    "import random\n",
    "from mindspore import Tensor,ops\n",
    "import tensorcircuit as tc\n",
    "import tensorflow as tf\n",
    "import mindspore.numpy as mnp\n",
    "from DQAS_tool import wash_pr,Mindspore_ansatz_micro,vag_nnp_micro\n",
    "import sys\n",
    "from typing import Union\n",
    "sys.path.append('..')\n",
    "from Test_tool import Test_ansatz\n",
    "from data_processing import X_train,X_test,y_train,y_test\n",
    "from mindquantum.core.circuit import change_param_name,apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_pool = PRGenerator('pool')\n",
    "parameterized_circuit= \\\n",
    "[UN(RX(pr_pool.new()),maps_obj=[0])+I.on(1),\n",
    " UN(RX(pr_pool.new()),maps_obj=[1])+I.on(0),\n",
    " UN(RX(pr_pool.new()),maps_obj=[0],maps_ctrl=[1]),\n",
    " UN(RX(pr_pool.new()),maps_obj=[1],maps_ctrl=[0]),\n",
    " UN(RY(pr_pool.new()),maps_obj=[0])+I.on(1),\n",
    " UN(RY(pr_pool.new()),maps_obj=[1])+I.on(0),\n",
    " UN(RY(pr_pool.new()),maps_obj=[0],maps_ctrl=[1]),\n",
    " UN(RY(pr_pool.new()),maps_obj=[1],maps_ctrl=[0]),\n",
    " UN(RZ(pr_pool.new()),maps_obj=[0])+I.on(1),\n",
    " UN(RZ(pr_pool.new()),maps_obj=[1])+I.on(0),\n",
    " UN(RZ(pr_pool.new()),maps_obj=[0],maps_ctrl=[1]),\n",
    " UN(RZ(pr_pool.new()),maps_obj=[1],maps_ctrl=[0]),]\n",
    "\n",
    "\n",
    "unparameterized_circuit = \\\n",
    "[UN(X,maps_obj=[0],maps_ctrl=[1]),\n",
    " UN(X,maps_obj=[1],maps_ctrl=[0]),\n",
    " UN(Z,maps_obj=[0],maps_ctrl=[1]),\n",
    " UN(Z,maps_obj=[1],maps_ctrl=[0]),\n",
    " UN(Y,maps_obj=[0],maps_ctrl=[1]),\n",
    " UN(Y,maps_obj=[1],maps_ctrl=[0]),\n",
    " ]\n",
    "ansatz_pr = PRGenerator('ansatz')\n",
    "shape_parametized = len(parameterized_circuit)\n",
    "shape_unparameterized = len(unparameterized_circuit)\n",
    "num_layer=5\n",
    "shape_nnp = (7,num_layer,shape_parametized)\n",
    "shape_stp = (num_layer,shape_unparameterized+shape_parametized)\n",
    "stddev = 0.03\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp)\n",
    "ops_onehot = ops.OneHot(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mindspore_ansatz_micro(Structure_p:np.array,\n",
    "                     parameterized_pool:list[Circuit],\n",
    "                     unparameterized_pool:list[Circuit],\n",
    "                     num_layer:int=6,\n",
    "                     n_qbits:int=8):\n",
    "    \"\"\"\n",
    "    和 DQAS 文章描述的一致，生成权重线路\n",
    "    更新了非参数化门的算符池引入\n",
    "    Structure_p:np.array DQAS中的权重参数,\n",
    "    Ansatz_p:np.array  DQAS中的Ansatz参数,\n",
    "    \"\"\"\n",
    "    if Structure_p.shape[0] != num_layer:\n",
    "        raise ValueError('Structure_p shape must be equal to num_layer')\n",
    "    \n",
    "    if Structure_p.shape[1] != len(parameterized_pool)+len(unparameterized_pool):\n",
    "        raise ValueError('Structure_p shape must be equal to size of pool')\n",
    "\n",
    "    if isinstance(Structure_p, np.ndarray):\n",
    "        my_stp = ms.Tensor(Structure_p, ms.float32)\n",
    "    else:\n",
    "        my_stp = Structure_p\n",
    "        \n",
    "    prg = PRGenerator('encoder')\n",
    "    nqbits = n_qbits\n",
    "    encoder = Circuit()\n",
    "    # encoder += UN(H, nqbits)                                 \n",
    "    for i in range(nqbits):                                  \n",
    "        encoder += RY(prg.new()).on(i)    \n",
    "    encoder = encoder.as_encoder()             \n",
    "        \n",
    "    sub_ansatz = Circuit()\n",
    "    #print(my_stp.shape)\n",
    "    for layer_index in range(my_stp.shape[0]):\n",
    "        for op_index in range(my_stp.shape[1]):\n",
    "            if my_stp[layer_index,op_index] == 0:\n",
    "                continue\n",
    "            if op_index < len(parameterized_pool):\n",
    "                before_ansatz = parameterized_pool[op_index]\n",
    "                before_ansatz = change_param_name(circuit_fn=before_ansatz,name_map={before_ansatz.ansatz_params_name[0]:f'ansatz{layer_index}'})\n",
    "                sub_ansatz += before_ansatz\n",
    "            else:\n",
    "                sub_ansatz += unparameterized_pool[op_index-len(parameterized_pool)]\n",
    "    \n",
    "    whole_ansatz = Circuit()\n",
    "    whole_ansatz += wash_pr(apply(sub_ansatz,[0,1]),index=0)\n",
    "    whole_ansatz += wash_pr(apply(sub_ansatz,[2,3]),index=1)\n",
    "    whole_ansatz += wash_pr(apply(sub_ansatz,[4,5]),index=2)\n",
    "    whole_ansatz += wash_pr(apply(sub_ansatz,[6,7]),index=3)\n",
    "    whole_ansatz += wash_pr(apply(sub_ansatz,[0,2]),index=4)\n",
    "    whole_ansatz += wash_pr(apply(sub_ansatz,[4,6]),index=5)\n",
    "    whole_ansatz += wash_pr(apply(sub_ansatz,[0,4]),index=6)\n",
    "    whole_ansatz =  wash_pr(whole_ansatz,index=None)\n",
    "                \n",
    "    finnal_ansatz = encoder.as_encoder() + whole_ansatz.as_ansatz()\n",
    "    return finnal_ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_from_structure(structures: np.array)->Tensor:\n",
    "    prob = ops.Softmax()(ms.Tensor(structures))\n",
    "    return ops.Argmax(axis=-1)(prob)\n",
    "\n",
    "def best_from_structure(structures: np.array)->Tensor:\n",
    "    return ops.Argmax(axis=-1)(ms.Tensor(structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_onehot = ops.OneHot(axis=-1)\n",
    "stp_onehot = best_from_structure(stp)\n",
    "stp_onehot = ops_onehot(ms.Tensor(stp_onehot),shape_parametized+shape_unparameterized,ms.Tensor(1),ms.Tensor(0))\n",
    "ansatz = Mindspore_ansatz_micro(Structure_p=stp_onehot,parameterized_pool=parameterized_circuit,unparameterized_pool=unparameterized_circuit,num_layer=num_layer,n_qbits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_from_structure(stp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 5, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroslike_grad_nnp_micro(batch_sturcture: Union[np.ndarray, ms.Tensor], grad_nnp: Union[np.ndarray, ms.Tensor], shape_parametized: int, ansatz_parameters: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    用于根据算出的梯度更新ansatz参数    \n",
    "    '''\n",
    "    if isinstance(batch_sturcture, np.ndarray):\n",
    "        mystp = ms.Tensor(batch_sturcture, ms.float32)\n",
    "    else:\n",
    "        mystp = batch_sturcture  # 如果 batch_sturcture 已经是 ms.Tensor 类型\n",
    "\n",
    "    op_index = [ops.Argmax()(i) for i in mystp]\n",
    "    print(op_index)\n",
    "    zeros_grad_nnp = np.zeros_like(ansatz_parameters)\n",
    "    count = 0\n",
    "    for each_sub in range(7):\n",
    "        for index,i in enumerate(op_index):\n",
    "            if i >= shape_parametized:\n",
    "                continue\n",
    "            zeros_grad_nnp[each_sub, i] = grad_nnp[count]\n",
    "            count += 1\n",
    "        \n",
    "    return zeros_grad_nnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1\n",
      "0 1 0\n",
      "0 2 3\n",
      "0 3 2\n",
      "1 0 1\n",
      "1 1 0\n",
      "1 2 3\n",
      "1 3 2\n",
      "2 0 1\n",
      "2 1 0\n",
      "2 2 3\n",
      "2 3 2\n",
      "3 0 1\n",
      "3 1 0\n",
      "3 2 3\n",
      "3 3 2\n",
      "4 0 1\n",
      "4 1 0\n",
      "4 2 3\n",
      "4 3 2\n",
      "5 0 1\n",
      "5 1 0\n",
      "5 2 3\n",
      "5 3 2\n",
      "6 0 1\n",
      "6 1 0\n",
      "6 2 3\n",
      "6 3 2\n",
      "[Tensor(shape=[], dtype=Int32, value= 1), Tensor(shape=[], dtype=Int32, value= 0), Tensor(shape=[], dtype=Int32, value= 3), Tensor(shape=[], dtype=Int32, value= 2), Tensor(shape=[], dtype=Int32, value= 16)]\n",
      "0 0 8\n",
      "0 1 8\n",
      "0 2 4\n",
      "1 0 8\n",
      "1 1 8\n",
      "1 2 4\n",
      "2 0 8\n",
      "2 1 8\n",
      "2 2 4\n",
      "3 0 8\n",
      "3 1 8\n",
      "3 2 4\n",
      "4 0 8\n",
      "4 1 8\n",
      "4 2 4\n",
      "5 0 8\n",
      "5 1 8\n",
      "5 2 4\n",
      "6 0 8\n",
      "6 1 8\n",
      "6 2 4\n",
      "[Tensor(shape=[], dtype=Int32, value= 8), Tensor(shape=[], dtype=Int32, value= 8), Tensor(shape=[], dtype=Int32, value= 4), Tensor(shape=[], dtype=Int32, value= 12), Tensor(shape=[], dtype=Int32, value= 12)]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for axis 1 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_structure:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#print(ops.Argmax()(i))          \u001b[39;00m\n\u001b[1;32m     33\u001b[0m     infd, grad_nnp \u001b[38;5;241m=\u001b[39m vag_nnp_micro(Structure_params\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m     34\u001b[0m                                 Ansatz_params\u001b[38;5;241m=\u001b[39mnnp,\n\u001b[1;32m     35\u001b[0m                                 paramerterized_pool\u001b[38;5;241m=\u001b[39mparameterized_circuit,  unparamerterized_pool\u001b[38;5;241m=\u001b[39munparameterized_circuit,\n\u001b[1;32m     36\u001b[0m                                 num_layer\u001b[38;5;241m=\u001b[39mnum_layer,n_qbits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)(ms\u001b[38;5;241m.\u001b[39mTensor(X_train),ms\u001b[38;5;241m.\u001b[39mTensor(y_train))\n\u001b[0;32m---> 38\u001b[0m     grad_nnp_zeroslike \u001b[38;5;241m=\u001b[39m \u001b[43mzeroslike_grad_nnp_micro\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_sturcture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrad_nnp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_nnp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshape_parametized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_parametized\u001b[49m\u001b[43m,\u001b[49m\u001b[43mansatz_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnnp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     gs \u001b[38;5;241m=\u001b[39m nmf_gradient(structures\u001b[38;5;241m=\u001b[39mstp,oh\u001b[38;5;241m=\u001b[39mi,num_layer\u001b[38;5;241m=\u001b[39mnum_layer,size_pool\u001b[38;5;241m=\u001b[39mstp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#print(infd,grad_nnp)\u001b[39;00m\n",
      "File \u001b[0;32m~/mac_vscode/华为 QAS 实习/DQAS/DQAS_tool.py:641\u001b[0m, in \u001b[0;36mzeroslike_grad_nnp_micro\u001b[0;34m(batch_sturcture, grad_nnp, shape_parametized, ansatz_parameters)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m shape_parametized:\n\u001b[1;32m    640\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mzeros_grad_nnp\u001b[49m\u001b[43m[\u001b[49m\u001b[43meach_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m grad_nnp[count]\n\u001b[1;32m    642\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m zeros_grad_nnp\n",
      "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for axis 1 with size 5"
     ]
    }
   ],
   "source": [
    "from DQAS_tool import  sampling_from_structure,zeroslike_grad_nnp_micro,nmf_gradient,vag_nnp_micro,DQAS_accuracy\n",
    "#from DQAS_tool import  DQASAnsatz_from_result,DQAS_accuracy\n",
    "K = tc.set_backend(\"tensorflow\")\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.06, 100, 0.5)\n",
    "structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.1))\n",
    "network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))\n",
    "verbose = False\n",
    "# 设置超参数\n",
    "epochs = 50\n",
    "batch_size=256\n",
    "avcost1 = 0\n",
    "ops_onehot = ops.OneHot(axis=-1)\n",
    "batch_loss_history=[] # 记录每个epoch的batch_size损失值\n",
    "structure_distribution_history=[] # 记录每个epoch的结构参数\n",
    "ansatz_params_history=[] # 记录每个epoch的网络参数\n",
    "best_candidates_history=[] # 记录每个epoch的最佳候选\n",
    "acc_history = [] #记录每个epoch的准确率\n",
    "\n",
    " \n",
    "for epoch in range(epochs):  # 更新结构参数的迭代\n",
    "    avcost2 = avcost1\n",
    "    costl = []\n",
    "    tmp = np.stack([sampling_from_structure(stp,num_layer,shape_parametized) for _ in range(batch_size)])\n",
    "    batch_structure = ops_onehot(ms.Tensor(tmp),shape_parametized+shape_unparameterized,ms.Tensor(1),ms.Tensor(0))\n",
    "    #print(batch_structure.shape)\n",
    "    # print(tmp,batch_structure)\n",
    "    loss_value = []\n",
    "    grad_nnps = []\n",
    "    grad_stps = []\n",
    "    \n",
    "    for i in batch_structure:\n",
    "        #print(ops.Argmax()(i))          \n",
    "        infd, grad_nnp = vag_nnp_micro(Structure_params=i,\n",
    "                                    Ansatz_params=nnp,\n",
    "                                    paramerterized_pool=parameterized_circuit,  unparamerterized_pool=unparameterized_circuit,\n",
    "                                    num_layer=num_layer,n_qbits=8)(ms.Tensor(X_train),ms.Tensor(y_train))\n",
    "        \n",
    "        grad_nnp_zeroslike = zeroslike_grad_nnp_micro(batch_sturcture=i,grad_nnp=grad_nnp[0],shape_parametized=shape_parametized,ansatz_parameters=nnp)\n",
    "        gs = nmf_gradient(structures=stp,oh=i,num_layer=num_layer,size_pool=stp.shape[1])\n",
    "        #print(infd,grad_nnp)\n",
    "        loss_value.append(infd)\n",
    "        grad_nnps.append(ms.Tensor(grad_nnp_zeroslike,dtype=ms.float64))\n",
    "        grad_stps.append(gs)\n",
    "\n",
    "      \n",
    "    infd = ops.stack(loss_value)\n",
    "    gnnp = ops.addn(grad_nnps)\n",
    "    gstp = [(infd[i] - avcost2) * grad_stps[i] for i in range(infd.shape[0])]\n",
    "    gstp_averge = ops.addn(gstp) / infd.shape[0]\n",
    "    avcost1 = sum(infd) / infd.shape[0]\n",
    "    # print(f'loss={infd}\\ngrad_nnp={gnnp}\\ngrandient_stp={gstp_averge}')\n",
    "    \n",
    "    gnnp_tf = tf.convert_to_tensor(gnnp.asnumpy(),dtype=tf.float64)\n",
    "    nnp_tf = tf.convert_to_tensor(nnp,dtype=tf.float64)\n",
    "    gstp_averge_tf = tf.convert_to_tensor(gstp_averge.reshape(stp.shape).asnumpy(),dtype=tf.float64)\n",
    "    stp_tf = tf.convert_to_tensor(stp,dtype=tf.float64)\n",
    "     # 更新参数\n",
    "    nnp_tf = network_opt.update(gnnp_tf, nnp_tf)\n",
    "    stp_tf = structure_opt.update(gstp_averge_tf, stp_tf) \n",
    "    \n",
    "    nnp = nnp_tf.numpy()\n",
    "    stp = stp_tf.numpy()\n",
    "\n",
    "    batch_loss_history.append(avcost1)\n",
    "    structure_distribution_history.append(stp)\n",
    "    ansatz_params_history.append(nnp)\n",
    "    #best_candidates_history.append(best_from_structure(cand_preset.asnumpy()))\n",
    "    cand_preset = best_from_structure(stp)\n",
    "    best_candidates_history.append(cand_preset.asnumpy())\n",
    "    \n",
    "\n",
    "    if epoch % 1 == 0 or epoch == epochs - 1:\n",
    "        print(\"----------epoch %s-----------\" % epoch)\n",
    "        print(\n",
    "            \"batched平均损失: \",\n",
    "            avcost1,\n",
    "        )\n",
    "    \n",
    "        if verbose:\n",
    "            print(\n",
    "                \"strcuture parameter: \\n\",\n",
    "                stp,\n",
    "                \"\\n network parameter: \\n\",\n",
    "                nnp,\n",
    "            )\n",
    "        \n",
    "        print(\"最好的候选结构:\",cand_preset)\n",
    "        stp_for_test = ops_onehot(ms.Tensor(cand_preset),shape_parametized+shape_unparameterized,ms.Tensor(1),ms.Tensor(0))\n",
    "        test_ansatz = Mindspore_ansatz_micro(Structure_p=stp_for_test,\n",
    "                                            parameterized_pool=parameterized_circuit,unparameterized_pool=unparameterized_circuit,\n",
    "                                            num_layer=num_layer,\n",
    "                                            n_qbits=8)\n",
    "        \n",
    "        ansatz_parameters=[]\n",
    "        for layerIndex,i in enumerate(cand_preset):\n",
    "            if i >=len(parameterized_circuit):\n",
    "                continue\n",
    "            else:\n",
    "                ansatz_parameters.append(nnp[layerIndex,i])\n",
    "        \n",
    "        acc = DQAS_accuracy(ansatz=test_ansatz,Network_params=ansatz_parameters,n_qbits=8)\n",
    "        acc_history.append(acc)\n",
    "        print(f'二分类准确率 Acc ={acc*100}% ')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
