{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor, ops\n",
    "import mindspore as ms\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from data_processing import X_train,X_test,y_train,y_test\n",
    "from DQAS_tool import Mindspore_ansatz2,sampling_from_structure,nmf_gradient,vag_nnp2,zeroslike_grad_nnp,generate_pauli_string\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorcircuit as tc\n",
    "import tensorflow as tf\n",
    "from DQAS_tool import best_from_structure,DQAS_accuracy,extract_parameterss\n",
    "# #Operator Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unbound_opeartor_pool = \\\n",
    "[\"X0 X1 I2 I3 I4 I5 I6 I7\", \n",
    " \"I0 X1 X2 I3 I4 I5 I6 I7\",\n",
    " \"I0 I1 X2 X3 I4 I5 I6 I7\",\n",
    " \"I0 I1 I2 X3 X4 I5 I6 I7\",\n",
    " \"I0 I1 I2 I3 X4 X5 I6 I7\",\n",
    " \"I0 I1 I2 I3 I4 I4 X6 I7\",\n",
    " \"Y0 Y1 I2 I3 I4 I4 I6 I7\",\n",
    " \"I0 Y1 Y2 I3 I4 I4 I6 I7\",\n",
    " \"I0 I1 Y2 Y3 I4 I4 I6 I7\",\n",
    " \"I0 I1 I2 Y3 Y4 I4 I6 I7\",\n",
    " \"I0 I1 I2 I3 Y4 Y5 I6 I7\",\n",
    " \"I0 I1 I2 I3 I4 Y5 Y6 I7\",\n",
    " ]\n",
    "bound_opeartor_pool = \\\n",
    "['ZYIXXXZX',\n",
    " 'IZZYYXZI',\n",
    " 'XIIXYIIZ',\n",
    " 'IIIYYIYX',\n",
    " 'IZZYIXZI',\n",
    " 'ZZYYYYYY']\n",
    "\n",
    "num_layer = 12\n",
    "# 定义标准差和形状\n",
    "stddev = 0.02\n",
    "shape_parametized = len(unbound_opeartor_pool) #12\n",
    "shape_unparametized = len(bound_opeartor_pool) #6\n",
    "shape_nnp = (num_layer, shape_parametized)\n",
    "shape_stp = (num_layer, shape_parametized+shape_unparametized)\n",
    "\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp)\n",
    "\n",
    "np.random.seed(10)\n",
    "unbound_opeartor_pool = [generate_pauli_string(n=8,seed=i)[0] for i in range(shape_parametized)]\n",
    "bound_opeartor_pool = [generate_pauli_string(n=8,seed=i)[1] for i in range(shape_parametized,shape_parametized+shape_unparametized)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from DQAS_tool import  DQASAnsatz_from_result,DQAS_accuracy\n",
    "K = tc.set_backend(\"tensorflow\")\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.06, 100, 0.5)\n",
    "structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.05))\n",
    "network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))\n",
    "verbose = False\n",
    "# 设置超参数\n",
    "epochs = 30\n",
    "batch_size=50\n",
    "shape_nnp = (num_layer, shape_parametized)\n",
    "shape_stp = (num_layer, shape_parametized+shape_unparametized)\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp).astype(np.float64)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp).astype(np.float64)\n",
    "print(stp.shape)\n",
    "avcost1 = 0\n",
    "\n",
    "ops_onehot = ops.OneHot(axis=-1)\n",
    "\n",
    "batch_loss_history=[] # 记录每个epoch的batch_size损失值\n",
    "structure_distribution_history=[] # 记录每个epoch的结构参数\n",
    "ansatz_params_history=[] # 记录每个epoch的网络参数\n",
    "best_candidates_history=[] # 记录每个epoch的最佳候选\n",
    "acc_history = [] #记录每个epoch的准确率\n",
    "\n",
    "\n",
    "for epoch in range(1):  # 更新结构参数的迭代\n",
    "    avcost2 = avcost1\n",
    "    costl = []\n",
    "    tmp = np.stack([sampling_from_structure(stp,num_layer,shape_parametized) for _ in range(batch_size)])\n",
    "    batch_structure = ops_onehot(ms.Tensor(tmp),shape_parametized+shape_unparametized,ms.Tensor(1),ms.Tensor(0))\n",
    "    #print(batch_structure.shape)\n",
    "    # print(tmp,batch_structure)\n",
    "    loss_value = []\n",
    "    grad_nnps = []\n",
    "    grad_stps = []\n",
    "    \n",
    "    for i in batch_structure:\n",
    "        #print(ops.Argmax()(i))          \n",
    "        infd, grad_nnp = vag_nnp2(Structure_params=i,\n",
    "                                  Ansatz_params=nnp,\n",
    "                                  paramerterized_pool=unbound_opeartor_pool,unparamerterized_pool=bound_opeartor_pool,\n",
    "                                  num_layer=num_layer,n_qbits=8)(ms.Tensor(X_train),ms.Tensor(y_train))\n",
    "        \n",
    "        grad_nnp_zeroslike = zeroslike_grad_nnp(batch_sturcture=i,grad_nnp=grad_nnp[0],shape_parametized=shape_parametized,ansatz_parameters=nnp)\n",
    "        gs = nmf_gradient(structures=stp,oh=i,num_layer=num_layer,size_pool=stp.shape[1])\n",
    "        #print(infd,grad_nnp)\n",
    "        loss_value.append(infd)\n",
    "        grad_nnps.append(ms.Tensor(grad_nnp_zeroslike,dtype=ms.float64))\n",
    "        grad_stps.append(gs)\n",
    "      \n",
    "    infd = ops.stack(loss_value)\n",
    "    gnnp = ops.addn(grad_nnps)\n",
    "    gstp = [(infd[i] - avcost2) * grad_stps[i] for i in range(infd.shape[0])]\n",
    "    gstp_averge = ops.addn(gstp) / infd.shape[0]\n",
    "    avcost1 = sum(infd) / infd.shape[0]\n",
    "    \n",
    "    gnnp_tf = tf.convert_to_tensor(gnnp.asnumpy(),dtype=tf.float64)\n",
    "    nnp_tf = tf.convert_to_tensor(nnp,dtype=tf.float64)\n",
    "    gstp_averge_tf = tf.convert_to_tensor(gstp_averge.reshape(stp.shape).asnumpy(),dtype=tf.float64)\n",
    "    stp_tf = tf.convert_to_tensor(stp,dtype=tf.float64)\n",
    "     # 更新参数\n",
    "    nnp_tf = network_opt.update(gnnp_tf, nnp_tf)\n",
    "    stp_tf = structure_opt.update(gstp_averge_tf, stp_tf) \n",
    "    \n",
    "    nnp = nnp_tf.numpy()\n",
    "    stp = stp_tf.numpy()\n",
    "\n",
    "    batch_loss_history.append(avcost1)\n",
    "    structure_distribution_history.append(stp)\n",
    "    ansatz_params_history.append(nnp)\n",
    "    #best_candidates_history.append(best_from_structure(cand_preset.asnumpy()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if epoch % 2 == 0 or epoch == epochs - 1:\n",
    "        print(\"----------epoch %s-----------\" % epoch)\n",
    "        print(\n",
    "            \"batched平均损失: \",\n",
    "            avcost1,\n",
    "        )\n",
    "    \n",
    "        if verbose:\n",
    "            print(\n",
    "                \"strcuture parameter: \\n\",\n",
    "                stp,\n",
    "                \"\\n network parameter: \\n\",\n",
    "                nnp,\n",
    "            )\n",
    "        \n",
    "        cand_preset = best_from_structure(stp)\n",
    "        best_candidates_history.append(cand_preset)\n",
    "        print(\"最好的候选结构:\",cand_preset)\n",
    "        stp_for_test = ops_onehot(ms.Tensor(cand_preset),shape_parametized+shape_unparametized,ms.Tensor(1),ms.Tensor(0))\n",
    "        test_ansatz = Mindspore_ansatz2(Structure_p=stp_for_test,parameterized_pool=unbound_opeartor_pool,unparameterized_pool=bound_opeartor_pool,num_layer=num_layer,n_qbits=8)\n",
    "        \n",
    "        ansatz_parameters=[]\n",
    "        for layerIndex,i in enumerate(cand_preset):\n",
    "            if i >=len(unbound_opeartor_pool):\n",
    "                continue\n",
    "            else:\n",
    "                ansatz_parameters.append(nnp[layerIndex,i])\n",
    "        \n",
    "        acc = DQAS_accuracy(ansatz=test_ansatz,Network_params=ansatz_parameters,n_qbits=8)\n",
    "        acc_history.append(acc)\n",
    "        print(f'二分类准确率 Acc ={acc*100}% ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m network_opt \u001b[38;5;241m=\u001b[39m \u001b[43mtc\u001b[49m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39moptimizer(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m1e-3\u001b[39m))\n\u001b[1;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m      3\u001b[0m stp_for_test \u001b[38;5;241m=\u001b[39m ops_onehot(ms\u001b[38;5;241m.\u001b[39mTensor(cand_preset),shape_parametized\u001b[38;5;241m+\u001b[39mshape_unparametized,ms\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;241m1\u001b[39m),ms\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tc' is not defined"
     ]
    }
   ],
   "source": [
    "network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(1e-3))\n",
    "epochs = 200\n",
    "stp_for_test = ops_onehot(ms.Tensor(cand_preset),shape_parametized+shape_unparametized,ms.Tensor(1),ms.Tensor(0))\n",
    "for epoch in range(epochs):\n",
    "    infd, grad_nnp = vag_nnp2(Structure_params=stp_for_test,Ansatz_params=nnp,paramerterized_pool=unbound_opeartor_pool,\n",
    "                              unparamerterized_pool=bound_opeartor_pool,\n",
    "                              num_layer=num_layer,n_qbits=8)(ms.Tensor(X_train),ms.Tensor(y_train))\n",
    "    grad_nnp_zeroslike = zeroslike_grad_nnp(batch_sturcture=stp_for_test,grad_nnp=grad_nnp[0],shape_parametized=shape_parametized,ansatz_parameters=nnp)\n",
    "    \n",
    "    nnp_tf  = tf.convert_to_tensor(nnp,dtype=tf.float64)\n",
    "    gnnp_tf = tf.convert_to_tensor(grad_nnp_zeroslike,dtype=tf.float64)\n",
    "    infd_tf = tf.convert_to_tensor(infd.numpy(),dtype=tf.float64)\n",
    "    nnp_tf = network_opt.update(gnnp_tf, nnp_tf)\n",
    "    nnp = nnp_tf.numpy()\n",
    "    if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "        print(f'epoch:{epoch},loss:{infd},grad:{grad_nnp[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ansatz(stp:np.array):\n",
    "    stp_for_test = ops_onehot(ms.Tensor(stp),shape_parametized+shape_unparametized,ms.Tensor(1),ms.Tensor(0))\n",
    "    test_ansatz = Mindspore_ansatz2(Structure_p=stp_for_test,parameterized_pool=unbound_opeartor_pool,unparameterized_pool=bound_opeartor_pool,num_layer=num_layer,n_qbits=8)\n",
    "    return test_ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
