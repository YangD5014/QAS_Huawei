{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Quantum/lib/python3.9/site-packages/mindquantum/simulator/__init__.py:17: UserWarning: Unable import mqvector gpu backend due to: cannot import name '_mq_vector_gpu' from partially initialized module 'mindquantum' (most likely due to a circular import) (/opt/miniconda3/envs/Quantum/lib/python3.9/site-packages/mindquantum/__init__.py)\n",
      "  from .available_simulator import SUPPORTED_SIMULATOR\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mindquantum.core.gates import RX, RY, RZ, H, X, Y, Z, CNOT\n",
    "from mindquantum.core.circuit import Circuit\n",
    "import mindspore as ms\n",
    "from mindquantum.simulator import  Simulator\n",
    "from mindquantum.core.gates import GroupedPauli\n",
    "from mindquantum.core.operators import TimeEvolution,QubitOperator\n",
    "from mindquantum.core.parameterresolver import PRConvertible,PRGenerator,ParameterResolver\n",
    "from DQAS_tool import generate_pauli_string,one_hot\n",
    "from mindquantum.core.gates import RotPauliString\n",
    "from mindquantum.core.gates import UnivMathGate\n",
    "from mindspore import Tensor, ops\n",
    "from mindquantum.core.circuit import UN\n",
    "from mindquantum.core.operators import Hamiltonian             # 导入Hamiltonian模块，用于构建哈密顿量\n",
    "from mindquantum.framework import MQLayer\n",
    "from mindspore.nn import  TrainOneStepCell\n",
    "from mindspore.nn import SoftmaxCrossEntropyWithLogits                         # 导入SoftmaxCrossEntropyWithLogits模块，用于定义损失函数\n",
    "from mindspore.nn import Adam                                                  # 导入Adam模块用于定义优化参数\n",
    "from mindspore.train import Accuracy, Model, LossMonitor                       # 导入Accuracy模块，用于评估预测准确率\n",
    "import mindspore as ms\n",
    "from mindspore import Parameter, Tensor\n",
    "from mindspore.dataset import NumpySlicesDataset\n",
    "from torch.utils.data import DataLoader# 导入NumpySlicesDataset模块，用于创建模型可以识别的数据集\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from data_processing import X_train,X_test,y_train,y_test\n",
    "from mindquantum.algorithm.nisq import HardwareEfficientAnsatz   \n",
    "\n",
    "from DQAS_tool import Mindspore_ansatz,loss_fn,vag_nnp,sampling_from_structure,vag_nnp,sampling_from_structure\n",
    "from mindquantum.framework import MQOps\n",
    "import mindspore.nn as nn\n",
    "import numpy as np\n",
    "import tensorcircuit as tc\n",
    "import tensorflow as tf\n",
    "  \n",
    "num_layer = 6\n",
    "# 定义标准差和形状\n",
    "stddev = 0.02\n",
    "shape_parametized = 12\n",
    "shape_unparametized = 4\n",
    "shape_nnp = (num_layer, shape_parametized)\n",
    "shape_stp = (num_layer, shape_parametized+shape_unparametized)\n",
    "\n",
    "shape_stp = (num_layer, shape_parametized)\n",
    "\n",
    "rtype = np.float64\n",
    "ctype = np.complex128\n",
    "# 使用 numpy 生成随机数矩阵\n",
    "np.random.seed(10)\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp).astype(rtype)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp).astype(rtype)\n",
    "# #Operator Pool\n",
    "unbound_opeartor_pool = [generate_pauli_string(n=8,seed=i)[0] for i in range(shape_parametized)]\n",
    "bound_opeartor_pool = [generate_pauli_string(n=8,seed=i)[1] for i in range(shape_parametized,shape_parametized+shape_unparametized)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.numpy as mnp\n",
    "def nmf_gradient(structures:np.array, oh:ms.Tensor,num_layer: int,size_pool:int):\n",
    "    \"\"\"\n",
    "    使用 MindSpore 实现蒙特卡洛梯度计算。\n",
    "    \"\"\"\n",
    "      # Step 1: 获取选择的索引\n",
    "    choice = ops.Argmax(axis=-1)(oh)\n",
    "    # Step 2: 计算概率\n",
    "    softmax = ops.Softmax(axis=-1)\n",
    "    prob = softmax(ms.Tensor(structures))\n",
    "    # Step 3: 获取概率矩阵中的值\n",
    "    indices = mnp.stack((mnp.arange(num_layer, dtype=ms.int64), choice), axis=1)\n",
    "    prob = ops.GatherNd()(prob, indices)\n",
    "    # Step 4: 变换概率矩阵\n",
    "    prob = prob.reshape(-1, 1)\n",
    "    prob = ops.Tile()(prob, (1, size_pool))\n",
    "    \n",
    "    # Step 5: 生成蒙特卡洛梯度\n",
    "    gradient = ops.TensorScatterAdd()(Tensor(-prob, ms.float64), indices, mnp.ones((num_layer,), dtype=ms.float64))\n",
    "    return gradient\n",
    "    \n",
    "    \n",
    "# 对向量化版本的封装\n",
    "# nmf_gradient_vmap = ops.vmap(nmf_gradient, in_axes=(None, 0, None, None))\n",
    "\n",
    "def best_from_structure(structures: np.array)->Tensor:\n",
    "    return ops.Argmax(axis=-1)(ms.Tensor(structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I0 I1 X2 Z3 I4 I5 Z6 I7',\n",
       " 'Y0 X1 Z2 X3 I4 I5 I6 I7',\n",
       " 'X0 X1 X2 Z3 Y4 Z5 Z6 Y7',\n",
       " 'Y0 Y1 Z2 I3 X4 X5 I6 Z7',\n",
       " 'Y0 Z1 X2 I3 I4 Y5 X6 X7',\n",
       " 'Z0 Z1 X2 I3 Y4 X5 Y6 X7',\n",
       " 'X0 I1 Z2 X3 X4 Y5 I6 Z7',\n",
       " 'Z0 Y1 I2 X3 X4 X5 Z6 X7',\n",
       " 'Y0 Z1 I2 Y3 Y4 X5 X6 Y7',\n",
       " 'I0 Z1 Z2 Y3 Y4 X5 Z6 I7',\n",
       " 'X0 I1 I2 X3 Y4 I5 I6 Z7',\n",
       " 'I0 I1 I2 Y3 Y4 I5 Y6 X7']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unbound_opeartor_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------epoch 0-----------\n",
      "batched average loss:  0.9081669\n",
      "strcuture parameter: \n",
      " [[-0.08094351 -0.02309322 -0.03992556  0.07383119  0.03872203  0.06704079\n",
      "   0.06375024 -0.08065072  0.05297612 -0.04368712 -0.0614877   0.04279221]\n",
      " [-0.04127123 -0.05074764  0.02989785 -0.03816022  0.05455326  0.05492952\n",
      "  -0.04807402 -0.04100885 -0.05570932  0.03269075 -0.06483143  0.07210842]\n",
      " [ 0.00963775 -0.03918414 -0.07884093 -0.08217361 -0.07012632  0.04479273\n",
      "   0.06460228  0.0160241   0.08347364  0.0732666   0.04729192  0.04419071]\n",
      " [-0.0690599  -0.03823659  0.05136822  0.07823693  0.03626788  0.06095579\n",
      "  -0.05072425  0.03305532 -0.01194317  0.05553886  0.06235179 -0.07136628]\n",
      " [-0.06444164  0.05167499  0.03825831 -0.03795615  0.05875951  0.03434878\n",
      "  -0.04613051 -0.0499161   0.0467107  -0.07297284  0.03328203  0.05420593]\n",
      " [ 0.07022976  0.0305463  -0.02564548  0.05360301  0.02459927  0.05646472\n",
      "   0.06766559  0.05528356 -0.00360657 -0.04382154 -0.03213783 -0.04977694]] \n",
      " network parameter: \n",
      " [[ 0.00390302 -0.02770985 -0.08244246 -0.06773999  0.06663828 -0.01390338\n",
      "   0.07018427  0.06827384 -0.03299341 -0.00172483 -0.06469917  0.0596936 ]\n",
      " [-0.01431354 -0.04906472 -0.04111832 -0.11957425  0.08435369 -0.03539656\n",
      "   0.0691904   0.07323533 -0.10586012 -0.03203239 -0.06661979  0.03854511]\n",
      " [-0.00489362 -0.07029629 -0.09420304 -0.04772425  0.08199487 -0.02058139\n",
      "   0.04571     0.0547692  -0.04893072  0.00435822 -0.03981432  0.05127775]\n",
      " [-0.04395074 -0.07357711 -0.04929401 -0.04508054  0.10449262 -0.01481689\n",
      "   0.06486605  0.0564401  -0.06805366  0.00525636 -0.05288593  0.05393671]\n",
      " [-0.04041523 -0.06245933 -0.06779752 -0.03482744  0.07893394 -0.03316085\n",
      "   0.0833183   0.04854807 -0.05723973  0.01598732 -0.08045504  0.06203188]\n",
      " [-0.02974681 -0.09882401 -0.04023933 -0.05940136  0.04354336 -0.03238776\n",
      "   0.05193895  0.08683008 -0.05186031 -0.01157206 -0.07340105  0.00211624]]\n",
      "best candidates so far: [ 3 11  8  3  4  0]\n",
      "----------epoch 2-----------\n",
      "batched average loss:  0.8955087\n",
      "strcuture parameter: \n",
      " [[-0.14059028 -0.08217909 -0.09948567  0.13046644  0.0983645   0.13153175\n",
      "   0.12336136 -0.14002182  0.11256365 -0.10392399 -0.12110358  0.10215794]\n",
      " [-0.10133385 -0.11136035  0.08983977 -0.09795298  0.11432602  0.11410256\n",
      "  -0.10967336 -0.10073092 -0.11471069  0.0905929  -0.12268219  0.1314597 ]\n",
      " [ 0.06856167 -0.09798629 -0.13847111 -0.14125996 -0.12935028  0.10425617\n",
      "   0.12360052  0.07511056  0.14288987  0.13236514  0.10651427  0.10353443]\n",
      " [-0.12776641 -0.09768466  0.11139885  0.13760076  0.09463137  0.12090624\n",
      "  -0.11046521  0.09183126 -0.07135828  0.12010063  0.12226645 -0.13101854]\n",
      " [-0.12476301  0.11171928  0.09690105 -0.09783572  0.11786689  0.09354256\n",
      "  -0.1071357  -0.1090746   0.10644292 -0.13277857  0.09229575  0.11472648]\n",
      " [ 0.12311703  0.09709826 -0.08382353  0.11335465  0.08432112  0.11540027\n",
      "   0.12458099  0.11470002 -0.06140217 -0.10332134 -0.09319246 -0.10910537]] \n",
      " network parameter: \n",
      " [[-8.60487556e-02 -1.40949921e-01 -1.79560712e-01 -1.63403783e-01\n",
      "   1.71418950e-01 -2.05314439e-02  1.68339889e-01  1.72584443e-01\n",
      "  -1.28458003e-01 -9.35342855e-02 -1.61174161e-01  1.58574404e-01]\n",
      " [-1.04844261e-01 -1.63106669e-01 -1.37010545e-01 -2.17441151e-01\n",
      "   1.87832281e-01 -2.46197093e-02  1.67858101e-01  1.78028529e-01\n",
      "  -2.03406592e-01 -1.24069339e-01 -1.63110627e-01  1.34841417e-01]\n",
      " [-8.18130476e-02 -1.83639507e-01 -1.92000666e-01 -1.43909162e-01\n",
      "   1.87795383e-01 -8.30398890e-03  1.43312241e-01  1.58532081e-01\n",
      "  -1.44589637e-01 -6.11327228e-02 -1.35540971e-01  1.44156387e-01]\n",
      " [-4.74131246e-02 -1.87996432e-01 -1.45586847e-01 -1.40843888e-01\n",
      "   2.10145473e-01  8.99918349e-04  1.63840859e-01  1.60405131e-01\n",
      "  -1.64349714e-01 -4.98327609e-02 -1.49537721e-01  1.34851693e-01]\n",
      " [ 2.68533115e-02 -1.74829800e-01 -1.64622452e-01 -1.30308167e-01\n",
      "   1.83920388e-01 -7.47569993e-03  1.82717178e-01  1.52488379e-01\n",
      "  -1.52942865e-01 -6.12899227e-02 -1.76657824e-01  1.21717154e-01]\n",
      " [ 5.01411978e-02 -2.11544858e-01 -1.36474783e-01 -1.55564264e-01\n",
      "   1.47448462e-01 -9.42392995e-05  1.50052612e-01  1.90378907e-01\n",
      "  -1.47605587e-01 -1.00603501e-01 -1.70168532e-01  2.11624166e-03]]\n",
      "best candidates so far: [ 5 11  8  3  4  6]\n",
      "----------epoch 4-----------\n",
      "batched average loss:  0.8668309\n",
      "strcuture parameter: \n",
      " [[-0.18027516 -0.1221717  -0.13702753  0.15555065  0.13793922  0.18523784\n",
      "   0.16666207 -0.17791856  0.15232173 -0.14582749 -0.16361792  0.13971966]\n",
      " [-0.14196045 -0.15296807  0.13029045 -0.13343048  0.15501743  0.14798905\n",
      "  -0.1522914  -0.14082892 -0.15696131  0.11857002 -0.16003761  0.17066771]\n",
      " [ 0.10588129 -0.13767511 -0.17844853 -0.17974412 -0.17008617  0.13908176\n",
      "   0.16142071  0.11540112  0.18301512  0.1751912   0.13120438  0.14360221]\n",
      " [-0.16684392 -0.13630141  0.15193412  0.17526382  0.13234409  0.16038821\n",
      "  -0.15010517  0.13137849 -0.11139395  0.15979256  0.17720465 -0.17152156]\n",
      " [-0.16576496  0.15235187  0.15995054 -0.13675273  0.1531105   0.1313982\n",
      "  -0.15021754 -0.14779203  0.14607907 -0.17332255  0.13189378  0.15550049]\n",
      " [ 0.15210219  0.1335585  -0.1185929   0.16557332  0.12285574  0.15404453\n",
      "   0.15789612  0.15401468 -0.10056972 -0.14234649 -0.13099474 -0.14867005]] \n",
      " network parameter: \n",
      " [[-0.1799834  -0.25466323 -0.28557272 -0.26964883  0.28119039 -0.08948938\n",
      "   0.27393458  0.28108099 -0.23449578 -0.19728272 -0.26821303  0.26165281]\n",
      " [-0.20366262 -0.27655011 -0.24327686 -0.32269497  0.29688236 -0.02940408\n",
      "   0.27363213  0.28675516 -0.31133541 -0.07861893 -0.27072429  0.1604493 ]\n",
      " [-0.13823044 -0.29771656 -0.29864902 -0.25022529  0.29888315 -0.0066471\n",
      "   0.2499311   0.26637746 -0.25076215  0.0080052  -0.2423128   0.10417926]\n",
      " [-0.03995768 -0.30114374 -0.25186849 -0.2476524   0.31918994  0.00746288\n",
      "   0.26984755  0.26652762 -0.27091281  0.02046762 -0.25560628  0.07266488]\n",
      " [-0.0266046  -0.28805026 -0.2694054  -0.23461857  0.29444559  0.01937975\n",
      "   0.28918795  0.26112775 -0.25958183 -0.00103177 -0.28265328  0.05161912]\n",
      " [-0.01342725 -0.32524202 -0.2426058  -0.26126918  0.25757922  0.04403472\n",
      "   0.25640308  0.29833621 -0.25493421 -0.20230327 -0.27539865  0.00211624]]\n",
      "best candidates so far: [ 5 11  8 10  2  3]\n",
      "----------epoch 6-----------\n",
      "batched average loss:  0.8277688\n",
      "strcuture parameter: \n",
      " [[-0.2103922  -0.15569666 -0.16016557  0.18892524  0.16570459  0.21252836\n",
      "   0.20222076 -0.20552877  0.18312241 -0.18111378 -0.19507708  0.16557523]\n",
      " [-0.17090116 -0.18251658  0.16158864 -0.15832622  0.18625217  0.17106026\n",
      "  -0.18363461 -0.17448708 -0.19278567  0.14130012 -0.18956327  0.19951226]\n",
      " [ 0.13408827 -0.16854961 -0.2079823  -0.20786703 -0.20104786  0.16875418\n",
      "   0.18926554  0.14691837  0.21200651  0.20868989  0.13295589  0.1734703 ]\n",
      " [-0.19551474 -0.16440822  0.18594408  0.2028849   0.15829958  0.18851837\n",
      "  -0.18026008  0.16028936 -0.14307568  0.17751001  0.24526316 -0.20234663]\n",
      " [-0.19495528  0.18923382  0.2275684  -0.16535797  0.17367959  0.16049445\n",
      "  -0.18402145 -0.17674159  0.17402155 -0.20436359  0.15950964  0.18463537]\n",
      " [ 0.19932387  0.13622099 -0.14604966  0.22216646  0.15145992  0.18255739\n",
      "   0.19264144  0.18362115 -0.1289224  -0.17398056 -0.16110405 -0.17890394]] \n",
      " network parameter: \n",
      " [[-0.27843603 -0.36961063 -0.39580144 -0.37960406  0.39382785 -0.14990916\n",
      "   0.38447607  0.39302511 -0.34474127 -0.30134795 -0.37883627  0.33516588]\n",
      " [-0.29744991 -0.39064819 -0.35377788 -0.43329524  0.40960198 -0.00299318\n",
      "   0.3839259   0.3991164  -0.42309116 -0.09011977 -0.38272597  0.09267476]\n",
      " [-0.09086564 -0.41282202 -0.40915729 -0.36120539  0.41212817  0.00630498\n",
      "   0.3608243   0.37801933 -0.36177631  0.01538355 -0.35360786  0.01933463]\n",
      " [ 0.01600115 -0.41536674 -0.36219201 -0.35821152  0.43193214  0.00445478\n",
      "   0.38064691  0.37772148 -0.38176907  0.01698782 -0.36593468 -0.01852923]\n",
      " [-0.0012901  -0.40187778 -0.37926267 -0.34440521  0.4078683  -0.00748871\n",
      "   0.39984974  0.37318153 -0.37065047 -0.01847694 -0.39360214 -0.04026159]\n",
      " [ 0.00290283 -0.44010861 -0.35350611 -0.37150992  0.37048774 -0.01276487\n",
      "   0.3667972   0.41021809 -0.36617949 -0.30391061 -0.38612879  0.00211624]]\n",
      "best candidates so far: [ 5 11  8 10  2  3]\n",
      "----------epoch 8-----------\n",
      "batched average loss:  0.7831126\n",
      "strcuture parameter: \n",
      " [[-0.23408455 -0.18216849 -0.1781417   0.204133    0.18983421  0.22317905\n",
      "   0.2381171  -0.23136379  0.20690631 -0.21708252 -0.22173252  0.18616881]\n",
      " [-0.19379477 -0.20100076  0.18840821 -0.17595469  0.21061045  0.18862735\n",
      "  -0.211248   -0.20520471 -0.21822412  0.15712087 -0.21437977  0.22192725]\n",
      " [ 0.15717693 -0.19528962 -0.23224771 -0.23070395 -0.22407983  0.19634917\n",
      "   0.20979453  0.16925665  0.23951489  0.23152505  0.14541755  0.19809102]\n",
      " [-0.21981363 -0.18582071  0.21713912  0.22635429  0.17693662  0.20832639\n",
      "  -0.20386207  0.18455739 -0.16817506  0.1711358   0.30619806 -0.22898532]\n",
      " [-0.21841212  0.21733247  0.30035393 -0.18807747  0.18909421  0.18114186\n",
      "  -0.21008426 -0.19986952  0.19589001 -0.22993004  0.17834541  0.2070315 ]\n",
      " [ 0.2642623   0.15053245 -0.1729961   0.25570193  0.17560693  0.20484208\n",
      "   0.19883448  0.20849509 -0.15364469 -0.20261955 -0.18496696 -0.20337693]] \n",
      " network parameter: \n",
      " [[-0.38009349 -0.48547945 -0.50869747 -0.49218343  0.50783641 -0.16188773\n",
      "   0.4974904   0.5070288  -0.45738473 -0.41057416 -0.49173895  0.35918992]\n",
      " [-0.39089433 -0.50591475 -0.46694567 -0.54651878  0.52278969  0.02691342\n",
      "   0.49676729  0.51344855 -0.53676296 -0.07165935 -0.49636971  0.01968992]\n",
      " [-0.12339398 -0.52858731 -0.52221592 -0.47441035  0.52655359  0.01009892\n",
      "   0.47409643  0.49181166 -0.4752809   0.02421604 -0.46697582 -0.05683779]\n",
      " [-0.01343395 -0.53077731 -0.47518286 -0.471379    0.54595231 -0.00313908\n",
      "   0.49367663  0.49143601 -0.49482693  0.01646528 -0.47843989 -0.08931021]\n",
      " [-0.04586717 -0.51680192 -0.49181521 -0.4570359   0.5225613  -0.02348005\n",
      "   0.51287067  0.48717634 -0.48400673 -0.01512426 -0.5067059  -0.09037251]\n",
      " [-0.03606461 -0.55596396 -0.46689997 -0.48416106  0.48494968 -0.04658639\n",
      "   0.47967574  0.52416585 -0.47940046 -0.40057991 -0.49878848  0.00211624]]\n",
      "best candidates so far: [ 6 11  8 10  2  0]\n",
      "----------epoch 10-----------\n",
      "batched average loss:  0.7409357\n",
      "strcuture parameter: \n",
      " [[-0.2546236  -0.20426962 -0.1947196   0.21284249  0.20915579  0.22694858\n",
      "   0.27476186 -0.25119473  0.22580493 -0.25378558 -0.24753557  0.2045086 ]\n",
      " [-0.21259575 -0.21521314  0.21832445 -0.18299744  0.22937716  0.20914806\n",
      "  -0.23334432 -0.23471561 -0.23951001  0.13843743 -0.23550799  0.2389644 ]\n",
      " [ 0.17486809 -0.21531975 -0.25143419 -0.24967413 -0.24133023  0.21770992\n",
      "   0.22667516  0.1875081   0.26516806  0.2482074   0.15257483  0.21571166]\n",
      " [-0.24531745 -0.20185228  0.24704552  0.24638695  0.18903481  0.22438905\n",
      "  -0.22404615  0.20482609 -0.19589845  0.15126731  0.37040491 -0.25207059]\n",
      " [-0.24229558  0.24170379  0.37084703 -0.20737179  0.2038002   0.19682783\n",
      "  -0.23727796 -0.21691785  0.21192666 -0.25262522  0.19354175  0.22559929]\n",
      " [ 0.31881468  0.18151633 -0.19673268  0.26953496  0.19787695  0.22136209\n",
      "   0.20143647  0.22976991 -0.17537229 -0.22582122 -0.20866902 -0.22404092]] \n",
      " network parameter: \n",
      " [[-0.48664921 -0.60024474 -0.62167743 -0.60490495  0.61882663 -0.15857998\n",
      "   0.61063036  0.62100031 -0.56886911 -0.52214068 -0.60239205  0.38399989]\n",
      " [-0.48998826 -0.62045799 -0.58138614 -0.65981221  0.63256046  0.01299397\n",
      "   0.60940561  0.62758715 -0.64937112 -0.05023168 -0.60788141  0.00371184]\n",
      " [-0.19225469 -0.64390428 -0.63612489 -0.58717991  0.63817901 -0.00381789\n",
      "   0.58681402  0.60578189 -0.58760782  0.03768981 -0.57841141 -0.06689168]\n",
      " [-0.07331152 -0.64571034 -0.58900127 -0.58464656  0.65654932 -0.0059126\n",
      "   0.60574975  0.60514891 -0.60562772  0.04387972 -0.58876096 -0.07337115]\n",
      " [-0.09790104 -0.63139071 -0.60506467 -0.56907784  0.63535943 -0.01276238\n",
      "   0.62486248  0.60114703 -0.59595289  0.03101116 -0.61781241 -0.05287196]\n",
      " [-0.05360344 -0.6713876  -0.58075091 -0.59628379  0.59713375 -0.04248515\n",
      "   0.59254367  0.63808895 -0.59143002 -0.48031349 -0.60940725  0.00211624]]\n",
      "best candidates so far: [ 6 11  8 10  2  0]\n",
      "----------epoch 12-----------\n",
      "batched average loss:  0.71617514\n",
      "strcuture parameter: \n",
      " [[-0.27152405 -0.22410246 -0.20944512  0.24716891  0.227241    0.2167475\n",
      "   0.30554181 -0.27048584  0.24206877 -0.2892432  -0.27494666  0.21923861]\n",
      " [-0.22778862 -0.2272432   0.24744695 -0.18466767  0.24488985  0.20617434\n",
      "  -0.25030472 -0.25905769 -0.25619599  0.10987485 -0.25332472  0.25201732]\n",
      " [ 0.18738308 -0.2333108  -0.26673333 -0.26472144 -0.25675108  0.22357646\n",
      "   0.2409771   0.20459122  0.29125034  0.26098158  0.14885822  0.22969749]\n",
      " [-0.26876018 -0.21420192  0.27548424  0.26341027  0.19855752  0.23693077\n",
      "  -0.24047883  0.22197585 -0.22087919  0.12571993  0.4121844  -0.27163256]\n",
      " [-0.26800711  0.26127899  0.44387677 -0.22234578  0.21792256  0.21004711\n",
      "  -0.26007062 -0.23169804  0.22472205 -0.27284447  0.20578918  0.23944288]\n",
      " [ 0.3574656   0.21664382 -0.21486714  0.30177405  0.21522174  0.23335653\n",
      "   0.2092116   0.24907371 -0.19601522 -0.24597838 -0.22938907 -0.24195874]] \n",
      " network parameter: \n",
      " [[-5.97225175e-01 -7.10553493e-01 -7.31453733e-01 -7.16025730e-01\n",
      "   7.21013903e-01 -1.81400732e-01  7.18480736e-01  7.31387715e-01\n",
      "  -6.75281676e-01 -6.35099855e-01 -7.05354643e-01  4.19061479e-01]\n",
      " [-5.95625885e-01 -7.30704704e-01 -6.92528165e-01 -7.70334766e-01\n",
      "   7.32868153e-01 -5.76986118e-04  7.16848072e-01  7.38691028e-01\n",
      "  -7.56491682e-01 -8.17122472e-02 -7.12164658e-01  1.80190065e-02]\n",
      " [-2.37676987e-01 -7.55116179e-01 -7.47449032e-01 -6.97252072e-01\n",
      "   7.39938516e-01 -1.80199373e-02  6.94233352e-01  7.17051608e-01\n",
      "  -6.94502603e-01  3.59527591e-02 -6.81978138e-01 -4.58894279e-02]\n",
      " [-8.77331091e-02 -7.57286691e-01 -6.99834896e-01 -6.95240253e-01\n",
      "   7.57722338e-01 -1.14299710e-02  7.12448009e-01  7.15473123e-01\n",
      "  -7.10347226e-01  6.67700965e-02 -6.89872252e-01 -3.33512788e-02]\n",
      " [-8.86673097e-02 -7.42116287e-01 -7.15643271e-01 -6.78037451e-01\n",
      "   7.40097815e-01 -9.24973631e-03  7.31702766e-01  7.11811205e-01\n",
      "  -7.02430214e-01  6.46654914e-02 -7.21753559e-01 -1.69624705e-03]\n",
      " [-1.90838678e-02 -7.83506205e-01 -6.91613048e-01 -7.05863533e-01\n",
      "   7.00058560e-01 -4.55850112e-02  7.00595362e-01  7.49149733e-01\n",
      "  -6.98058947e-01 -5.75708729e-01 -7.12687245e-01  2.11624166e-03]]\n",
      "best candidates so far: [ 6 11  8 10  2  0]\n",
      "----------epoch 14-----------\n",
      "batched average loss:  0.69583595\n",
      "strcuture parameter: \n",
      " [[-0.28592594 -0.23576153 -0.22330349  0.28866794  0.24129796  0.20890651\n",
      "   0.3302694  -0.2890557   0.25558444 -0.32074141 -0.2972649   0.23115933]\n",
      " [-0.24066598 -0.234113    0.27419172 -0.18421977  0.25680977  0.20123737\n",
      "  -0.26388572 -0.28042248 -0.26903022  0.08694591 -0.2702706   0.26270309]\n",
      " [ 0.194857   -0.24618314 -0.27840502 -0.27719196 -0.27050506  0.23205367\n",
      "   0.2520583   0.22039469  0.31134289  0.26985763  0.15767896  0.24144456]\n",
      " [-0.28965006 -0.22351311  0.30069268  0.28027534  0.20546055  0.24670244\n",
      "  -0.25513089  0.23582289 -0.24469201  0.09760017  0.44421206 -0.28887228]\n",
      " [-0.29432064  0.28365374  0.50257654 -0.2343158   0.22890539  0.22052948\n",
      "  -0.28001146 -0.24471399  0.23585278 -0.29004907  0.21640115  0.25003869]\n",
      " [ 0.38494717  0.25325205 -0.22875797  0.3320763   0.23006996  0.24301991\n",
      "   0.21624099  0.26620473 -0.21687806 -0.26330367 -0.24982924 -0.25736283]] \n",
      " network parameter: \n",
      " [[-0.71095009 -0.81508239 -0.83441441 -0.82033727  0.80698954 -0.1880319\n",
      "   0.81676581  0.8334009  -0.77182657 -0.74881689 -0.79761681  0.41581943]\n",
      " [-0.70593948 -0.83523027 -0.79723975 -0.87395428  0.81672236  0.02517045\n",
      "   0.81488061  0.84131737 -0.85342662 -0.09264383 -0.80409859  0.00253926]\n",
      " [-0.31201092 -0.86173715 -0.8527457  -0.80065281  0.82489267  0.00376837\n",
      "   0.79176328  0.82022733 -0.78940489  0.07781113 -0.77362516 -0.04839522]\n",
      " [-0.13255384 -0.86353139 -0.80445132 -0.79999737  0.84258611 -0.00653849\n",
      "   0.80857919  0.81722285 -0.80385497  0.12719796 -0.77827143 -0.01663754]\n",
      " [-0.12129573 -0.84842795 -0.81908623 -0.78053914  0.82834734 -0.0216648\n",
      "   0.82880248  0.81435753 -0.79874646  0.1277831  -0.81395856  0.02485381]\n",
      " [-0.03502205 -0.89028957 -0.79613596 -0.80772486  0.78746023 -0.08132923\n",
      "   0.8001499   0.852669   -0.79463237 -0.68034462 -0.80481701  0.00211624]]\n",
      "best candidates so far: [ 6  2  8 10  2  0]\n",
      "----------epoch 16-----------\n",
      "batched average loss:  0.6871678\n",
      "strcuture parameter: \n",
      " [[-0.29827024 -0.24371499 -0.2317801   0.33232743  0.25291319  0.19305044\n",
      "   0.35107545 -0.30524434  0.26668965 -0.34838582 -0.31748216  0.24073107]\n",
      " [-0.25095266 -0.23762126  0.29632394 -0.18102786  0.26610763  0.20347432\n",
      "  -0.27722287 -0.29763663 -0.28075955  0.06635031 -0.28434342  0.27121327]\n",
      " [ 0.19951004 -0.25605014 -0.28793825 -0.28686544 -0.28245154  0.24329262\n",
      "   0.26117326  0.23354328  0.32811116  0.27771956  0.16309428  0.25059814]\n",
      " [-0.30898178 -0.23021668  0.32247348  0.29564836  0.20954388  0.25453113\n",
      "  -0.2676991   0.24747415 -0.2643367   0.06590991  0.47002952 -0.30434588]\n",
      " [-0.31798032  0.30492217  0.56007849 -0.24436874  0.23669832  0.22845717\n",
      "  -0.29595161 -0.25575673  0.24432269 -0.30447521  0.22453093  0.2581563 ]\n",
      " [ 0.40518937  0.29677628 -0.24019804  0.36281636  0.24182704  0.25107264\n",
      "   0.21996226  0.27995298 -0.23459298 -0.27835031 -0.26889442 -0.27013738]] \n",
      " network parameter: \n",
      " [[-0.8263487  -0.91115504 -0.92901806 -0.91499138  0.87275429 -0.20874669\n",
      "   0.90392962  0.92277706 -0.85579097 -0.86367281 -0.87632912  0.38614034]\n",
      " [-0.8196326  -0.93195386 -0.89338754 -0.96834294  0.88023406  0.04652773\n",
      "   0.900672    0.93135666 -0.9364961  -0.11134284 -0.88140969 -0.04528929]\n",
      " [-0.40321294 -0.96079414 -0.95028019 -0.89470218  0.88914881  0.00298374\n",
      "   0.87641715  0.91114545 -0.86991103  0.14636648 -0.84979505 -0.08239838]\n",
      " [-0.19989568 -0.96253603 -0.90149335 -0.89507852  0.906917   -0.0403237\n",
      "   0.89157923  0.90708737 -0.88346415  0.21258342 -0.85145877 -0.03298474]\n",
      " [-0.17822175 -0.94751465 -0.91480764 -0.87338484  0.89613106 -0.07528921\n",
      "   0.91391964  0.90534547 -0.88135985  0.21388808 -0.89075827  0.02379142]\n",
      " [-0.08985989 -0.99001428 -0.89201828 -0.89948777  0.85525643 -0.15123106\n",
      "   0.88932007  0.94447731 -0.87853548 -0.7893227  -0.88348175  0.00211624]]\n",
      "best candidates so far: [ 6  2  8 10  2  0]\n",
      "----------epoch 18-----------\n",
      "batched average loss:  0.6809\n",
      "strcuture parameter: \n",
      " [[-0.30854092 -0.24918878 -0.2386672   0.37954845  0.26106814  0.17181043\n",
      "   0.36922319 -0.31947872  0.27605856 -0.37252412 -0.33462493  0.24814767]\n",
      " [-0.25968974 -0.2383199   0.3152802  -0.17785706  0.27373705  0.20096958\n",
      "  -0.28966019 -0.31125797 -0.29135594  0.04748691 -0.29584196  0.27825675]\n",
      " [ 0.20308561 -0.26334623 -0.2952735  -0.29471681 -0.29257211  0.24768172\n",
      "   0.26957529  0.24400678  0.34148749  0.28435222  0.16642204  0.25799714]\n",
      " [-0.32577949 -0.23514798  0.34252099  0.30837867  0.2126056   0.26091305\n",
      "  -0.27803003  0.25698815 -0.28075415  0.03485526  0.48580863 -0.31760379]\n",
      " [-0.3378395   0.3229104   0.61591885 -0.25307604  0.24143542  0.23451768\n",
      "  -0.31005617 -0.26513416  0.25174926 -0.31698673  0.23025093  0.26521972]\n",
      " [ 0.422328    0.33873728 -0.25060147  0.38658271  0.2511012   0.25786232\n",
      "   0.22360355  0.2919288  -0.25074666 -0.29100561 -0.28595895 -0.28081268]] \n",
      " network parameter: \n",
      " [[-0.94134196 -0.99698354 -1.01394091 -0.99911326  0.91834049 -0.23249391\n",
      "   0.97898675  0.99647531 -0.92611062 -0.97979792 -0.94068461  0.37549203]\n",
      " [-0.93613871 -1.01933452 -0.97995964 -1.05103648  0.92233705  0.07936065\n",
      "   0.97313381  1.00663654 -1.00485108 -0.16645886 -0.94335803 -0.10023258]\n",
      " [-0.50552597 -1.0507775  -1.0385537  -0.97740969  0.93184448  0.00772769\n",
      "   0.94664536  0.98758102 -0.93483849  0.22341047 -0.90951337 -0.1267719 ]\n",
      " [-0.28385726 -1.05301466 -0.98975823 -0.97825878  0.94979916 -0.07116052\n",
      "   0.96048902  0.98273809 -0.94737611  0.30912575 -0.90889738 -0.06000344]\n",
      " [-0.26142937 -1.03798872 -1.0010995  -0.95435962  0.94253623 -0.13873573\n",
      "   0.98612497  0.98252331 -0.94862114  0.31219505 -0.95198047  0.01383767]\n",
      " [-0.17140836 -1.08085143 -0.97748443 -0.97908174  0.90302477 -0.23998556\n",
      "   0.96699801  1.02292058 -0.94799679 -0.89991077 -0.94812289  0.00211624]]\n",
      "best candidates so far: [ 3  2  8 10  2  0]\n",
      "----------epoch 20-----------\n",
      "batched average loss:  0.67676675\n",
      "strcuture parameter: \n",
      " [[-0.3171837  -0.25200933 -0.24276792  0.42407216  0.26699034  0.15056734\n",
      "   0.38435702 -0.33230207  0.28404536 -0.39401122 -0.34989636  0.25403446]\n",
      " [-0.26619737 -0.2355734   0.33201561 -0.1756923   0.27972318  0.19807717\n",
      "  -0.30092301 -0.3224441  -0.30097374  0.02430459 -0.30588389  0.28418953]\n",
      " [ 0.20646227 -0.26904715 -0.30155353 -0.30112817 -0.301387    0.24889874\n",
      "   0.27662839  0.2530245   0.35215023  0.28991858  0.16982652  0.2640689 ]\n",
      " [-0.3397558  -0.23863721  0.36162746  0.31916191  0.21346579  0.26619744\n",
      "  -0.28679075  0.26420679 -0.29547424  0.0131572   0.49391153 -0.32892143]\n",
      " [-0.35380848  0.34142517  0.66855303 -0.26048124  0.24367824  0.23885531\n",
      "  -0.32332785 -0.27346482  0.25753946 -0.32772594  0.23356111  0.27091007]\n",
      " [ 0.44209012  0.38153429 -0.25973133  0.40793496  0.2585087   0.26322107\n",
      "   0.22373822  0.30132068 -0.26482137 -0.30170782 -0.30049566 -0.28972204]] \n",
      " network parameter: \n",
      " [[-1.05287625 -1.07185939 -1.08886071 -1.07028075  0.9445548  -0.27896488\n",
      "   1.04190279  1.0534272  -0.9826415  -1.09820755 -0.99119286  0.37576689]\n",
      " [-1.05464046 -1.09707955 -1.05667329 -1.1213311   0.94469321  0.12435908\n",
      "   1.03231648  1.06456298 -1.05838482 -0.24449172 -0.99106187 -0.1651689 ]\n",
      " [-0.61529355 -1.13128984 -1.11712044 -1.04784032  0.95579736  0.03425208\n",
      "   1.00265001  1.04708277 -0.98494228  0.31676712 -0.95420638 -0.17249361]\n",
      " [-0.37960599 -1.13433587 -1.06887448 -1.04855859  0.97290737 -0.08602832\n",
      "   1.01598997  1.04317732 -0.99598865  0.41305701 -0.95128155 -0.0815653 ]\n",
      " [-0.35946266 -1.11998224 -1.07764402 -1.02236992  0.96951233 -0.20264819\n",
      "   1.04577344  1.04481352 -1.00047129  0.41797922 -0.99835709  0.01533964]\n",
      " [-0.26721954 -1.16247019 -1.05184473 -1.04527203  0.93309791 -0.34057348\n",
      "   1.03318493  1.08761287 -1.00306222 -0.99997431 -0.99942446  0.00211624]]\n",
      "best candidates so far: [ 3  2  8 10  2  0]\n",
      "----------epoch 22-----------\n",
      "batched average loss:  0.6774516\n",
      "strcuture parameter: \n",
      " [[-0.32436178 -0.25289146 -0.245935    0.46136995  0.27121168  0.13492915\n",
      "   0.3964803  -0.34368086  0.29091451 -0.41146542 -0.36292531  0.25871175]\n",
      " [-0.2719064  -0.23158904  0.34739486 -0.17470832  0.28421687  0.19516388\n",
      "  -0.31029844 -0.33152287 -0.30941331  0.00302296 -0.31488188  0.2893504 ]\n",
      " [ 0.20984229 -0.27290782 -0.30591374 -0.30679459 -0.30889039  0.24956904\n",
      "   0.28276104  0.26013552  0.36044722  0.29506931  0.16912571  0.26870993]\n",
      " [-0.3509921  -0.24093331  0.37848577  0.32810009  0.21370393  0.27071181\n",
      "  -0.29399017  0.26938339 -0.30717859 -0.00439907  0.49489709 -0.3381871 ]\n",
      " [-0.36597649  0.35888709  0.71382843 -0.26668342  0.24451925  0.24201431\n",
      "  -0.33516804 -0.2805678   0.26181552 -0.33658795  0.23582876  0.27556495]\n",
      " [ 0.45715848  0.42272802 -0.26663885  0.42527908  0.26434743  0.2678301\n",
      "   0.22217622  0.30892874 -0.27662348 -0.31053766 -0.31307393 -0.29727059]] \n",
      " network parameter: \n",
      " [[-1.15912081 -1.13651619 -1.15419933 -1.12828113  0.95337575 -0.34682272\n",
      "   1.09349008  1.0940354  -1.02655006 -1.2180054  -1.02975603  0.37378297]\n",
      " [-1.17439331 -1.16514429 -1.12424033 -1.17899699  0.95033167  0.18212693\n",
      "   1.07945093  1.10566175 -1.09822097 -0.33768869 -1.02554961 -0.23822247]\n",
      " [-0.73004781 -1.20246118 -1.18665851 -1.10560191  0.96372409  0.07402117\n",
      "   1.0460583   1.08989467 -1.02170684  0.41938029 -0.98497922 -0.21281661]\n",
      " [-0.48550618 -1.20670485 -1.13836026 -1.10604197  0.97842692 -0.0936674\n",
      "   1.05944386  1.08779281 -1.03123063  0.52389696 -0.97979571 -0.08375117]\n",
      " [-0.4671978  -1.19343296 -1.1443514  -1.07739608  0.97958559 -0.27373714\n",
      "   1.09385157  1.0920471  -1.03796014  0.52930108 -1.03090823  0.04425674]\n",
      " [-0.37341603 -1.23529479 -1.115569   -1.0980196   0.94679729 -0.44936514\n",
      "   1.08855643  1.13817698 -1.04519342 -1.08387606 -1.03850395  0.00211624]]\n",
      "best candidates so far: [ 3  2  8 10  2  0]\n",
      "----------epoch 24-----------\n",
      "batched average loss:  0.6765812\n",
      "strcuture parameter: \n",
      " [[-0.33041399 -0.25228605 -0.24884474  0.49287982  0.2733359   0.12257823\n",
      "   0.40689368 -0.35417652  0.29652799 -0.42563281 -0.3736335   0.26308804]\n",
      " [-0.2769377  -0.22626248  0.36146877 -0.17547351  0.28738586  0.19134777\n",
      "  -0.31790903 -0.33897732 -0.31673098 -0.01663947 -0.32288446  0.29407661]\n",
      " [ 0.21326348 -0.27530399 -0.30928441 -0.31151357 -0.31578983  0.25011119\n",
      "   0.28767914  0.26621284  0.36692318  0.29919011  0.16326366  0.27284642]\n",
      " [-0.36034926 -0.24262354  0.39331064  0.33570218  0.2124369   0.27503342\n",
      "  -0.30025112  0.27307023 -0.31699881 -0.01725953  0.49202015 -0.34553281]\n",
      " [-0.37587247  0.37582977  0.75308677 -0.2722663   0.24471243  0.24478997\n",
      "  -0.34550637 -0.28648021  0.26449464 -0.34401209  0.23690625  0.27909042]\n",
      " [ 0.47144487  0.46902872 -0.27309778  0.43522711  0.268556    0.27153201\n",
      "   0.22097907  0.31504542 -0.28592197 -0.31826256 -0.32422448 -0.30362257]] \n",
      " network parameter: \n",
      " [[-1.2548574  -1.19177023 -1.21055223 -1.1744967   0.94775885 -0.43253563\n",
      "   1.13386785  1.11961273 -1.05865496 -1.33961055 -1.05769409  0.36487183]\n",
      " [-1.29244355 -1.22420327 -1.18330333 -1.22537376  0.94120506  0.2197292\n",
      "   1.11510925  1.13017922 -1.1258813  -0.44286271 -1.04892951 -0.31705309]\n",
      " [-0.84825691 -1.26489824 -1.24737359 -1.15200967  0.95709675  0.09835225\n",
      "   1.07727736  1.1155954  -1.04670209  0.53023384 -1.00392463 -0.2465494 ]\n",
      " [-0.5983473  -1.27063345 -1.19889363 -1.1516382   0.96908488 -0.12068032\n",
      "   1.09119263  1.1176819  -1.05449665  0.63938578 -0.99680976 -0.05712758]\n",
      " [-0.58148693 -1.25888026 -1.20141633 -1.12022119  0.97516639 -0.36193352\n",
      "   1.13131153  1.1245904  -1.0627278   0.64547436 -1.05177663  0.10561395]\n",
      " [-0.48665259 -1.30012598 -1.16921085 -1.13820594  0.94709004 -0.56419528\n",
      "   1.13402171  1.17541997 -1.07429486 -1.13714344 -1.0665005   0.00211624]]\n",
      "best candidates so far: [ 3  2  8 10  2  0]\n",
      "----------epoch 26-----------\n",
      "batched average loss:  0.67700845\n",
      "strcuture parameter: \n",
      " [[-0.33567765 -0.25151589 -0.24874248  0.51871954  0.27418583  0.12366695\n",
      "   0.41526335 -0.36402112  0.30098192 -0.43696734 -0.38192738  0.26646445]\n",
      " [-0.28133621 -0.21932483  0.37497153 -0.17697109  0.28917577  0.18461551\n",
      "  -0.32386939 -0.34568021 -0.32350966 -0.03150448 -0.32978325  0.29826769]\n",
      " [ 0.21605806 -0.27665242 -0.31137966 -0.315699   -0.32197985  0.25338341\n",
      "   0.29078797  0.27070629  0.37266407  0.30261513  0.15380205  0.27684655]\n",
      " [-0.36736806 -0.24353256  0.40680092  0.34177221  0.21090382  0.27847551\n",
      "  -0.30553231  0.27535078 -0.32436391 -0.02579848  0.48797446 -0.35158329]\n",
      " [-0.38435817  0.39363738  0.78727933 -0.27715175  0.24294935  0.2472266\n",
      "  -0.35512447 -0.2915316   0.26489658 -0.35017246  0.23693422  0.28209399]\n",
      " [ 0.48544479  0.51696399 -0.27870386  0.43703005  0.27091499  0.27507319\n",
      "   0.22060357  0.32006941 -0.29362696 -0.32517505 -0.3344689  -0.30893256]] \n",
      " network parameter: \n",
      " [[-1.3398732  -1.23873994 -1.25923667 -1.20930035  0.93008513 -0.52297274\n",
      "   1.16479234  1.13191354 -1.08070632 -1.46327453 -1.07633346  0.35514165]\n",
      " [-1.40888719 -1.27525668 -1.23472068 -1.26126002  0.91934906  0.27149107\n",
      "   1.14018172  1.1409795  -1.14309232 -0.55605845 -1.06276195 -0.40103215]\n",
      " [-0.968065   -1.31966334 -1.3004352  -1.18791479  0.93918077  0.14443277\n",
      "   1.09745202  1.12727131 -1.06031248  0.64682526 -1.01274061 -0.26578131]\n",
      " [-0.71372216 -1.32713565 -1.25134332 -1.18645519  0.94878708 -0.12852151\n",
      "   1.11272626  1.13441156 -1.06691418  0.75724946 -1.0028558  -0.00165707]\n",
      " [-0.69785749 -1.31736483 -1.25025981 -1.15228017  0.95826076 -0.46108048\n",
      "   1.15936526  1.14492535 -1.07517843  0.76516237 -1.06230765  0.1905321 ]\n",
      " [-0.60299632 -1.35757254 -1.21433311 -1.16690298  0.93540065 -0.68260699\n",
      "   1.17084787  1.2009456  -1.09141877 -1.16377868 -1.0856495   0.00211624]]\n",
      "best candidates so far: [ 3  2  8 10  2  1]\n",
      "----------epoch 28-----------\n",
      "batched average loss:  0.6755631\n",
      "strcuture parameter: \n",
      " [[-0.33952235 -0.25020832 -0.24811883  0.53609756  0.27410368  0.12603125\n",
      "   0.42285475 -0.3721256   0.30462624 -0.44562997 -0.38877532  0.26887653]\n",
      " [-0.28464115 -0.21212702  0.38751307 -0.17940957  0.29016499  0.17733911\n",
      "  -0.3282005  -0.35203298 -0.33067003 -0.03948798 -0.33571079  0.30175464]\n",
      " [ 0.21832977 -0.27776078 -0.31331383 -0.31943308 -0.32787968  0.26192841\n",
      "   0.29240157  0.2741295   0.37777582  0.3058923   0.14446109  0.28073503]\n",
      " [-0.37295498 -0.2438285   0.41872821  0.34659434  0.20835169  0.2815196\n",
      "  -0.31015948  0.27707574 -0.33083724 -0.0311452   0.48224232 -0.35614   ]\n",
      " [-0.39151584  0.40965259  0.82113521 -0.28165102  0.23966918  0.24908928\n",
      "  -0.36430045 -0.29600501  0.2639242  -0.35533403  0.23632874  0.28495385]\n",
      " [ 0.50111791  0.56972441 -0.28390144  0.42901221  0.27222783  0.27787359\n",
      "   0.22298851  0.32414467 -0.30161469 -0.33134147 -0.34326493 -0.31351936]] \n",
      " network parameter: \n",
      " [[-1.40951669 -1.27866145 -1.30151278 -1.23358575  0.90386614 -0.62466972\n",
      "   1.18777782  1.13340762 -1.09310144 -1.58864541 -1.08662855  0.31324952]\n",
      " [-1.52340664 -1.31932418 -1.27979669 -1.28800569  0.88894972  0.34106553\n",
      "   1.15614518  1.14065964 -1.15059519 -0.67452075 -1.06849144 -0.49894382]\n",
      " [-1.08804126 -1.3676291  -1.34670995 -1.21431195  0.91199986  0.20576179\n",
      "   1.10795319  1.12842952 -1.06397377  0.76765037 -1.01359583 -0.28629745]\n",
      " [-0.8309506  -1.37726158 -1.29655051 -1.21107669  0.91909872 -0.13771851\n",
      "   1.12542015  1.14043429 -1.07016233  0.87900135 -1.00127035  0.07113963]\n",
      " [-0.81575886 -1.36964071 -1.29137859 -1.17464166  0.93076232 -0.56933113\n",
      "   1.17931684  1.1553191  -1.07788477  0.88775515 -1.06531792  0.28973006]\n",
      " [-0.7226953  -1.40926203 -1.25140881 -1.18519922  0.91406841 -0.80468699\n",
      "   1.19993638  1.21718881 -1.0992165  -1.15950988 -1.09787249  0.00211624]]\n",
      "best candidates so far: [ 3  2  8 10  2  1]\n",
      "----------epoch 30-----------\n",
      "batched average loss:  0.673665\n",
      "strcuture parameter: \n",
      " [[-0.34244894 -0.24950243 -0.24660495  0.54595825  0.27349167  0.12915318\n",
      "   0.43084099 -0.37949373  0.30748485 -0.45330678 -0.39524609  0.27091946]\n",
      " [-0.28667865 -0.20672002  0.39843568 -0.18212344  0.29067949  0.17252783\n",
      "  -0.33125061 -0.35808661 -0.33771683 -0.03681686 -0.34091671  0.30448218]\n",
      " [ 0.22082819 -0.27828361 -0.31366995 -0.32309097 -0.33312319  0.2691963\n",
      "   0.29324347  0.27619589  0.38197486  0.30959565  0.12873518  0.28426875]\n",
      " [-0.37730515 -0.24337042  0.42987191  0.34979272  0.204177    0.28461244\n",
      "  -0.3142991   0.27839518 -0.33627313 -0.0367118   0.47652337 -0.35984926]\n",
      " [-0.39887942  0.42406242  0.85100678 -0.28567139  0.23626384  0.25094918\n",
      "  -0.37241565 -0.29995174  0.26220537 -0.35933895  0.23584856  0.28722374]\n",
      " [ 0.513435    0.62318103 -0.28812438  0.4188589   0.27223235  0.28083208\n",
      "   0.22338621  0.32698702 -0.30984651 -0.33608879 -0.35117706 -0.31731062]] \n",
      " network parameter: \n",
      " [[-1.46253823 -1.31289993 -1.33842869 -1.24837349  0.87082067 -0.73137012\n",
      "   1.2043147   1.1264148  -1.09683129 -1.71513743 -1.08960795  0.25632414]\n",
      " [-1.63625486 -1.357717   -1.31951113 -1.30627894  0.85208606  0.41625143\n",
      "   1.16508958  1.13103384 -1.14951148 -0.7968435  -1.06738545 -0.60567653]\n",
      " [-1.20775145 -1.4099841  -1.38777812 -1.23213346  0.8784234   0.27726647\n",
      "   1.11116385  1.1197187  -1.05907006  0.89146544 -1.00739352 -0.29737736]\n",
      " [-0.94821069 -1.42208216 -1.33601103 -1.22686605  0.8818295  -0.14120344\n",
      "   1.1308461   1.13687617 -1.06467471  1.00337337 -0.99317505  0.15807959]\n",
      " [-0.93647563 -1.41658501 -1.32631984 -1.18850065  0.89591853 -0.68355113\n",
      "   1.19244753  1.15689431 -1.07214313  1.01295075 -1.06152595  0.39904162]\n",
      " [-0.84487136 -1.45578674 -1.28199934 -1.19508769  0.88416477 -0.9291656\n",
      "   1.22243708  1.22535667 -1.09843313 -1.12855611 -1.10390908  0.00211624]]\n",
      "best candidates so far: [ 3  2  8 10  2  1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorcircuit as tc\n",
    "import tensorflow as tf\n",
    "from DQAS_tool import  DQASAnsatz_from_result\n",
    "K = tc.set_backend(\"tensorflow\")\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.06, 100, 0.5)\n",
    "structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.05))\n",
    "network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))\n",
    "verbose = True\n",
    "# 设置超参数\n",
    "epochs = 100\n",
    "batch_size=50\n",
    "shape_nnp = (num_layer, shape_parametized)\n",
    "shape_stp = (num_layer, shape_parametized)\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp).astype(rtype)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp).astype(rtype)\n",
    "#print(stp.shape)\n",
    "avcost1 = 0\n",
    "\n",
    "ops_onehot = ops.OneHot(axis=-1)\n",
    "\n",
    "batch_loss_history=[] # 记录每个epoch的batch_size损失值\n",
    "structure_distribution_history=[] # 记录每个epoch的结构参数\n",
    "ansatz_params_history=[] # 记录每个epoch的网络参数\n",
    "best_candidates_history=[] # 记录每个epoch的最佳候选\n",
    "\n",
    "\n",
    "for epoch in range(epochs):  # 更新结构参数的迭代\n",
    "    avcost2 = avcost1\n",
    "    costl = []\n",
    "    tmp = np.stack([sampling_from_structure(stp,num_layer,shape_parametized) for _ in range(batch_size)])\n",
    "    batch_structure = ops_onehot(ms.Tensor(tmp),shape_parametized,ms.Tensor(1),ms.Tensor(0))\n",
    "    #print(batch_structure.shape)\n",
    "    # print(tmp,batch_structure)\n",
    "    loss_value = []\n",
    "    grad_nnps = []\n",
    "    grad_stps = []\n",
    "    \n",
    "    for i in batch_structure:          \n",
    "        infd, grad_nnp = vag_nnp(Structure_params=i,Ansatz_params=nnp,paramerterized_pool=unbound_opeartor_pool,num_layer=num_layer,n_qbits=8)(ms.Tensor(X_train),ms.Tensor(y_train))\n",
    "        gs = nmf_gradient(structures=stp,oh=i,num_layer=num_layer,size_pool=shape_parametized)\n",
    "        #print(infd,grad_nnp)\n",
    "        loss_value.append(infd)\n",
    "        grad_nnps.append(grad_nnp[0])\n",
    "        grad_stps.append(gs)\n",
    "    \n",
    "    infd = ops.stack(loss_value)\n",
    "    gnnp = ops.addn(grad_nnps)\n",
    "    gstp = [(infd[i] - avcost2) * grad_stps[i] for i in range(infd.shape[0])]\n",
    "    gstp_averge = ops.addn(gstp) / infd.shape[0]\n",
    "    avcost1 = sum(infd) / infd.shape[0]\n",
    "    \n",
    "    gnnp_tf = tf.convert_to_tensor(gnnp.reshape(nnp.shape).asnumpy(),dtype=tf.float64)\n",
    "    nnp_tf = tf.convert_to_tensor(nnp,dtype=tf.float64)\n",
    "    gstp_averge_tf = tf.convert_to_tensor(gstp_averge.reshape(stp.shape).asnumpy(),dtype=tf.float64)\n",
    "    stp_tf = tf.convert_to_tensor(stp,dtype=tf.float64)\n",
    "    # 更新参数\n",
    "    nnp_tf = network_opt.update(gnnp_tf, nnp_tf)\n",
    "    stp_tf = structure_opt.update(gstp_averge_tf, stp_tf) \n",
    "    \n",
    "    nnp = nnp_tf.numpy()\n",
    "    stp = stp_tf.numpy()\n",
    "    \n",
    "    batch_loss_history.append(avcost1)\n",
    "    structure_distribution_history.append(stp)\n",
    "    ansatz_params_history.append(nnp)\n",
    "    #best_candidates_history.append(best_from_structure(cand_preset.asnumpy()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if epoch % 2 == 0 or epoch == epochs - 1:\n",
    "        print(\"----------epoch %s-----------\" % epoch)\n",
    "        print(\n",
    "            \"batched average loss: \",\n",
    "            avcost1,\n",
    "        )\n",
    "    \n",
    "        if verbose:\n",
    "            print(\n",
    "                \"strcuture parameter: \\n\",\n",
    "                stp,\n",
    "                \"\\n network parameter: \\n\",\n",
    "                nnp,\n",
    "            )\n",
    "        \n",
    "        cand_preset = best_from_structure(stp)\n",
    "        best_candidates_history.append(best_from_structure(cand_preset.asnumpy()))\n",
    "        print(\"best candidates so far:\",cand_preset)\n",
    "        # print(\n",
    "        #     \"corresponding weights for each gate:\",\n",
    "        #     [K.numpy(nnp[j, i]) if i < 6 else 0.0 for j, i in enumerate(cand_preset)],\n",
    "        # )\n",
    "        \n",
    "\n",
    "    # if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "    #     _,acc = DQASAnsatz_from_result(best_candidate=cand_preset.asnumpy(),parameterized_pool=unbound_opeartor_pool,num_layer=num_layer,n_qbits=8)\n",
    "    #     print(\"目前的二分类精度是:\",np.max(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
