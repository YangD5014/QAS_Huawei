{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mindquantum.core.gates import RX, RY, RZ, H, X, Y, Z, CNOT\n",
    "from mindquantum.core.circuit import Circuit\n",
    "import mindspore as ms\n",
    "from mindquantum.simulator import  Simulator\n",
    "from mindquantum.core.gates import GroupedPauli\n",
    "from mindquantum.core.operators import TimeEvolution,QubitOperator\n",
    "from mindquantum.core.parameterresolver import PRConvertible,PRGenerator,ParameterResolver\n",
    "from DQAS_tool import generate_pauli_string,one_hot,unbound_opeartor_pool\n",
    "from mindquantum.core.gates import RotPauliString\n",
    "from mindquantum.core.gates import UnivMathGate\n",
    "from mindspore import Tensor, ops\n",
    "from mindquantum.core.circuit import UN\n",
    "from mindquantum.core.operators import Hamiltonian             # 导入Hamiltonian模块，用于构建哈密顿量\n",
    "from mindquantum.framework import MQLayer\n",
    "from mindspore.nn import  TrainOneStepCell\n",
    "from mindspore.nn import SoftmaxCrossEntropyWithLogits                         # 导入SoftmaxCrossEntropyWithLogits模块，用于定义损失函数\n",
    "from mindspore.nn import Adam                                                  # 导入Adam模块用于定义优化参数\n",
    "from mindspore.train import Accuracy, Model, LossMonitor                       # 导入Accuracy模块，用于评估预测准确率\n",
    "import mindspore as ms\n",
    "from mindspore import Parameter, Tensor\n",
    "from mindspore.dataset import NumpySlicesDataset\n",
    "from torch.utils.data import DataLoader# 导入NumpySlicesDataset模块，用于创建模型可以识别的数据集\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from data_processing import X_train,X_test,y_train,y_test\n",
    "from mindquantum.algorithm.nisq import HardwareEfficientAnsatz     \n",
    "num_layer = 3\n",
    "# 定义标准差和形状\n",
    "stddev = 0.02\n",
    "shape_parametized = 8\n",
    "shape_unparametized = 4\n",
    "shape_nnp = (num_layer, shape_parametized)\n",
    "shape_stp = (num_layer, shape_parametized+shape_unparametized)\n",
    "\n",
    "shape_stp = (num_layer, shape_parametized)\n",
    "\n",
    "rtype = np.float64\n",
    "ctype = np.complex128\n",
    "# 使用 numpy 生成随机数矩阵\n",
    "np.random.seed(10)\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp).astype(rtype)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp).astype(rtype)\n",
    "# #Operator Pool\n",
    "unbound_opeartor_pool = [generate_pauli_string(n=8,seed=i)[0] for i in range(shape_parametized)]\n",
    "bound_opeartor_pool = [generate_pauli_string(n=8,seed=i)[1] for i in range(shape_parametized,shape_parametized+shape_unparametized)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DQAS_tool import Mindspore_ansatz,loss_fn,vag_nnp,sampling_from_structure,vag_nnp,vag_nnp_function,sampling_from_structure\n",
    "from mindquantum.framework import MQOps\n",
    "import mindspore.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.numpy as mnp\n",
    "def nmf_gradient(structures:np.array, oh:ms.Tensor,num_layer: int,size_pool:int):\n",
    "    \"\"\"\n",
    "    使用 MindSpore 实现蒙特卡洛梯度计算。\n",
    "    \"\"\"\n",
    "      # Step 1: 获取选择的索引\n",
    "    choice = ops.Argmax(axis=-1)(oh)\n",
    "    # Step 2: 计算概率\n",
    "    softmax = ops.Softmax(axis=-1)\n",
    "    prob = softmax(ms.Tensor(structures))\n",
    "    # Step 3: 获取概率矩阵中的值\n",
    "    indices = mnp.stack((mnp.arange(num_layer, dtype=ms.int64), choice), axis=1)\n",
    "    prob = ops.GatherNd()(prob, indices)\n",
    "    # Step 4: 变换概率矩阵\n",
    "    prob = prob.reshape(-1, 1)\n",
    "    prob = ops.Tile()(prob, (1, size_pool))\n",
    "    \n",
    "    # Step 5: 生成蒙特卡洛梯度\n",
    "    gradient = ops.TensorScatterAdd()(Tensor(-prob, ms.float64), indices, mnp.ones((num_layer,), dtype=ms.float64))\n",
    "    return gradient\n",
    "    \n",
    "    \n",
    "# 对向量化版本的封装\n",
    "# nmf_gradient_vmap = ops.vmap(nmf_gradient, in_axes=(None, 0, None, None))\n",
    "\n",
    "def best_from_structure(structures: np.array):\n",
    "    return ops.Argmax(axis=-1)(ms.Tensor(structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorcircuit as tc\n",
    "import tensorflow as tf\n",
    "K = tc.set_backend(\"tensorflow\")\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.06, 100, 0.5)\n",
    "structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.12))\n",
    "network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))\n",
    "verbose = True\n",
    "# 设置超参数\n",
    "epochs = 100\n",
    "batch_size=50\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp).astype(rtype)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp).astype(rtype)\n",
    "# 定义学习率衰减\n",
    "# lr = nn.ExponentialDecayLR(learning_rate=0.06, decay_rate=0.5, decay_steps=100)\n",
    "# # 定义优化器\n",
    "# structure_params = Parameter(Tensor(stp), name='Structure_params')\n",
    "# ansatz_params    = Parameter(Tensor(nnp), name='Ansatz_params')\n",
    "\n",
    "# structure_opt = nn.Adam(params=[structure_params], learning_rate=0.12)\n",
    "# network_opt = nn.Adam(params=[ansatz_params], learning_rate=lr)\n",
    "avcost1 = 0\n",
    "\n",
    "ops_onehot = ops.OneHot(axis=-1)\n",
    "\n",
    "for epoch in range(epochs):  # 更新结构参数的迭代\n",
    "    avcost2 = avcost1\n",
    "    costl = []\n",
    "    tmp = np.stack([sampling_from_structure(stp,3,8) for _ in range(batch_size)])\n",
    "    batch_structure = ops_onehot(ms.Tensor(tmp),8,ms.Tensor(1),ms.Tensor(0))\n",
    "    # print(tmp,batch_structure)\n",
    "    loss_value = []\n",
    "    grad_nnps = []\n",
    "    grad_stps = []\n",
    "    \n",
    "    for i in batch_structure:          \n",
    "        infd, grad_nnp = vag_nnp(Structure_params=i,Ansatz_params=nnp, n_layer=3,n_qbits=8)(ms.Tensor(X_train),ms.Tensor(y_train))\n",
    "        gs = nmf_gradient(structures=stp,oh=i,num_layer=3,size_pool=8)\n",
    "        \n",
    "        #print(infd,grad_nnp)\n",
    "        loss_value.append(infd)\n",
    "        grad_nnps.append(grad_nnp[0])\n",
    "        grad_stps.append(gs)\n",
    "    \n",
    "    infd = ops.stack(loss_value)\n",
    "    gnnp = ops.addn(grad_nnps)\n",
    "    gstp = [(infd[i] - avcost2) * grad_stps[i] for i in range(infd.shape[0])]\n",
    "    gstp_averge = ops.addn(gstp) / infd.shape[0]\n",
    "    avcost1 = sum(infd) / infd.shape[0]\n",
    "    \n",
    "    gnnp_tf = tf.convert_to_tensor(gnnp.reshape(nnp.shape).asnumpy(),dtype=tf.float64)\n",
    "    nnp_tf = tf.convert_to_tensor(nnp,dtype=tf.float64)\n",
    "    gstp_averge_tf = tf.convert_to_tensor(gstp_averge.reshape(stp.shape).asnumpy(),dtype=tf.float64)\n",
    "    stp_tf = tf.convert_to_tensor(stp,dtype=tf.float64)\n",
    "    # 更新参数\n",
    "    nnp_tf = network_opt.update(gnnp_tf, nnp_tf)\n",
    "    stp_tf = structure_opt.update(gstp_averge_tf, stp_tf) \n",
    "    \n",
    "    nnp = nnp_tf.numpy()\n",
    "    stp = stp_tf.numpy()\n",
    "    \n",
    "    \n",
    "    if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "        print(\"----------epoch %s-----------\" % epoch)\n",
    "        print(\n",
    "            \"batched average loss: \",\n",
    "            avcost1,\n",
    "        )\n",
    "    \n",
    "        if verbose:\n",
    "            print(\n",
    "                \"strcuture parameter: \\n\",\n",
    "                stp,\n",
    "                \"\\n network parameter: \\n\",\n",
    "                nnp,\n",
    "            )\n",
    "        \n",
    "        cand_preset = best_from_structure(stp)\n",
    "        print(\"best candidates so far:\",cand_preset)\n",
    "        # print(\n",
    "        #     \"corresponding weights for each gate:\",\n",
    "        #     [K.numpy(nnp[j, i]) if i < 6 else 0.0 for j, i in enumerate(cand_preset)],\n",
    "        # )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I0 I1 X2 Z3 I4 I5 Z6 I7',\n",
       " 'Y0 X1 Z2 X3 I4 I5 I6 I7',\n",
       " 'X0 X1 X2 Z3 Y4 Z5 Z6 Y7',\n",
       " 'Y0 Y1 Z2 I3 X4 X5 I6 Z7',\n",
       " 'Y0 Z1 X2 I3 I4 Y5 X6 X7',\n",
       " 'Z0 Z1 X2 I3 Y4 X5 Y6 X7',\n",
       " 'X0 I1 Z2 X3 X4 Y5 I6 Z7',\n",
       " 'Z0 Y1 I2 X3 X4 X5 Z6 X7']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unbound_opeartor_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnnp.reshape(nnp.shape).asnumpy()\n",
    "gnnp_tf = tf.convert_to_tensor(gnnp.reshape(nnp.shape).asnumpy(),dtype=tf.float64)\n",
    "nnp_tf = tf.convert_to_tensor(nnp,dtype=tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.72586032e-08,  1.36091495e-02,  7.08998181e-03,\n",
       "         1.16583984e-03,  5.36890607e-03, -1.50375925e-07,\n",
       "        -1.36224274e-03, -2.24929256e-03],\n",
       "       [ 7.57133023e-08,  1.36107672e-02,  1.37904137e-02,\n",
       "         3.07811541e-04,  2.60351575e-03, -2.02096544e-08,\n",
       "        -2.31005158e-03, -1.54890760e-03],\n",
       "       [ 3.04580894e-09,  1.84652936e-02,  7.09007960e-03,\n",
       "        -5.39380533e-04,  2.60332157e-03, -2.56724313e-08,\n",
       "        -1.60007132e-03, -1.54892216e-03]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnnp_tf.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp = network_opt.update(gnnp_tf, nnp_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 8), dtype=float64, numpy=\n",
       "array([[-9.55692689e-03, -5.76082439e-02, -8.63090953e-02,\n",
       "        -7.81198747e-02, -9.76621702e-02, -4.05456351e-03,\n",
       "         7.12617415e-02,  7.33694538e-02],\n",
       "       [ 1.55234675e-02, -5.57504269e-02, -1.00629721e-01,\n",
       "        -7.62470566e-02, -5.06417149e-02,  7.19465431e-06,\n",
       "         8.59827429e-02,  2.29825579e-02],\n",
       "       [ 3.40375434e-02, -6.60809879e-02, -7.67932112e-02,\n",
       "         8.68686691e-02, -7.68064266e-02,  3.62715725e-03,\n",
       "         5.30593249e-02,  8.73690202e-02]])>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "For 'Optimizer', all elements of the argument 'parameters' must be 'Parameter' or 'dict', please check the 'parameters'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m grads \u001b[38;5;241m=\u001b[39m gnnp\u001b[38;5;241m.\u001b[39mreshape(nnp\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m grads \u001b[38;5;241m=\u001b[39m ms\u001b[38;5;241m.\u001b[39mTensor(grads,ms\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 4\u001b[0m structure_opt \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m structure_params(grads)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:67\u001b[0m, in \u001b[0;36mopt_init_args_register.<locals>.deco\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     arguments\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_args\u001b[39m\u001b[38;5;124m'\u001b[39m, arguments)\n\u001b[0;32m---> 67\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:747\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, learning_rate, beta1, beta2, eps, use_locking, use_nesterov, weight_decay, loss_scale, use_amsgrad, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;129m@opt_init_args_register\u001b[39m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, beta1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, beta2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.999\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m, use_locking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    746\u001b[0m              use_nesterov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, loss_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, use_amsgrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 747\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m     valid_keys \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_lazy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_offload\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m valid_keys:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/optimizer.py:205\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, learning_rate, parameters, weight_decay, loss_scale)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(parameters)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, Parameter) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m parameters) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m parameters):\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, all elements of the argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdict\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m please check the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss_scale, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    209\u001b[0m     loss_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss_scale)\n",
      "\u001b[0;31mTypeError\u001b[0m: For 'Optimizer', all elements of the argument 'parameters' must be 'Parameter' or 'dict', please check the 'parameters'."
     ]
    }
   ],
   "source": [
    "\n",
    "grads = gnnp.reshape(nnp.shape)\n",
    "grads = ms.Tensor(grads,ms.float32)\n",
    "\n",
    "structure_opt = nn.Adam(params=[Tensor(stp,ms.float32)], learning_rate=0.12)\n",
    "structure_params(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The types of arguments in Map must be consistent, but the types of arguments are inconsistent.\nThere are 7 inputs of `map`, corresponding type info:\nIn file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:901\n                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,\n                                      ^\n.\nThe type of the second argument in Map is: List[Tensor[Float32]].\nThe type of the third argument in Map is: Tuple[Ref[Tensor[Float32]]].\nThe type of the 4th argument in Map is: Tuple[Ref[Tensor[Float32]]].\nThe type of the 5th argument in Map is: Tuple[Ref[Tensor[Float32]]].\nThe type of the 6th argument in Map is: Tuple[Bool].\nThe type of the 7th argument in Map is: Tuple[Bool].\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/frontend/operator/composite/map.cc:265 Make\n\n----------------------------------------------------\n- The Traceback of Net Construct Code:\n----------------------------------------------------\n# 0 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:916\n        if not self.use_offload:\n# 1 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:917\n            gradients = self.gradients_centralization(gradients)\n            ^\n# 2 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:928\n        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)\n               ^\n# 3 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:817\n        if self.use_offload:\n# 4 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:826\n            if self.use_dist_optimizer:\n# 5 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:866\n                if self.is_group_lr:\n# 6 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:887\n                    if self.use_lazy:\n# 7 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:894\n                        if self.use_amsgrad:\n# 8 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:901\n                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,\n                            ^\n# 9 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:894\n                        if self.use_amsgrad:\n# 10 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:887\n                    if self.use_lazy:\n# 11 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:866\n                if self.is_group_lr:\n# 12 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:826\n            if self.use_dist_optimizer:\n# 13 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:901\n                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,\n                                      ^\n (See file '/Users/yangjianfei/mac_vscode/华为 QAS 实习/DQAS/rank_0/om/analyze_fail.ir' for more details. Get instructions about `analyze_fail.ir` at https://www.mindspore.cn/search?inputValue=analyze_fail.ir)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m structure_opt \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39m[structure_param], learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.12\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 使用优化器更新参数\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mstructure_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 打印更新后的参数\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated structure_param:\u001b[39m\u001b[38;5;124m\"\u001b[39m, structure_param\u001b[38;5;241m.\u001b[39masnumpy())\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/cell.py:705\u001b[0m, in \u001b[0;36mCell.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    704\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mclear_res()\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Parameter):\n\u001b[1;32m    708\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/cell.py:701\u001b[0m, in \u001b[0;36mCell.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mnew_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 701\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mend_graph(\u001b[38;5;28mself\u001b[39m, output, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/cell.py:482\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[0;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shard_fn(\u001b[38;5;241m*\u001b[39mcast_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 482\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcast_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_forward_hook:\n\u001b[1;32m    484\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/common/api.py:718\u001b[0m, in \u001b[0;36mjit.<locals>.wrap_mindspore.<locals>.staging_specialize\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pynative_parallel() \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m _PYNATIVE_PARALLEL_FUNC_NAME:\n\u001b[1;32m    717\u001b[0m     process_obj \u001b[38;5;241m=\u001b[39m hash_args\n\u001b[0;32m--> 718\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43m_MindsporeFunctionExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/common/api.py:121\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.wrapper\u001b[0;34m(*arg, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 121\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_python_data(results)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/common/api.py:350\u001b[0m, in \u001b[0;36m_MindsporeFunctionExecutor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    349\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mclear_res()\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mget_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecompile_only\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/common/api.py:344\u001b[0m, in \u001b[0;36m_MindsporeFunctionExecutor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mget_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m context\u001b[38;5;241m.\u001b[39mPYNATIVE_MODE:\n\u001b[1;32m    343\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mset_jit_compile_status(\u001b[38;5;28;01mTrue\u001b[39;00m, phase)\n\u001b[0;32m--> 344\u001b[0m     phase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mset_jit_compile_status(\u001b[38;5;28;01mFalse\u001b[39;00m, phase)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/common/api.py:443\u001b[0m, in \u001b[0;36m_MindsporeFunctionExecutor.compile\u001b[0;34m(self, method_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, ms\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCell):\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_executor\u001b[38;5;241m.\u001b[39mset_weights_values(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mparameters_dict())\n\u001b[0;32m--> 443\u001b[0m     is_compile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compile:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecutor compile failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The types of arguments in Map must be consistent, but the types of arguments are inconsistent.\nThere are 7 inputs of `map`, corresponding type info:\nIn file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:901\n                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,\n                                      ^\n.\nThe type of the second argument in Map is: List[Tensor[Float32]].\nThe type of the third argument in Map is: Tuple[Ref[Tensor[Float32]]].\nThe type of the 4th argument in Map is: Tuple[Ref[Tensor[Float32]]].\nThe type of the 5th argument in Map is: Tuple[Ref[Tensor[Float32]]].\nThe type of the 6th argument in Map is: Tuple[Bool].\nThe type of the 7th argument in Map is: Tuple[Bool].\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/frontend/operator/composite/map.cc:265 Make\n\n----------------------------------------------------\n- The Traceback of Net Construct Code:\n----------------------------------------------------\n# 0 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:916\n        if not self.use_offload:\n# 1 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:917\n            gradients = self.gradients_centralization(gradients)\n            ^\n# 2 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:928\n        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)\n               ^\n# 3 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:817\n        if self.use_offload:\n# 4 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:826\n            if self.use_dist_optimizer:\n# 5 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:866\n                if self.is_group_lr:\n# 6 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:887\n                    if self.use_lazy:\n# 7 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:894\n                        if self.use_amsgrad:\n# 8 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:901\n                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,\n                            ^\n# 9 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:894\n                        if self.use_amsgrad:\n# 10 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:887\n                    if self.use_lazy:\n# 11 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:866\n                if self.is_group_lr:\n# 12 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:826\n            if self.use_dist_optimizer:\n# 13 In file /opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/nn/optim/adam.py:901\n                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,\n                                      ^\n (See file '/Users/yangjianfei/mac_vscode/华为 QAS 实习/DQAS/rank_0/om/analyze_fail.ir' for more details. Get instructions about `analyze_fail.ir` at https://www.mindspore.cn/search?inputValue=analyze_fail.ir)"
     ]
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor, Parameter\n",
    "\n",
    "# 假设 stp 和 nnp 已经定义\n",
    "stp = np.random.randn(8)  # 示例数据\n",
    "nnp = np.random.randn(8)  # 示例数据\n",
    "\n",
    "# 假设 gnnp 已经计算出来\n",
    "gnnp = Tensor(np.random.randn(8), ms.float32)  # 示例数据\n",
    "\n",
    "# 将 gnnp 重新调整形状并转换为 Tensor\n",
    "grads = gnnp.reshape(nnp.shape)\n",
    "grads = Tensor(grads, ms.float32)\n",
    "\n",
    "# 将 stp 转换为 Parameter\n",
    "structure_param = Parameter(Tensor(stp, ms.float32), name=\"structure_param\")\n",
    "\n",
    "# 定义优化器\n",
    "structure_opt = nn.Adam(params=[structure_param], learning_rate=0.12)\n",
    "\n",
    "# 使用优化器更新参数\n",
    "\n",
    "\n",
    "# 打印更新后的参数\n",
    "print(\"Updated structure_param:\", structure_param.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[3, 8], dtype=Float32, value=\n",
       "[[-9.88132456e-08,  1.32359611e-02, -1.76628027e-03 ...  5.35264235e-07, -4.51928005e-03, -2.10772082e-03],\n",
       " [-2.20070717e-07,  1.32363131e-02, -1.25867198e-03 ... -1.39021199e-06, -3.57847963e-03, -2.90810829e-03],\n",
       " [-1.14492039e-07,  1.78225935e-02, -1.25893648e-03 ...  3.06085539e-08, -7.34172668e-03, -2.10750639e-03]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnnp.reshape(nnp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01122259,  0.01832192,  0.0024974 , -0.00064747, -0.05200137,\n",
       "         0.01730596,  0.01005961,  0.00038118],\n",
       "       [-0.02940138, -0.01214009, -0.01184792,  0.0424261 , -0.04052099,\n",
       "        -0.04069649, -0.0063086 ,  0.00610193],\n",
       "       [ 0.00237795, -0.00130611,  0.02833612,  0.0174096 , -0.02848663,\n",
       "         0.04429366,  0.053757  ,  0.02116067]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[3, 8], dtype=Float64, value=\n",
       "[[ 7.02791789e-02, -1.11342516e-01,  7.02871897e-02 ...  2.51894877e-01,  7.02913501e-02, -1.11342516e-01],\n",
       " [-1.13839345e-01, -1.13839345e-01, -1.13839345e-01 ...  4.31045921e-01, -1.13839345e-01,  6.77940914e-02],\n",
       " [ 6.72984868e-02,  6.72980577e-02, -1.14335379e-01 ... -1.14335379e-01,  2.48890272e-01, -1.14335379e-01]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gstp_averge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[], dtype=Float32, value= 4.54124)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(infd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[3, 8], dtype=Float64, value=\n",
       "[[-8.05453571e-02,  1.78959055e-01,  4.92015483e-02 ... -8.05453571e-02, -8.05453571e-02,  4.92067510e-02],\n",
       " [-8.02022707e-02, -8.02022707e-02,  4.95423953e-02 ... -8.02022707e-02,  3.09048212e-01, -8.02022707e-02],\n",
       " [-8.20206445e-02, -8.20206445e-02,  4.77262609e-02 ... -8.20206445e-02,  1.77476964e-01,  4.77314635e-02]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops.addn(gstp)/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[5], dtype=Float32, value= [ 9.08508897e-01,  9.08438444e-01,  9.08429801e-01,  9.08451319e-01,  9.08427000e-01])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.addn(grad_nnps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_onehot = ops.OneHot(axis=-1)\n",
    "batch_size=10\n",
    "tmp = np.stack([sampling_from_structure(stp,3,8) for _ in range(batch_size)])\n",
    "batch_structure = ops_onehot(ms.Tensor(tmp),8,ms.Tensor(1),ms.Tensor(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = ops.Argmax(axis=-1)(batch_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[3, 8], dtype=Float64, value=\n",
       "[[-1.26967952e-01,  8.73032048e-01, -1.26967952e-01 ... -1.26967952e-01, -1.26967952e-01, -1.26967952e-01],\n",
       " [-1.26017213e-01, -1.26017213e-01, -1.26017213e-01 ... -1.26017213e-01, -1.26017213e-01, -1.26017213e-01],\n",
       " [-1.28127679e-01, -1.28127679e-01,  8.71872321e-01 ... -1.28127679e-01, -1.28127679e-01, -1.28127679e-01]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_gradient(structures=stp,oh=batch_structure[0],num_layer=3,size_pool=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ansatz =Mindspore_ansatz(Structure_p=stp,n_layer=2,n_qbits=8)\n",
    "\n",
    "ansatz = HardwareEfficientAnsatz(8, single_rot_gate_seq=[RY], entangle_gate=X, depth=2).circuit   \n",
    "ansatz = encoder + ansatz.as_ansatz()\n",
    "sim = Simulator(backend='mqvector', n_qubits=8)\n",
    "hams = [Hamiltonian(QubitOperator(f'Z{i}')) for i in [0, 1]]\n",
    "grad_ops = sim.get_expectation_with_grad(hams, ansatz)\n",
    "Myops = MQOps(grad_ops)\n",
    "\n",
    "\n",
    "def forward_fn(encode_p, ansatz_p, y_label):\n",
    "    eval_obserables = Myops(encode_p, ansatz_p)\n",
    "    loss = loss_fn(eval_obserables, y_label)\n",
    "    return loss\n",
    "grad_fn = ms.value_and_grad(fn=forward_fn,grad_position=None,weights=nnp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=140\n",
    "grad_fn(ms.Tensor(X_train[0:index]),nnp, ms.Tensor(y_train[0:index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "c = np.array([7, 8, 9])\n",
    "\n",
    "# 沿着新轴堆叠数组\n",
    "result = np.stack((a, b, c), axis=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_fn(Net,loss_fun,x, y):\n",
    "    z = Net(x)\n",
    "    #print(z)\n",
    "    loss = loss_fun(z, y)\n",
    "    return loss\n",
    "\n",
    "def my_fun(Structure_params:np.array,Ansatz_params:np.array):\n",
    "    ansatz = Mindspore_ansatz(Structure_params,3,8)\n",
    "    sim = Simulator(backend='mqvector',n_qubits=8)\n",
    "    hams = [Hamiltonian(QubitOperator(f'Z{i}')) for i in [0,1]]\n",
    "    grad_ops= sim.get_expectation_with_grad(hams,ansatz)\n",
    "    loss = ms.nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean') # 定义损失函数  \n",
    "    # f,_,grad_ansatz_params = grad_ops(X_train[0],Ansatz_params) #grad_ansatz_params\n",
    "    QuantumNet = MQLayer(grad_ops)\n",
    "    opti = Adam(QuantumNet.trainable_params(), learning_rate=0.5)     # 需要优化的是Quantumnet中可训练的参数，学习率设为0.5\n",
    "    grad_fn = ms.value_and_grad(forward_fn, None, weights=QuantumNet.trainable_params())\n",
    "    \n",
    "    value,grad_ansatz_params = grad_fn(ms.Tensor(X_train), ms.Tensor(y_train))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Mindspore_ansatz(Structure_p=stp,Ansatz_p=nnp,n_layer=3,n_qbits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.trainable_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = ms.nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean') # 定义损失函数    \n",
    "def forward_fn(x, y):\n",
    "    z = net(x)\n",
    "    print(z)\n",
    "    loss = loss_fn(z, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fn = ms.value_and_grad(forward_fn, None, weights=net.trainable_params())\n",
    "loss, grads = grad_fn(X_train[0],y_train[0])\n",
    "print(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
