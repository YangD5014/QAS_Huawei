{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Quantum/lib/python3.9/site-packages/mindquantum/simulator/__init__.py:17: UserWarning: Unable import mqvector gpu backend due to: cannot import name '_mq_vector_gpu' from partially initialized module 'mindquantum' (most likely due to a circular import) (/opt/miniconda3/envs/Quantum/lib/python3.9/site-packages/mindquantum/__init__.py)\n",
      "  from .available_simulator import SUPPORTED_SIMULATOR\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
      "Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mindquantum.core.gates import RX, RY, RZ, H, X, Y, Z, CNOT\n",
    "from mindquantum.core.circuit import Circuit\n",
    "import mindspore as ms\n",
    "from mindquantum.simulator import  Simulator\n",
    "from mindquantum.core.gates import GroupedPauli\n",
    "from mindquantum.core.operators import TimeEvolution,QubitOperator\n",
    "from mindquantum.core.parameterresolver import PRConvertible,PRGenerator,ParameterResolver\n",
    "from DQAS_tool import generate_pauli_string,one_hot\n",
    "from mindquantum.core.gates import RotPauliString\n",
    "from mindquantum.core.gates import UnivMathGate\n",
    "from mindspore import Tensor, ops\n",
    "from mindquantum.core.circuit import UN\n",
    "from mindquantum.core.operators import Hamiltonian             # 导入Hamiltonian模块，用于构建哈密顿量\n",
    "from mindquantum.framework import MQLayer\n",
    "from mindspore.nn import  TrainOneStepCell\n",
    "from mindspore.nn import SoftmaxCrossEntropyWithLogits                         # 导入SoftmaxCrossEntropyWithLogits模块，用于定义损失函数\n",
    "from mindspore.nn import Adam                                                  # 导入Adam模块用于定义优化参数\n",
    "from mindspore.train import Accuracy, Model, LossMonitor                       # 导入Accuracy模块，用于评估预测准确率\n",
    "import mindspore as ms\n",
    "from mindspore import Parameter, Tensor\n",
    "from mindspore.dataset import NumpySlicesDataset\n",
    "from torch.utils.data import DataLoader# 导入NumpySlicesDataset模块，用于创建模型可以识别的数据集\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from data_processing import X_train,X_test,y_train,y_test\n",
    "from mindquantum.algorithm.nisq import HardwareEfficientAnsatz   \n",
    "\n",
    "from DQAS_tool import Mindspore_ansatz,loss_fn,vag_nnp,sampling_from_structure,vag_nnp,sampling_from_structure\n",
    "from mindquantum.framework import MQOps\n",
    "import mindspore.nn as nn\n",
    "import numpy as np\n",
    "import tensorcircuit as tc\n",
    "import tensorflow as tf\n",
    "  \n",
    "num_layer = 6\n",
    "# 定义标准差和形状\n",
    "stddev = 0.02\n",
    "shape_parametized = 12\n",
    "shape_unparametized = 4\n",
    "shape_nnp = (num_layer, shape_parametized)\n",
    "shape_stp = (num_layer, shape_parametized+shape_unparametized)\n",
    "\n",
    "shape_stp = (num_layer, shape_parametized)\n",
    "\n",
    "rtype = np.float64\n",
    "ctype = np.complex128\n",
    "# 使用 numpy 生成随机数矩阵\n",
    "np.random.seed(10)\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp).astype(rtype)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp).astype(rtype)\n",
    "# #Operator Pool\n",
    "unbound_opeartor_pool = [generate_pauli_string(n=8,seed=i)[0] for i in range(shape_parametized)]\n",
    "bound_opeartor_pool = [generate_pauli_string(n=8,seed=i)[1] for i in range(shape_parametized,shape_parametized+shape_unparametized)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.numpy as mnp\n",
    "def nmf_gradient(structures:np.array, oh:ms.Tensor,num_layer: int,size_pool:int):\n",
    "    \"\"\"\n",
    "    使用 MindSpore 实现蒙特卡洛梯度计算。\n",
    "    \"\"\"\n",
    "      # Step 1: 获取选择的索引\n",
    "    choice = ops.Argmax(axis=-1)(oh)\n",
    "    # Step 2: 计算概率\n",
    "    softmax = ops.Softmax(axis=-1)\n",
    "    prob = softmax(ms.Tensor(structures))\n",
    "    # Step 3: 获取概率矩阵中的值\n",
    "    indices = mnp.stack((mnp.arange(num_layer, dtype=ms.int64), choice), axis=1)\n",
    "    prob = ops.GatherNd()(prob, indices)\n",
    "    # Step 4: 变换概率矩阵\n",
    "    prob = prob.reshape(-1, 1)\n",
    "    prob = ops.Tile()(prob, (1, size_pool))\n",
    "    \n",
    "    # Step 5: 生成蒙特卡洛梯度\n",
    "    gradient = ops.TensorScatterAdd()(Tensor(-prob, ms.float64), indices, mnp.ones((num_layer,), dtype=ms.float64))\n",
    "    return gradient\n",
    "    \n",
    "    \n",
    "# 对向量化版本的封装\n",
    "# nmf_gradient_vmap = ops.vmap(nmf_gradient, in_axes=(None, 0, None, None))\n",
    "\n",
    "def best_from_structure(structures: np.array)->Tensor:\n",
    "    return ops.Argmax(axis=-1)(ms.Tensor(structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I0 I1 X2 Z3 I4 I5 Z6 I7',\n",
       " 'Y0 X1 Z2 X3 I4 I5 I6 I7',\n",
       " 'X0 X1 X2 Z3 Y4 Z5 Z6 Y7',\n",
       " 'Y0 Y1 Z2 I3 X4 X5 I6 Z7',\n",
       " 'Y0 Z1 X2 I3 I4 Y5 X6 X7',\n",
       " 'Z0 Z1 X2 I3 Y4 X5 Y6 X7',\n",
       " 'X0 I1 Z2 X3 X4 Y5 I6 Z7',\n",
       " 'Z0 Y1 I2 X3 X4 X5 Z6 X7',\n",
       " 'Y0 Z1 I2 Y3 Y4 X5 X6 Y7',\n",
       " 'I0 Z1 Z2 Y3 Y4 X5 Z6 I7',\n",
       " 'X0 I1 I2 X3 Y4 I5 I6 Z7',\n",
       " 'I0 I1 I2 Y3 Y4 I5 Y6 X7']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unbound_opeartor_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------epoch 0-----------\n",
      "batched average loss:  0.9081669\n",
      "strcuture parameter: \n",
      " [[-0.15093987 -0.09307831 -0.10991064  0.14375304  0.10871103  0.13696269\n",
      "   0.13373923 -0.1506358   0.12297179 -0.11367219 -0.13147277  0.11278609]\n",
      " [-0.11126413 -0.12073264  0.09988689 -0.10814521  0.12454894  0.12485383\n",
      "  -0.11805901 -0.11099384 -0.1256943   0.10261504 -0.13482433  0.14210409]\n",
      " [ 0.07962673 -0.10917706 -0.14883385 -0.15216886 -0.14011925  0.1147135\n",
      "   0.13459126  0.08601307  0.15346261  0.14325558  0.11721266  0.11418458]\n",
      " [-0.13904486 -0.10823294  0.12135728  0.14823083  0.10625695  0.13095147\n",
      "  -0.12071948  0.10304922 -0.08192812  0.1254643   0.13227715 -0.14135917]\n",
      " [-0.13442656  0.12166407  0.10818468 -0.10795138  0.12874859  0.10434269\n",
      "  -0.11611543 -0.11991133  0.11669978 -0.14296807  0.10327111  0.12420161]\n",
      " [ 0.14015985  0.10047664 -0.09563024  0.12353309  0.0945932   0.12646042\n",
      "   0.13759572  0.12527749 -0.07359132 -0.11381439 -0.10212258 -0.11977328]] \n",
      " network parameter: \n",
      " [[ 0.00390302 -0.02770985 -0.08244246 -0.06773999  0.06663828 -0.01390338\n",
      "   0.07018427  0.06827384 -0.03299341 -0.00172483 -0.06469917  0.0596936 ]\n",
      " [-0.01431354 -0.04906472 -0.04111832 -0.11957425  0.08435369 -0.03539656\n",
      "   0.0691904   0.07323533 -0.10586012 -0.03203239 -0.06661979  0.03854511]\n",
      " [-0.00489362 -0.07029629 -0.09420304 -0.04772425  0.08199487 -0.02058139\n",
      "   0.04571     0.0547692  -0.04893072  0.00435822 -0.03981432  0.05127775]\n",
      " [-0.04395074 -0.07357711 -0.04929401 -0.04508054  0.10449262 -0.01481689\n",
      "   0.06486605  0.0564401  -0.06805366  0.00525636 -0.05288593  0.05393671]\n",
      " [-0.04041523 -0.06245933 -0.06779752 -0.03482744  0.07893394 -0.03316085\n",
      "   0.0833183   0.04854807 -0.05723973  0.01598732 -0.08045504  0.06203188]\n",
      " [-0.02974681 -0.09882401 -0.04023933 -0.05940136  0.04354336 -0.03238776\n",
      "   0.05193895  0.08683008 -0.05186031 -0.01157206 -0.07340105  0.00211624]]\n",
      "best candidates so far: [ 3 11  8  3  4  0]\n",
      "epoch: 1 step: 20, loss is 0.6048343777656555\n",
      "epoch: 1 step: 40, loss is 0.7059780359268188\n",
      "epoch: 2 step: 11, loss is 0.6920477747917175\n",
      "epoch: 2 step: 31, loss is 0.5154622793197632\n",
      "epoch: 3 step: 2, loss is 0.6701983213424683\n",
      "epoch: 3 step: 22, loss is 0.5372308492660522\n",
      "epoch: 3 step: 42, loss is 0.5431520342826843\n",
      "epoch: 4 step: 13, loss is 0.46864885091781616\n",
      "epoch: 4 step: 33, loss is 0.44380983710289\n",
      "epoch: 5 step: 4, loss is 0.5350371599197388\n",
      "epoch: 5 step: 24, loss is 0.5560908913612366\n",
      "epoch: 5 step: 44, loss is 0.6522330045700073\n",
      "epoch: 6 step: 15, loss is 0.5052977800369263\n",
      "epoch: 6 step: 35, loss is 0.5499836802482605\n",
      "epoch: 7 step: 6, loss is 0.6030213832855225\n",
      "epoch: 7 step: 26, loss is 0.5492084622383118\n",
      "epoch: 7 step: 46, loss is 0.5152297019958496\n",
      "epoch: 8 step: 17, loss is 0.4211757183074951\n",
      "epoch: 8 step: 37, loss is 0.5210688710212708\n",
      "epoch: 9 step: 8, loss is 0.729620099067688\n",
      "epoch: 9 step: 28, loss is 0.6054342985153198\n",
      "epoch: 9 step: 48, loss is 0.5352886915206909\n",
      "epoch: 10 step: 19, loss is 0.5675187706947327\n",
      "epoch: 10 step: 39, loss is 0.9013776779174805\n",
      "epoch: 11 step: 10, loss is 0.5949681997299194\n",
      "epoch: 11 step: 30, loss is 0.6288268566131592\n",
      "epoch: 12 step: 1, loss is 0.6048967838287354\n",
      "epoch: 12 step: 21, loss is 0.679344117641449\n",
      "epoch: 12 step: 41, loss is 0.47094234824180603\n",
      "epoch: 13 step: 12, loss is 0.6704117059707642\n",
      "epoch: 13 step: 32, loss is 0.5457038879394531\n",
      "epoch: 14 step: 3, loss is 0.4839800000190735\n",
      "epoch: 14 step: 23, loss is 0.4767431616783142\n",
      "epoch: 14 step: 43, loss is 0.5025757551193237\n",
      "epoch: 15 step: 14, loss is 0.5636670589447021\n",
      "epoch: 15 step: 34, loss is 0.6514253616333008\n",
      "目前的二分类精度是: 0.7136929460580913\n",
      "----------epoch 2-----------\n",
      "batched average loss:  0.8956672\n",
      "strcuture parameter: \n",
      " [[-0.29447102 -0.23963145 -0.25608605  0.30238125  0.25453066  0.27554075\n",
      "   0.27767671 -0.29500573  0.26630593 -0.25575417 -0.27828601  0.25584087]\n",
      " [-0.25485945 -0.26641999  0.24467212 -0.24772697  0.26815951  0.27730709\n",
      "  -0.26496413 -0.25625319 -0.26854898  0.24503029 -0.27919801  0.2849011 ]\n",
      " [ 0.22567238 -0.2512174  -0.29193407 -0.29568011 -0.28435767  0.24323798\n",
      "   0.27875118  0.22822422  0.29283895  0.28733627  0.26141073  0.25787435]\n",
      " [-0.28268849 -0.25097556  0.26312113  0.29150882  0.24839291  0.27267553\n",
      "  -0.26193891  0.24463227 -0.22536671  0.28104521  0.28441569 -0.28442861]\n",
      " [-0.27911776  0.26454423  0.27260659 -0.24977594  0.27261524  0.2477289\n",
      "  -0.26333502 -0.26264907  0.25871723 -0.28591161  0.24476218  0.26609042]\n",
      " [ 0.28120657  0.25104768 -0.23663393  0.26164138  0.23919736  0.26945337\n",
      "   0.26903096  0.26703462 -0.21835211 -0.25580862 -0.24852838 -0.26241896]] \n",
      " network parameter: \n",
      " [[-0.08626141 -0.14209764 -0.18021777 -0.16374912  0.17004535 -0.03204219\n",
      "   0.16811844  0.17310781 -0.12853255 -0.09389087 -0.16185814  0.15837692]\n",
      " [-0.10647063 -0.16328612 -0.13718406 -0.21729872  0.18753391 -0.02226017\n",
      "   0.16762185  0.17785518 -0.20287587 -0.10902725 -0.16436925  0.13470804]\n",
      " [-0.04479783 -0.18405778 -0.19126988 -0.14457878  0.18836824 -0.00671841\n",
      "   0.14284234  0.15828366 -0.14367746 -0.04996101 -0.13714494  0.14414561]\n",
      " [-0.04901205 -0.18858578 -0.14594375 -0.14034635  0.20951683  0.00239931\n",
      "   0.16332611  0.16014509 -0.16402964 -0.03933969 -0.14933232  0.1318824 ]\n",
      " [ 0.03138331 -0.17517078 -0.16521292 -0.12926317  0.18336476 -0.01235685\n",
      "   0.18256738  0.15273215 -0.15216652 -0.05439172 -0.17780554  0.13583329]\n",
      " [ 0.05465571 -0.21208685 -0.13665112 -0.15544707  0.14676034 -0.00260575\n",
      "   0.15014326  0.19076915 -0.14733506 -0.10021204 -0.17095517  0.00211624]]\n",
      "best candidates so far: [ 3 11  8  3  4  0]\n",
      "----------epoch 4-----------\n",
      "batched average loss:  0.8677079\n",
      "strcuture parameter: \n",
      " [[-0.39169983 -0.34400704 -0.3630946   0.43740439  0.35719835  0.41930981\n",
      "   0.37113071 -0.39787419  0.36173343 -0.35423109 -0.38376502  0.35010034]\n",
      " [-0.35088592 -0.36651843  0.34702304 -0.34290708  0.36345369  0.37826772\n",
      "  -0.36619176 -0.35781005 -0.3646736   0.3506778  -0.37921208  0.38023075]\n",
      " [ 0.32213469 -0.34511863 -0.3874235  -0.39136381 -0.38556105  0.28804936\n",
      "   0.37928588  0.32834752  0.37875545  0.37757691  0.38595237  0.35532882]\n",
      " [-0.38594976 -0.34605407  0.35849073  0.38394641  0.34509726  0.36536138\n",
      "  -0.35712542  0.33826588 -0.31665537  0.43748356  0.3973503  -0.38355369]\n",
      " [-0.38542962  0.36368046  0.35640957 -0.34403699  0.37100154  0.34054638\n",
      "  -0.36728748 -0.35792885  0.36590463 -0.38432698  0.33959688  0.35875884]\n",
      " [ 0.34787908  0.37204502 -0.32511576  0.38828165  0.3358254   0.36257033\n",
      "   0.34978227  0.36150172 -0.31972991 -0.34819274 -0.35160246 -0.35863951]] \n",
      " network parameter: \n",
      " [[-0.18071895 -0.25562226 -0.28692924 -0.2706805   0.27932176 -0.07722023\n",
      "   0.27580494  0.28192951 -0.23430578 -0.19575453 -0.26809188  0.26049021]\n",
      " [-0.20188431 -0.27667931 -0.243263   -0.32529263  0.29722908 -0.01800956\n",
      "   0.27422351  0.28646962 -0.30922356 -0.06757313 -0.27098609  0.15134489]\n",
      " [-0.11206614 -0.29751626 -0.29767212 -0.25047509  0.2989026  -0.00133364\n",
      "   0.24951877  0.2651621  -0.2491655   0.01145866 -0.24238001  0.0973419 ]\n",
      " [-0.07402572 -0.30268467 -0.25190867 -0.24736942  0.31847962  0.01389631\n",
      "   0.27179892  0.26780986 -0.26953138  0.03011392 -0.25575759  0.06836009]\n",
      " [-0.02831598 -0.2875921  -0.2727919  -0.23589713  0.29214207  0.01160918\n",
      "   0.28943558  0.26131333 -0.2560146  -0.00512285 -0.28337411  0.07083476]\n",
      " [-0.01112189 -0.32511262 -0.24316776 -0.26113385  0.25646553  0.0404238\n",
      "   0.25665211  0.29850109 -0.25310616 -0.20173349 -0.27730204  0.00211624]]\n",
      "best candidates so far: [ 3 11 10  9  4  3]\n",
      "----------epoch 6-----------\n",
      "batched average loss:  0.82516235\n",
      "strcuture parameter: \n",
      " [[-0.46882779 -0.42959387 -0.45881558  0.61274704  0.43715249  0.54110792\n",
      "   0.43928379 -0.48754545  0.43486906 -0.44096543 -0.47397726  0.418027  ]\n",
      " [-0.43129128 -0.445799    0.42355444 -0.41433985  0.43780946  0.47302042\n",
      "  -0.44067279 -0.44652157 -0.4405708   0.4048259  -0.45830503  0.4539002 ]\n",
      " [ 0.40631238 -0.41835196 -0.46148046 -0.46737105 -0.46680511  0.27878838\n",
      "   0.45505804  0.39436976  0.44966212  0.44502241  0.51962014  0.42636634]\n",
      " [-0.4777441  -0.41649264  0.43556242  0.45494589  0.41980791  0.43175032\n",
      "  -0.42965502  0.41137827 -0.39575751  0.53186527  0.50935863 -0.46651625]\n",
      " [-0.46686084  0.44504752  0.4680603  -0.41911735  0.44505837  0.40678029\n",
      "  -0.45128948 -0.43313047  0.44904891 -0.46423717  0.40682641  0.43060548]\n",
      " [ 0.40905391  0.47527007 -0.40232375  0.55254668  0.40791002  0.43398743\n",
      "   0.34334092  0.43460435 -0.39484351 -0.42557855 -0.44581497 -0.4350215 ]] \n",
      " network parameter: \n",
      " [[-0.27896235 -0.37011235 -0.39770771 -0.38096064  0.39216362 -0.12378704\n",
      "   0.3873154   0.39387592 -0.34454728 -0.29897281 -0.37880936  0.3289329 ]\n",
      " [-0.30476325 -0.39110895 -0.35423605 -0.43672054  0.4098764  -0.00710485\n",
      "   0.38492636  0.39863594 -0.41967233 -0.11226515 -0.38184897  0.08044848]\n",
      " [-0.06208872 -0.41226995 -0.408679   -0.3612441   0.4120041   0.0009197\n",
      "   0.36080538  0.37686681 -0.35861389 -0.01932942 -0.35214702  0.01037621]\n",
      " [-0.01669884 -0.41733874 -0.36215175 -0.35778659  0.43107496 -0.00120022\n",
      "   0.38359928  0.37889721 -0.37969343  0.00849437 -0.36634035 -0.02360477]\n",
      " [ 0.0111136  -0.40156074 -0.38361894 -0.34717599  0.40478647 -0.00567631\n",
      "   0.40021578  0.37344502 -0.36598784 -0.03708811 -0.39410095 -0.01874911]\n",
      " [ 0.01779252 -0.43956569 -0.35436686 -0.37087409  0.36941283 -0.01380739\n",
      "   0.36791573  0.40972782 -0.36229192 -0.30401354 -0.38806294  0.00211624]]\n",
      "best candidates so far: [ 3  5 10  9  2  3]\n",
      "----------epoch 8-----------\n",
      "batched average loss:  0.78255296\n",
      "strcuture parameter: \n",
      " [[-0.53214185 -0.50027232 -0.53058766  0.79535407  0.50525811  0.61777007\n",
      "   0.49341071 -0.57236402  0.49129051 -0.52118472 -0.56265495  0.47088749]\n",
      " [-0.49809803 -0.51683175  0.49131216 -0.48507163  0.50441808  0.5207841\n",
      "  -0.51096606 -0.53712781 -0.51257616  0.43107616 -0.52523792  0.51131053]\n",
      " [ 0.48106048 -0.48531404 -0.52339168 -0.53277099 -0.54146389  0.25975672\n",
      "   0.51671753  0.44400207  0.50636515  0.51183474  0.61379769  0.48117128]\n",
      " [-0.57461604 -0.4756216   0.50256061  0.51682966  0.47277278  0.48512712\n",
      "  -0.49148994  0.46786031 -0.47509716  0.61909609  0.6254376  -0.54270048]\n",
      " [-0.55042378  0.51907309  0.57296373 -0.48005551  0.51003667  0.45059266\n",
      "  -0.51579374 -0.49505554  0.52191726 -0.53200033  0.45984524  0.48638632]\n",
      " [ 0.42158925  0.61847403 -0.4813598   0.69308364  0.4659077   0.48699793\n",
      "   0.27896364  0.49603891 -0.46494245 -0.49707756 -0.54267616 -0.49910077]] \n",
      " network parameter: \n",
      " [[-0.37984976 -0.48562974 -0.5109535  -0.49397823  0.50602458 -0.14733256\n",
      "   0.50075677  0.50803363 -0.45707537 -0.40620343 -0.49185017  0.36630962]\n",
      " [-0.40750305 -0.50655405 -0.46771557 -0.5502064   0.5237017   0.01015574\n",
      "   0.49766038  0.51247689 -0.53247626 -0.07501186 -0.49504518  0.01157744]\n",
      " [-0.08021621 -0.52771856 -0.52220152 -0.47449321  0.52584784  0.01012634\n",
      "   0.4741087   0.49081989 -0.47089674  0.01997459 -0.46422252 -0.06009061]\n",
      " [-0.03255582 -0.53280147 -0.4749026  -0.4708564   0.54472023 -0.00357065\n",
      "   0.49716953  0.49219487 -0.4923191   0.04037302 -0.4790211  -0.08990589]\n",
      " [-0.01740867 -0.51677139 -0.49708551 -0.46064929  0.51872214 -0.00957028\n",
      "   0.5133082   0.48762896 -0.47869229  0.00724702 -0.50710393 -0.07433222]\n",
      " [-0.01237424 -0.55411335 -0.46797406 -0.4833413   0.48362348 -0.04223238\n",
      "   0.48157549  0.52301047 -0.47440926 -0.40828609 -0.50110641  0.00211624]]\n",
      "best candidates so far: [ 3  5 10 10  2  3]\n",
      "----------epoch 10-----------\n",
      "batched average loss:  0.74496764\n",
      "strcuture parameter: \n",
      " [[-0.58759917 -0.57195151 -0.607854    1.00375782  0.56579723  0.63541174\n",
      "   0.53534386 -0.66252264  0.53808845 -0.60784792 -0.65521901  0.50888417]\n",
      " [-0.557225   -0.57824316  0.55031289 -0.54997448  0.55790646  0.5488764\n",
      "  -0.58245253 -0.62489556 -0.58072992  0.50128821 -0.58774774  0.56105546]\n",
      " [ 0.54594533 -0.54634547 -0.57863865 -0.59025563 -0.61119531  0.20546171\n",
      "   0.56993165  0.50177707  0.55882914  0.55991518  0.67051308  0.52355699]\n",
      " [-0.6687856  -0.52586953  0.56051349  0.56849559  0.51906678  0.52998645\n",
      "  -0.54609976  0.51132886 -0.55439654  0.70363632  0.73108478 -0.61453583]\n",
      " [-0.63698073  0.58181059  0.70440881 -0.53273553  0.56644407  0.48147915\n",
      "  -0.58805566 -0.54962082  0.58673185 -0.59209315  0.50668635  0.53085261]\n",
      " [ 0.4286118   0.76660527 -0.55685074  0.85271058  0.50971896  0.52657648\n",
      "   0.2399517   0.54314206 -0.53355759 -0.56526365 -0.63544673 -0.55276202]] \n",
      " network parameter: \n",
      " [[-0.48547418 -0.60003773 -0.62422897 -0.60775811  0.61668007 -0.15911936\n",
      "   0.614343    0.62186546 -0.56964428 -0.51670314 -0.60362673  0.4264355 ]\n",
      " [-0.51278655 -0.62131207 -0.58165795 -0.66367797  0.63322681  0.01529296\n",
      "   0.60932729  0.62632092 -0.64442014 -0.06024792 -0.60652933  0.00802863]\n",
      " [-0.14595658 -0.64219048 -0.63596902 -0.58797922  0.6362043   0.00743647\n",
      "   0.58722913  0.60551069 -0.58276899  0.04082667 -0.57439341 -0.05088778]\n",
      " [-0.08578905 -0.64752807 -0.58833947 -0.58420917  0.6552854  -0.00464846\n",
      "   0.6097773   0.60545646 -0.60394281  0.03500754 -0.58996615 -0.07978849]\n",
      " [-0.07482426 -0.63102882 -0.61158221 -0.57388668  0.62948959 -0.01049858\n",
      "   0.6250158   0.60146108 -0.59059194  0.02966475 -0.61918948 -0.04988597]\n",
      " [-0.05745834 -0.66818869 -0.5818318  -0.59622407  0.59392153 -0.04339821\n",
      "   0.59555415  0.63593086 -0.5858822  -0.51750771 -0.61277036  0.00211624]]\n",
      "best candidates so far: [ 3 11 10 10  2  3]\n",
      "epoch: 1 step: 20, loss is 0.595525860786438\n",
      "epoch: 1 step: 40, loss is 0.6920354962348938\n",
      "epoch: 2 step: 11, loss is 0.6686005592346191\n",
      "epoch: 2 step: 31, loss is 0.5230880975723267\n",
      "epoch: 3 step: 2, loss is 0.6697254180908203\n",
      "epoch: 3 step: 22, loss is 0.5380865335464478\n",
      "epoch: 3 step: 42, loss is 0.5399667024612427\n",
      "epoch: 4 step: 13, loss is 0.47521457076072693\n",
      "epoch: 4 step: 33, loss is 0.4448532164096832\n",
      "epoch: 5 step: 4, loss is 0.5356670618057251\n",
      "epoch: 5 step: 24, loss is 0.5573278665542603\n",
      "epoch: 5 step: 44, loss is 0.6522585153579712\n",
      "epoch: 6 step: 15, loss is 0.5058647394180298\n",
      "epoch: 6 step: 35, loss is 0.5490736961364746\n",
      "epoch: 7 step: 6, loss is 0.6000157594680786\n",
      "epoch: 7 step: 26, loss is 0.548587441444397\n",
      "epoch: 7 step: 46, loss is 0.5158213376998901\n",
      "epoch: 8 step: 17, loss is 0.421375572681427\n",
      "epoch: 8 step: 37, loss is 0.5201526880264282\n",
      "epoch: 9 step: 8, loss is 0.7297791242599487\n",
      "epoch: 9 step: 28, loss is 0.6059272289276123\n",
      "epoch: 9 step: 48, loss is 0.5352960228919983\n",
      "epoch: 10 step: 19, loss is 0.5678616166114807\n",
      "epoch: 10 step: 39, loss is 0.8996948003768921\n",
      "epoch: 11 step: 10, loss is 0.5953639149665833\n",
      "epoch: 11 step: 30, loss is 0.6293978691101074\n",
      "epoch: 12 step: 1, loss is 0.604820191860199\n",
      "epoch: 12 step: 21, loss is 0.681201696395874\n",
      "epoch: 12 step: 41, loss is 0.471809446811676\n",
      "epoch: 13 step: 12, loss is 0.670520007610321\n",
      "epoch: 13 step: 32, loss is 0.5457342863082886\n",
      "epoch: 14 step: 3, loss is 0.483945369720459\n",
      "epoch: 14 step: 23, loss is 0.4749101996421814\n",
      "epoch: 14 step: 43, loss is 0.50251704454422\n",
      "epoch: 15 step: 14, loss is 0.5630839467048645\n",
      "epoch: 15 step: 34, loss is 0.6512729525566101\n",
      "目前的二分类精度是: 0.7178423236514523\n",
      "----------epoch 12-----------\n",
      "batched average loss:  0.7133721\n",
      "strcuture parameter: \n",
      " [[-0.63635558 -0.65221763 -0.6875837   1.21656099  0.61990235  0.60243786\n",
      "   0.57641336 -0.7510725   0.57741947 -0.69811001 -0.75102178  0.53513132]\n",
      " [-0.61270878 -0.63457798  0.61129439 -0.617154    0.60311005  0.57056121\n",
      "  -0.66328147 -0.71742735 -0.65215235  0.57137914 -0.64941693  0.60434541]\n",
      " [ 0.60762449 -0.59824343 -0.63034483 -0.64150882 -0.6741306   0.12571721\n",
      "   0.60816232  0.55510995  0.60014982  0.59217231  0.75698871  0.55841023]\n",
      " [-0.75892099 -0.57032329  0.61354794  0.61296112  0.5566915   0.56253375\n",
      "  -0.59024128  0.54455534 -0.63525872  0.80407826  0.85144845 -0.6794553 ]\n",
      " [-0.72246247  0.63258216  0.87334192 -0.57803997  0.61339986  0.50388238\n",
      "  -0.66564946 -0.59711408  0.63822036 -0.64485579  0.54801291  0.56387492]\n",
      " [ 0.40842606  0.93358183 -0.63554952  1.0193328   0.54450454  0.55444001\n",
      "   0.18517235  0.58052215 -0.60670912 -0.63128143 -0.7237559  -0.59963786]] \n",
      " network parameter: \n",
      " [[-0.59523252 -0.70967044 -0.73445816 -0.71833641  0.71744661 -0.19859176\n",
      "   0.724506    0.73115279 -0.67739649 -0.62883005 -0.70854809  0.46716463]\n",
      " [-0.62146848 -0.73238988 -0.6936089  -0.77336892  0.73352409  0.00328663\n",
      "   0.71517012  0.73612728 -0.75002691 -0.07388297 -0.71032287  0.02089263]\n",
      " [-0.21117704 -0.75341439 -0.74699512 -0.69804397  0.73670295 -0.01515913\n",
      "   0.69446885  0.71674426 -0.6883096   0.04642488 -0.67814186 -0.0198962 ]\n",
      " [-0.12387435 -0.758603   -0.69948769 -0.6944967   0.75618012 -0.02165713\n",
      "   0.71778872  0.71493805 -0.70909156  0.03066592 -0.69367416 -0.04807828]\n",
      " [-0.08015225 -0.74156351 -0.7249277  -0.68369387  0.73095985 -0.0269519\n",
      "   0.7311896   0.71188139 -0.69628555  0.05145646 -0.72330462 -0.00642853]\n",
      " [-0.0416603  -0.77993271 -0.69271039 -0.70619294  0.6950876  -0.06071813\n",
      "   0.70492834  0.7455342  -0.69150835 -0.62937537 -0.71803998  0.00211624]]\n",
      "best candidates so far: [ 3  2 10 10  2  3]\n",
      "----------epoch 14-----------\n",
      "batched average loss:  0.697776\n",
      "strcuture parameter: \n",
      " [[-0.67925717 -0.73271699 -0.7631661   1.42442588  0.66863821  0.55749274\n",
      "   0.60674992 -0.83664536  0.60946006 -0.78533766 -0.84291171  0.55363684]\n",
      " [-0.66227389 -0.68598002  0.67308643 -0.68308728  0.64066851  0.57949215\n",
      "  -0.74372766 -0.80391726 -0.71938105  0.62673891 -0.70658541  0.64092399]\n",
      " [ 0.6610381  -0.6401771  -0.67431657 -0.68450387 -0.73027686  0.02955997\n",
      "   0.63840149  0.60660945  0.6369789   0.61328407  0.81470229  0.58812758]\n",
      " [-0.84157777 -0.60788234  0.66265824  0.65087982  0.58570931  0.58712789\n",
      "  -0.62949931  0.5706416  -0.71170564  0.91542603  0.95765315 -0.73738686]\n",
      " [-0.80191691  0.68029308  1.03829468 -0.61686275  0.64928954  0.52173955\n",
      "  -0.73732824 -0.63833673  0.68738226 -0.69244881  0.57553343  0.58889504]\n",
      " [ 0.3451925   1.10883811 -0.70810064  1.19114465  0.56790795  0.57359612\n",
      "   0.09758099  0.61320546 -0.67879167 -0.6923899  -0.81070353 -0.64164343]] \n",
      " network parameter: \n",
      " [[-0.70853116 -0.81195329 -0.83794163 -0.82177922  0.80212333 -0.1965941\n",
      "   0.82554773  0.8321887  -0.77584917 -0.73963055 -0.80210295  0.46324729]\n",
      " [-0.73283735 -0.83701408 -0.80019365 -0.87579286  0.81743807  0.03073671\n",
      "   0.8113696   0.83826887 -0.84548717 -0.08166    -0.80189492 -0.00167952]\n",
      " [-0.29052114 -0.85890724 -0.85249888 -0.80158435  0.82067295  0.00343088\n",
      "   0.79106236  0.82092712 -0.78325265  0.08508608 -0.76900504 -0.02322212]\n",
      " [-0.17900014 -0.86402121 -0.80458939 -0.79762523  0.84034251 -0.00488178\n",
      "   0.81510451  0.81779777 -0.80333673  0.07876985 -0.78459252 -0.04912752]\n",
      " [-0.09619302 -0.84665731 -0.83254855 -0.78621034  0.81676827 -0.02030149\n",
      "   0.82792829  0.81505104 -0.79217206  0.11317906 -0.81461568  0.0091048 ]\n",
      " [-0.02707851 -0.88638963 -0.79757376 -0.8096991   0.78122761 -0.08425532\n",
      "   0.80523757  0.84936313 -0.78785425 -0.74371313 -0.81177393  0.00211624]]\n",
      "best candidates so far: [ 3  2 10 10  2  3]\n",
      "----------epoch 16-----------\n",
      "batched average loss:  0.68477786\n",
      "strcuture parameter: \n",
      " [[-0.71798078 -0.81115091 -0.83451821  1.63628196  0.70297431  0.48669527\n",
      "   0.62723262 -0.92040171  0.63543373 -0.86970579 -0.93035096  0.56491073]\n",
      " [-0.70737641 -0.72978645  0.7278771  -0.74285567  0.6715313   0.59359196\n",
      "  -0.81572741 -0.8837938  -0.78318264  0.67523698 -0.75769749  0.67224193]\n",
      " [ 0.71099435 -0.67436358 -0.71221381 -0.72150017 -0.78097022 -0.07760396\n",
      "   0.66304166  0.64937551  0.66697808  0.62930693  0.87000043  0.61217837]\n",
      " [-0.91767783 -0.63883879  0.70466612  0.68386021  0.60608637  0.60632347\n",
      "  -0.66333958  0.59285384 -0.78318286  1.00457097  1.06408675 -0.78980287]\n",
      " [-0.87484653  0.72902021  1.20173988 -0.65124315  0.67350854  0.53489254\n",
      "  -0.80323984 -0.67542493  0.72550078 -0.73497713  0.59203976  0.61059455]\n",
      " [ 0.26264967  1.2879773  -0.77636241  1.34455769  0.58510722  0.58597645\n",
      "  -0.00809912  0.64046006 -0.75155418 -0.74912757 -0.89621855 -0.67914306]] \n",
      " network parameter: \n",
      " [[-0.82285334 -0.90526305 -0.93230913 -0.91576113  0.86685957 -0.22347237\n",
      "   0.91487663  0.9194809  -0.8612172  -0.85333342 -0.88184282  0.42968608]\n",
      " [-0.84753898 -0.93318128 -0.89766556 -0.96849239  0.88058512  0.04028409\n",
      "   0.89560337  0.92721812 -0.92700021 -0.12430123 -0.87854874 -0.05580179]\n",
      " [-0.38487087 -0.95677286 -0.94929466 -0.89556668  0.88400301  0.00148533\n",
      "   0.87452127  0.91160696 -0.86342898  0.14228658 -0.84401757 -0.05730288]\n",
      " [-0.25342636 -0.96228735 -0.90111308 -0.89103734  0.90391056 -0.02484416\n",
      "   0.8991      0.90943026 -0.88320408  0.15126999 -0.85950268 -0.08033077]\n",
      " [-0.13543268 -0.94518648 -0.93114358 -0.87830945  0.88254635 -0.06169774\n",
      "   0.91274456  0.90620697 -0.87378943  0.19749755 -0.89031616 -0.00361967]\n",
      " [-0.03406701 -0.98540014 -0.89326303 -0.90317404  0.84796515 -0.15124589\n",
      "   0.89449942  0.94220572 -0.87131164 -0.86028837 -0.89143402  0.00211624]]\n",
      "best candidates so far: [ 3  2 10 10  2  3]\n",
      "----------epoch 18-----------\n",
      "batched average loss:  0.68059045\n",
      "strcuture parameter: \n",
      " [[-0.75096309 -0.88154733 -0.8987735   1.83046963  0.72546108  0.42453901\n",
      "   0.63971208 -0.99464464  0.65661663 -0.94522719 -1.00897057  0.57300855]\n",
      " [-0.74577976 -0.76328199  0.77852546 -0.79489096  0.69673238  0.59718076\n",
      "  -0.87772657 -0.95310886 -0.83909287  0.69393227 -0.80106429  0.69889513]\n",
      " [ 0.75339667 -0.70272551 -0.74289881 -0.75293765 -0.82476667 -0.17988959\n",
      "   0.68418419  0.6868819   0.69361853  0.6443894   0.89714523  0.63172002]\n",
      " [-0.98364732 -0.66508077  0.74289399  0.71143198  0.61916899  0.62294125\n",
      "  -0.69200943  0.61132731 -0.84570789  1.07740305  1.15146346 -0.83506129]\n",
      " [-0.94044103  0.77603502  1.36021929 -0.68113921  0.68955142  0.54499174\n",
      "  -0.86309848 -0.70697439  0.75473891 -0.77200259  0.60207964  0.62732797]\n",
      " [ 0.16791695  1.46230761 -0.8387676   1.47931108  0.59583304  0.59415886\n",
      "  -0.11198051  0.66076292 -0.8189999  -0.80042603 -0.97424185 -0.71171683]] \n",
      " network parameter: \n",
      " [[-0.9367274  -0.98874001 -1.01641381 -0.99881196  0.91150304 -0.26887779\n",
      "   0.99167543  0.99013105 -0.93196228 -0.96948703 -0.94707499  0.39196649]\n",
      " [-0.96463727 -1.02005801 -0.98496769 -1.04944464  0.92075964  0.05429906\n",
      "   0.96677331  0.99978207 -0.99362324 -0.19889454 -0.93981694 -0.1193419 ]\n",
      " [-0.48795952 -1.04564642 -1.03640818 -0.97781146  0.92596879  0.00546538\n",
      "   0.9437032   0.98578808 -0.92786787  0.21229164 -0.90261979 -0.10317447]\n",
      " [-0.34367148 -1.05154835 -0.98828016 -0.97264084  0.94567401 -0.05072566\n",
      "   0.96908128  0.98538545 -0.94781515  0.24085096 -0.91717861 -0.11660136]\n",
      " [-0.20237675 -1.03548226 -1.02011386 -0.95846659  0.92741187 -0.11786163\n",
      "   0.98466527  0.98240806 -0.94023594  0.29548983 -0.94972684 -0.01108654]\n",
      " [-0.07893389 -1.07654167 -0.9782882  -0.98414366  0.89516333 -0.23788629\n",
      "   0.97213603  1.0210296  -0.94093481 -0.97747259 -0.95663973  0.00211624]]\n",
      "best candidates so far: [ 3  2 10 10  2  3]\n",
      "----------epoch 20-----------\n",
      "batched average loss:  0.67729914\n",
      "strcuture parameter: \n",
      " [[-0.77908848 -0.94563279 -0.95623289  2.01428574  0.73838459  0.35543236\n",
      "   0.64346536 -1.06028201  0.67215877 -1.01405565 -1.07924505  0.57802108]\n",
      " [-0.77800145 -0.7861357   0.82486259 -0.83879455  0.71654908  0.58839029\n",
      "  -0.93150032 -1.0129617  -0.88808871  0.70953587 -0.8379163   0.72125227]\n",
      " [ 0.79196079 -0.72662877 -0.76833173 -0.77963716 -0.86281974 -0.2731844\n",
      "   0.69998559  0.71650954  0.71645646  0.65752424  0.91489413  0.6485331 ]\n",
      " [-1.03921478 -0.6870117   0.77711545  0.73458194  0.62910902  0.63734973\n",
      "  -0.7158697   0.62677502 -0.90036801  1.12744911  1.22164164 -0.87370197]\n",
      " [-0.99687346  0.81810642  1.51459182 -0.7071496   0.69944989  0.55228616\n",
      "  -0.91628751 -0.73406443  0.77572162 -0.80444172  0.60686674  0.64069217]\n",
      " [ 0.06871635  1.62985421 -0.89576657  1.58861652  0.60137652  0.5989155\n",
      "  -0.21481942  0.67653379 -0.88168786 -0.84582425 -1.04366842 -0.73993572]] \n",
      " network parameter: \n",
      " [[-1.04729536 -1.06199847 -1.09032848 -1.06851074  0.93772868 -0.32228055\n",
      "   1.05592492  1.04433712 -0.98881562 -1.0881695  -0.99894776  0.37618319]\n",
      " [-1.08372085 -1.09731772 -1.06195586 -1.11784188  0.94221394  0.09299492\n",
      "   1.02498486  1.0559186  -1.04584476 -0.29239235 -0.98728277 -0.17583047]\n",
      " [-0.59776358 -1.12495912 -1.11364553 -1.04745253  0.94821033  0.04191386\n",
      "   0.99925188  1.04340472 -0.9769673   0.30099352 -0.94660894 -0.14006415]\n",
      " [-0.44571475 -1.13180581 -1.06557257 -1.04174708  0.96731005 -0.04924519\n",
      "   1.02562708  1.04507248 -0.99808969  0.34313666 -0.9593042  -0.1338515 ]\n",
      " [-0.2921594  -1.11660653 -1.09916975 -1.02581568  0.95351467 -0.16952332\n",
      "   1.04434282  1.04322755 -0.99250401  0.4020314  -0.99396771  0.0106004 ]\n",
      " [-0.15623099 -1.15862982 -1.05235499 -1.05141594  0.92358942 -0.32995951\n",
      "   1.0384093   1.0851989  -0.99732086 -1.08704969 -1.00776588  0.00211624]]\n",
      "best candidates so far: [ 3  2 10 10  2  1]\n",
      "epoch: 1 step: 20, loss is 0.5964168906211853\n",
      "epoch: 1 step: 40, loss is 0.6921425461769104\n",
      "epoch: 2 step: 11, loss is 0.6686891317367554\n",
      "epoch: 2 step: 31, loss is 0.522663414478302\n",
      "epoch: 3 step: 2, loss is 0.6735018491744995\n",
      "epoch: 3 step: 22, loss is 0.5381630659103394\n",
      "epoch: 3 step: 42, loss is 0.5396997928619385\n",
      "epoch: 4 step: 13, loss is 0.47499680519104004\n",
      "epoch: 4 step: 33, loss is 0.4448848366737366\n",
      "epoch: 5 step: 4, loss is 0.5349681377410889\n",
      "epoch: 5 step: 24, loss is 0.5574382543563843\n",
      "epoch: 5 step: 44, loss is 0.6530110239982605\n",
      "epoch: 6 step: 15, loss is 0.5066654682159424\n",
      "epoch: 6 step: 35, loss is 0.5490187406539917\n",
      "epoch: 7 step: 6, loss is 0.6002084612846375\n",
      "epoch: 7 step: 26, loss is 0.548949658870697\n",
      "epoch: 7 step: 46, loss is 0.5163158178329468\n",
      "epoch: 8 step: 17, loss is 0.421570360660553\n",
      "epoch: 8 step: 37, loss is 0.520176112651825\n",
      "epoch: 9 step: 8, loss is 0.7294198274612427\n",
      "epoch: 9 step: 28, loss is 0.6061629056930542\n",
      "epoch: 9 step: 48, loss is 0.536220908164978\n",
      "epoch: 10 step: 19, loss is 0.5677199959754944\n",
      "epoch: 10 step: 39, loss is 0.9000698328018188\n",
      "epoch: 11 step: 10, loss is 0.5953766107559204\n",
      "epoch: 11 step: 30, loss is 0.6290572881698608\n",
      "epoch: 12 step: 1, loss is 0.6047469973564148\n",
      "epoch: 12 step: 21, loss is 0.681164562702179\n",
      "epoch: 12 step: 41, loss is 0.4715573787689209\n",
      "epoch: 13 step: 12, loss is 0.6705939769744873\n",
      "epoch: 13 step: 32, loss is 0.5457719564437866\n",
      "epoch: 14 step: 3, loss is 0.4858708381652832\n",
      "epoch: 14 step: 23, loss is 0.47478538751602173\n",
      "epoch: 14 step: 43, loss is 0.5020002722740173\n",
      "epoch: 15 step: 14, loss is 0.5630381107330322\n",
      "epoch: 15 step: 34, loss is 0.6514589786529541\n",
      "目前的二分类精度是: 0.7178423236514523\n",
      "----------epoch 22-----------\n",
      "batched average loss:  0.67634493\n",
      "strcuture parameter: \n",
      " [[-0.80271658 -1.00180522 -1.00602407  2.17882948  0.74255965  0.28542017\n",
      "   0.64708459 -1.1174125   0.68335133 -1.07381205 -1.14050857  0.58107967]\n",
      " [-0.80459527 -0.80114296  0.86496168 -0.8754902   0.73121788  0.580196\n",
      "  -0.97685545 -1.0639796  -0.9301785   0.73892581 -0.86906818  0.73986493]\n",
      " [ 0.82925717 -0.74416522 -0.78893117 -0.80200691 -0.89467521 -0.34848164\n",
      "   0.71380974  0.74003631  0.73467719  0.66992684  0.91268016  0.66110305]\n",
      " [-1.08372457 -0.70464145  0.80886431  0.75447231  0.63322557  0.64938234\n",
      "  -0.73572963  0.63959465 -0.94552737  1.16868793  1.27451547 -0.90612018]\n",
      " [-1.04721513  0.85622317  1.66272803 -0.72974372  0.70193471  0.55777362\n",
      "  -0.96489098 -0.75764416  0.78868566 -0.83220788  0.60921362  0.65154378]\n",
      " [-0.03135216  1.79198314 -0.94601037  1.6560482   0.60455022  0.60226664\n",
      "  -0.31133753  0.68850738 -0.93664154 -0.88497354 -1.10616315 -0.76409778]] \n",
      " network parameter: \n",
      " [[-1.15094828 -1.12533537 -1.15422001 -1.12434903  0.94734378 -0.39381706\n",
      "   1.1078534   1.08282866 -1.03296036 -1.20868113 -1.03843848  0.37313925]\n",
      " [-1.20348605 -1.16494156 -1.12920923 -1.17377709  0.94550956  0.13028485\n",
      "   1.07106928  1.09578253 -1.08540232 -0.39721829 -1.0219903  -0.23300932]\n",
      " [-0.71268245 -1.1950096  -1.18102876 -1.10441004  0.95319139  0.0878559\n",
      "   1.04207176  1.08460579 -1.01241156  0.40270156 -0.97626436 -0.17139417]\n",
      " [-0.5556943  -1.20320318 -1.13269675 -1.09819299  0.97146843 -0.05389056\n",
      "   1.069898    1.0881266  -1.03518276  0.45241127 -0.98663562 -0.12444011]\n",
      " [-0.39202586 -1.18871953 -1.16826072 -1.08049112  0.9619993  -0.23820975\n",
      "   1.09276606  1.08887277 -1.03120915  0.51402648 -1.02515904  0.06489849]\n",
      " [-0.24947197 -1.23205955 -1.11551559 -1.10482092  0.93551014 -0.43267094\n",
      "   1.09414678  1.13511831 -1.04097608 -1.1733198  -1.04665546  0.00211624]]\n",
      "best candidates so far: [ 3  2 10 10  2  1]\n",
      "----------epoch 24-----------\n",
      "batched average loss:  0.6759107\n",
      "strcuture parameter: \n",
      " [[-0.82276367 -1.04996617 -1.04874598  2.31816397  0.74304758  0.22642907\n",
      "   0.65067538 -1.16591919  0.69246111 -1.12318437 -1.19277292  0.58339797]\n",
      " [-0.82790387 -0.81255249  0.90240576 -0.90753073  0.74096327  0.57211496\n",
      "  -1.01718183 -1.10805842 -0.96703661  0.76899003 -0.89554775  0.75621816]\n",
      " [ 0.86406353 -0.757424   -0.80508679 -0.82095617 -0.92143309 -0.4123191\n",
      "   0.72516976  0.75886508  0.74833124  0.67975907  0.90647781  0.67109658]\n",
      " [-1.12012399 -0.71877238  0.83723751  0.77137816  0.63107775  0.65953292\n",
      "  -0.75238591  0.64971165 -0.98341251  1.22540712  1.30546569 -0.93283297]\n",
      " [-1.09143599  0.89376083  1.801104   -0.74929885  0.70036815  0.56195148\n",
      "  -1.00942745 -0.77826586  0.79487057 -0.85609254  0.60684151  0.65965871]\n",
      " [-0.12454578  1.95327579 -0.99023178  1.68195451  0.60415188  0.60445636\n",
      "  -0.40250046  0.69606505 -0.98487669 -0.91882215 -1.16178004 -0.78462659]] \n",
      " network parameter: \n",
      " [[-1.24428629 -1.17987864 -1.20877053 -1.1660447   0.9421209  -0.48254989\n",
      "   1.14857425  1.10684885 -1.06493593 -1.3310748  -1.06688294  0.36426561]\n",
      " [-1.32291394 -1.22392062 -1.18713725 -1.21794145  0.93337796  0.15908801\n",
      "   1.10607502  1.12008861 -1.11330179 -0.50897186 -1.04589101 -0.30286933]\n",
      " [-0.83108353 -1.25649084 -1.23915073 -1.14920196  0.94282443  0.13543581\n",
      "   1.07378308  1.10961989 -1.03515647  0.51318515 -0.99460962 -0.19947705]\n",
      " [-0.66962844 -1.26633862 -1.19018119 -1.14229525  0.96056297 -0.06340357\n",
      "   1.1031058   1.11582628 -1.06011657  0.56506661 -1.0017653  -0.08513544]\n",
      " [-0.49907653 -1.2524911  -1.22733285 -1.12315444  0.95591971 -0.32379708\n",
      "   1.13098185  1.11988409 -1.05700017  0.6299437  -1.04493672  0.14254665]\n",
      " [-0.35318888 -1.2973621  -1.16859861 -1.14517553  0.9327179  -0.5439808\n",
      "   1.14041566  1.17150343 -1.07257846 -1.22443599 -1.07566951  0.00211624]]\n",
      "best candidates so far: [ 3  2 10 10  2  1]\n",
      "----------epoch 26-----------\n",
      "batched average loss:  0.67573076\n",
      "strcuture parameter: \n",
      " [[-0.83944354 -1.08919213 -1.08498714  2.43340816  0.74312925  0.17572904\n",
      "   0.65576246 -1.20700947  0.70020367 -1.16294145 -1.2366023   0.58484217]\n",
      " [-0.84671345 -0.82190411  0.93614307 -0.93648647  0.74620743  0.56969973\n",
      "  -1.05379802 -1.14451776 -0.99854181  0.80072747 -0.9181788   0.77107305]\n",
      " [ 0.89906026 -0.76795367 -0.81913031 -0.83702101 -0.9443335  -0.45940845\n",
      "   0.73427005  0.77332291  0.75706419  0.68864967  0.88955884  0.67932267]\n",
      " [-1.15031847 -0.7291347   0.86160941  0.78483217  0.62603826  0.66889446\n",
      "  -0.76578807  0.65742554 -1.01526672  1.27720783  1.32881862 -0.95488851]\n",
      " [-1.12756923  0.93375097  1.91292825 -0.76539915  0.69391792  0.56486377\n",
      "  -1.04659194 -0.79625102  0.79824277 -0.87607697  0.60498354  0.66627233]\n",
      " [-0.21361636  2.11895788 -1.03061414  1.65501095  0.6009226   0.60498221\n",
      "  -0.4895937   0.70064424 -1.02803686 -0.94852344 -1.21135024 -0.80233214]] \n",
      " network parameter: \n",
      " [[-1.32456336 -1.22708619 -1.25491341 -1.19306535  0.92533699 -0.58506207\n",
      "   1.1788483   1.11842642 -1.08627375 -1.45507638 -1.08561473  0.33374834]\n",
      " [-1.44285042 -1.27530714 -1.23646918 -1.25071499  0.90845206  0.18070943\n",
      "   1.13045382  1.13061934 -1.13076215 -0.62599567 -1.06041287 -0.38835013]\n",
      " [-0.95223343 -1.31050138 -1.28874071 -1.18253308  0.92046113  0.19497713\n",
      "   1.0950386   1.12083579 -1.04724313  0.62872853 -1.00325068 -0.2282196 ]\n",
      " [-0.7873455  -1.32237412 -1.2388612  -1.17465827  0.93740444 -0.08300333\n",
      "   1.12624586  1.12930741 -1.0740313   0.68244463 -1.00721298 -0.02132245]\n",
      " [-0.6118422  -1.3091552  -1.27633121 -1.15465407  0.9375176  -0.42213284\n",
      "   1.15989261  1.13787532 -1.07258358  0.74975676 -1.05582302  0.23863536]\n",
      " [-0.46420148 -1.35556272 -1.21234731 -1.17250487  0.91742135 -0.66111384\n",
      "   1.17804713  1.1962378  -1.09363405 -1.23810541 -1.0969945   0.00211624]]\n",
      "best candidates so far: [ 3  2  0 10  2  1]\n",
      "----------epoch 28-----------\n",
      "batched average loss:  0.67408895\n",
      "strcuture parameter: \n",
      " [[-0.85259151 -1.12063027 -1.11497193  2.52389813  0.74466043  0.132118\n",
      "   0.6633379  -1.24076478  0.70790665 -1.19564625 -1.27276189  0.5866211 ]\n",
      " [-0.86378491 -0.8265612   0.96495959 -0.96201655  0.74846465  0.57379255\n",
      "  -1.08549221 -1.17583103 -1.02638815  0.83239053 -0.93707353  0.78466897]\n",
      " [ 0.93099403 -0.77607547 -0.83053154 -0.85072131 -0.96469053 -0.4978137\n",
      "   0.7411085   0.7853666   0.76156187  0.69756177  0.86890193  0.68651853]\n",
      " [-1.17738305 -0.73792215  0.88214533  0.79473107  0.61803115  0.67761145\n",
      "  -0.77754554  0.66305384 -1.04308985  1.33818734  1.3470526  -0.97407897]\n",
      " [-1.16133134  0.96721871  2.02443496 -0.77971848  0.68369934  0.56688916\n",
      "  -1.08174207 -0.81211193  0.79662138 -0.89350431  0.60041047  0.67096547]\n",
      " [-0.31477922  2.28535803 -1.0720742   1.59575667  0.59426441  0.60343962\n",
      "  -0.5868858   0.70075866 -1.07123563 -0.97672419 -1.25965247 -0.81857954]] \n",
      " network parameter: \n",
      " [[-1.39136933 -1.26797529 -1.29446396 -1.20710511  0.89938361 -0.69467016\n",
      "   1.20064938  1.11941586 -1.09720425 -1.58034455 -1.0957375   0.27889734]\n",
      " [-1.56431006 -1.32014135 -1.27889744 -1.27321739  0.87294157  0.22714286\n",
      "   1.14614382  1.1293929  -1.13838826 -0.74697479 -1.0663084  -0.48695307]\n",
      " [-1.07602731 -1.35803049 -1.33158334 -1.20590346  0.8887343   0.27636245\n",
      "   1.10661743  1.11966862 -1.04933166  0.74632354 -1.0035902  -0.25220812]\n",
      " [-0.90874824 -1.37193931 -1.2806163  -1.19693133  0.90452019 -0.07573363\n",
      "   1.14052767  1.13083934 -1.0776053   0.80297161 -1.00393209  0.0641898 ]\n",
      " [-0.73023668 -1.35948487 -1.31721821 -1.17636848  0.90979241 -0.52202001\n",
      "   1.18087881  1.14429966 -1.07862028  0.87261248 -1.05888708  0.34618934]\n",
      " [-0.58151546 -1.40797648 -1.24815725 -1.18889633  0.89229375 -0.78221237\n",
      "   1.20838769  1.21080255 -1.10504821 -1.21776264 -1.11157672  0.00211624]]\n",
      "best candidates so far: [ 3  2  0 10  2  1]\n",
      "----------epoch 30-----------\n",
      "batched average loss:  0.67304647\n",
      "strcuture parameter: \n",
      " [[-0.86291166 -1.14713195 -1.14055436  2.59958055  0.74400839  0.09655357\n",
      "   0.67052241 -1.26909609  0.71427796 -1.22144979 -1.30351014  0.58798553]\n",
      " [-0.87795191 -0.8257209   0.99012063 -0.9862519   0.74926392  0.58110462\n",
      "  -1.11300337 -1.20298718 -1.05224148  0.85907536 -0.95376752  0.79640676]\n",
      " [ 0.96500907 -0.78339179 -0.83799218 -0.86249307 -0.98263325 -0.53565998\n",
      "   0.74429806  0.79193099  0.76480227  0.70480698  0.84208236  0.69281612]\n",
      " [-1.1999317  -0.74441447  0.90105736  0.80244763  0.6076691   0.68551947\n",
      "  -0.78757527  0.66746835 -1.0658203   1.39525368  1.35736976 -0.99041225]\n",
      " [-1.19111744  0.9985403   2.1254765  -0.79250514  0.6711742   0.56851464\n",
      "  -1.11453668 -0.82620906  0.79063387 -0.90850414  0.59186149  0.67567472]\n",
      " [-0.41339869  2.44610443 -1.11340317  1.50886854  0.58492422  0.60071343\n",
      "  -0.68950154  0.69786074 -1.11363992 -1.00323359 -1.30545735 -0.83354171]] \n",
      " network parameter: \n",
      " [[-1.44134534 -1.30345699 -1.32822281 -1.20891988  0.86727095 -0.81007569\n",
      "   1.21476231  1.11205073 -1.09897569 -1.70664629 -1.09895319  0.20577237]\n",
      " [-1.68604049 -1.35919938 -1.31515399 -1.28657568  0.83243229  0.2763425\n",
      "   1.15426383  1.11892683 -1.1363035  -0.87127428 -1.06570176 -0.59620601]\n",
      " [-1.2017177  -1.39969766 -1.36850175 -1.22042459  0.85051206  0.36546141\n",
      "   1.11024574  1.10727257 -1.04133044  0.86839711 -0.99779684 -0.28283661]\n",
      " [-1.03300484 -1.41591799 -1.31607703 -1.21045238  0.86528136 -0.07215233\n",
      "   1.14727347  1.12229361 -1.07179406  0.92690359 -0.99427675  0.16248018]\n",
      " [-0.85215514 -1.40415895 -1.35077013 -1.1894272   0.87558904 -0.63061258\n",
      "   1.19504125  1.14158064 -1.07475113  0.99813432 -1.05558507  0.46062875]\n",
      " [-0.70329691 -1.45508143 -1.27698297 -1.19609339  0.86057943 -0.90652172\n",
      "   1.23242955  1.21668591 -1.10720515 -1.16172838 -1.12012844  0.00211624]]\n",
      "best candidates so far: [3 2 0 9 2 1]\n",
      "epoch: 1 step: 20, loss is 0.6217631101608276\n",
      "epoch: 1 step: 40, loss is 0.7314348816871643\n",
      "epoch: 2 step: 11, loss is 0.7248407602310181\n",
      "epoch: 2 step: 31, loss is 0.5120940804481506\n",
      "epoch: 3 step: 2, loss is 0.7271486520767212\n",
      "epoch: 3 step: 22, loss is 0.5555857419967651\n",
      "epoch: 3 step: 42, loss is 0.4994909167289734\n",
      "epoch: 4 step: 13, loss is 0.4396984577178955\n",
      "epoch: 4 step: 33, loss is 0.4802752435207367\n",
      "epoch: 5 step: 4, loss is 0.47549915313720703\n",
      "epoch: 5 step: 24, loss is 0.6103399395942688\n",
      "epoch: 5 step: 44, loss is 0.6531403064727783\n",
      "epoch: 6 step: 15, loss is 0.5390165448188782\n",
      "epoch: 6 step: 35, loss is 0.5472528338432312\n",
      "epoch: 7 step: 6, loss is 0.5599801540374756\n",
      "epoch: 7 step: 26, loss is 0.5572453737258911\n",
      "epoch: 7 step: 46, loss is 0.5375239849090576\n",
      "epoch: 8 step: 17, loss is 0.42025527358055115\n",
      "epoch: 8 step: 37, loss is 0.5153096914291382\n",
      "epoch: 9 step: 8, loss is 0.7576204538345337\n",
      "epoch: 9 step: 28, loss is 0.6295233964920044\n",
      "epoch: 9 step: 48, loss is 0.5203899145126343\n",
      "epoch: 10 step: 19, loss is 0.5760096311569214\n",
      "epoch: 10 step: 39, loss is 0.9284610748291016\n",
      "epoch: 11 step: 10, loss is 0.6259978413581848\n",
      "epoch: 11 step: 30, loss is 0.6766059994697571\n",
      "epoch: 12 step: 1, loss is 0.6156290173530579\n",
      "epoch: 12 step: 21, loss is 0.7252498865127563\n",
      "epoch: 12 step: 41, loss is 0.45102566480636597\n",
      "epoch: 13 step: 12, loss is 0.6819435358047485\n",
      "epoch: 13 step: 32, loss is 0.5447179675102234\n",
      "epoch: 14 step: 3, loss is 0.5202748775482178\n",
      "epoch: 14 step: 23, loss is 0.4526219964027405\n",
      "epoch: 14 step: 43, loss is 0.5104929804801941\n",
      "epoch: 15 step: 14, loss is 0.5672909021377563\n",
      "epoch: 15 step: 34, loss is 0.7005779147148132\n",
      "目前的二分类精度是: 0.6970954356846473\n",
      "----------epoch 32-----------\n",
      "batched average loss:  0.67013395\n",
      "strcuture parameter: \n",
      " [[-0.87180238 -1.17040842 -1.16475789  2.66497645  0.74117715  0.06832438\n",
      "   0.67567151 -1.29538077  0.71889801 -1.24598299 -1.33214159  0.58926245]\n",
      " [-0.89063188 -0.82581566  1.01152901 -1.01166624  0.74904692  0.59975806\n",
      "  -1.13849483 -1.22820796 -1.07652954  0.89058711 -0.96899238  0.80625545]\n",
      " [ 1.00264273 -0.78998124 -0.84535936 -0.87320201 -0.99884673 -0.57717199\n",
      "   0.7464168   0.7928059   0.7643496   0.71220849  0.80259263  0.69907502]\n",
      " [-1.21839478 -0.7497477   0.91816819  0.80745489  0.59766085  0.69198466\n",
      "  -0.79656545  0.67081069 -1.08642188  1.46445252  1.35500915 -1.00505475]\n",
      " [-1.21905442  1.02689061  2.22516642 -0.80496106  0.65634866  0.56828947\n",
      "  -1.1468022  -0.83932197  0.77891737 -0.92240709  0.58111076  0.67941381]\n",
      " [-0.5399536   2.60705658 -1.16192802  1.39573254  0.57080821  0.59466846\n",
      "  -0.81646713  0.68952139 -1.16382298 -1.0322963  -1.3570165  -0.84901533]] \n",
      " network parameter: \n",
      " [[-1.47655293 -1.33444604 -1.35703496 -1.20031928  0.82992911 -0.92835795\n",
      "   1.22217328  1.09796836 -1.09295908 -1.83308728 -1.09596148  0.12226916]\n",
      " [-1.80885401 -1.39336173 -1.34632295 -1.29168305  0.78661447  0.33272757\n",
      "   1.15579292  1.1005831  -1.1263222  -0.99741057 -1.05906705 -0.71006274]\n",
      " [-1.32855156 -1.4362151  -1.4003938  -1.22665791  0.80754493  0.45965081\n",
      "   1.10714356  1.0863562  -1.02519956  0.99325236 -0.9858068  -0.30162319]\n",
      " [-1.15920404 -1.45466309 -1.34623725 -1.21621398  0.8212031  -0.08854755\n",
      "   1.14737291  1.10404566 -1.05786421  1.05340278 -0.97822794  0.27183049]\n",
      " [-0.97703219 -1.44356279 -1.3784491  -1.19478689  0.83734475 -0.7458304\n",
      "   1.20309899  1.13093699 -1.06221131  1.12529783 -1.04746954  0.58067055]\n",
      " [-0.82815861 -1.49794516 -1.30002721 -1.19601759  0.82407687 -1.03252986\n",
      "   1.25071498  1.21552303 -1.10088049 -1.0853308  -1.12424903  0.00211624]]\n",
      "best candidates so far: [3 2 0 9 2 1]\n",
      "----------epoch 34-----------\n",
      "batched average loss:  0.6690007\n",
      "strcuture parameter: \n",
      " [[-0.88012007 -1.19238949 -1.18801166  2.72119742  0.73940734  0.03935102\n",
      "   0.67838474 -1.31993211  0.7220199  -1.26852212 -1.36020926  0.59068168]\n",
      " [-0.90237551 -0.82449798  1.0304695  -1.03830785  0.74718296  0.61353216\n",
      "  -1.16238102 -1.25177935 -1.10000866  0.9322057  -0.98306156  0.81527294]\n",
      " [ 1.04056767 -0.7947144  -0.85155538 -0.88239786 -1.01320039 -0.61766145\n",
      "   0.74688216  0.78972138  0.76096409  0.72051672  0.76353583  0.7040611 ]\n",
      " [-1.23517547 -0.75382309  0.93248578  0.81109785  0.58703555  0.69747278\n",
      "  -0.80515576  0.6728099  -1.10951358  1.54035012  1.34723038 -1.01805856]\n",
      " [-1.24600508  1.05378019  2.31764414 -0.81649046  0.64021749  0.56814737\n",
      "  -1.17795919 -0.85208185  0.76381805 -0.93473424  0.56836319  0.68215049]\n",
      " [-0.68412897  2.76201697 -1.21456505  1.26114485  0.55223952  0.58678293\n",
      "  -0.96018433  0.67736179 -1.21754971 -1.06219705 -1.41178922 -0.86445905]] \n",
      " network parameter: \n",
      " [[-1.49822723 -1.3621758  -1.38173601 -1.1826423   0.78971841 -1.04880137\n",
      "   1.22432533  1.07909761 -1.07823622 -1.96033001 -1.08752129  0.02982485]\n",
      " [-1.93009021 -1.42383477 -1.37311645 -1.28985196  0.73800972  0.38654355\n",
      "   1.15166214  1.07616538 -1.10857247 -1.12543824 -1.0475187  -0.82506078]\n",
      " [-1.45515732 -1.46865426 -1.42821795 -1.22599218  0.76155691  0.55946022\n",
      "   1.0985471   1.05936084 -1.00073322  1.11954376 -0.969057   -0.31063237]\n",
      " [-1.28567683 -1.48910659 -1.3719226  -1.21563331  0.77472479 -0.11457493\n",
      "   1.14154421  1.08030741 -1.03608504  1.18204263 -0.95730791  0.388575  ]\n",
      " [-1.10215157 -1.47859696 -1.40097352 -1.19397584  0.79592492 -0.86268573\n",
      "   1.20587466  1.11352669 -1.04336129  1.25356418 -1.03454103  0.70427771]\n",
      " [-0.95149107 -1.53724214 -1.31841155 -1.19013315  0.78422283 -1.16074694\n",
      "   1.26392795  1.20873538 -1.08742209 -1.00093171 -1.12465466  0.00211624]]\n",
      "best candidates so far: [3 2 0 9 2 1]\n",
      "----------epoch 36-----------\n",
      "batched average loss:  0.666929\n",
      "strcuture parameter: \n",
      " [[-8.88135167e-01 -1.21551127e+00 -1.21143703e+00  2.77261464e+00\n",
      "   7.37361977e-01  1.68917847e-03  6.77757792e-01 -1.34361716e+00\n",
      "   7.23019300e-01 -1.29054171e+00 -1.38834352e+00  5.91787678e-01]\n",
      " [-9.13506521e-01 -8.27815643e-01  1.04777498e+00 -1.06812247e+00\n",
      "   7.44214694e-01  6.17126098e-01 -1.18745058e+00 -1.27530557e+00\n",
      "  -1.12429648e+00  9.86429520e-01 -9.96939162e-01  8.23790372e-01]\n",
      " [ 1.08154594e+00 -7.99767135e-01 -8.57305213e-01 -8.91285056e-01\n",
      "  -1.02638830e+00 -6.59222572e-01  7.43477791e-01  7.83204848e-01\n",
      "   7.55184491e-01  7.30583885e-01  7.24197473e-01  7.07347068e-01]\n",
      " [-1.25230945e+00 -7.56748658e-01  9.44570738e-01  8.11622235e-01\n",
      "   5.74266118e-01  7.01768834e-01 -8.13212158e-01  6.74133732e-01\n",
      "  -1.13338219e+00  1.63229250e+00  1.33494275e+00 -1.03048629e+00]\n",
      " [-1.27401399e+00  1.07941084e+00  2.41044039e+00 -8.28651904e-01\n",
      "   6.21033110e-01  5.66692305e-01 -1.21046366e+00 -8.64656482e-01\n",
      "   7.44887560e-01 -9.46582715e-01  5.51427031e-01  6.83507820e-01]\n",
      " [-8.54665734e-01  2.91306829e+00 -1.27580528e+00  1.09983797e+00\n",
      "   5.28245921e-01  5.76040715e-01 -1.12985506e+00  6.60418432e-01\n",
      "  -1.27939516e+00 -1.09512643e+00 -1.47475489e+00 -8.81204605e-01]] \n",
      " network parameter: \n",
      " [[-1.50874384 -1.38771613 -1.40286757 -1.15636934  0.74807718 -1.17253924\n",
      "   1.22159768  1.05682828 -1.05674154 -2.08700592 -1.07440892 -0.06817279]\n",
      " [-2.04924034 -1.45144875 -1.39618827 -1.28186044  0.68770979  0.3959824\n",
      "   1.14195986  1.04738473 -1.0843253  -1.25159541 -1.03144496 -0.94046344]\n",
      " [-1.58068177 -1.49776385 -1.45252293 -1.21985149  0.71423797  0.65475068\n",
      "   1.08479354  1.02851166 -0.96979687  1.2478519  -0.94840497 -0.30573377]\n",
      " [-1.41020148 -1.51986432 -1.39387777 -1.20963744  0.72641226 -0.16640407\n",
      "   1.13096684  1.0523771  -1.00744382  1.31126417 -0.93303059  0.51038167]\n",
      " [-1.22343374 -1.5098836  -1.4192718  -1.18796129  0.75263178 -0.98313726\n",
      "   1.20423938  1.09184081 -1.01889574  1.38119729 -1.01819633  0.83027715]\n",
      " [-1.06991536 -1.57345734 -1.33323611 -1.17961712  0.74225251 -1.29023105\n",
      "   1.27311912  1.1977612  -1.0679295  -0.91696655 -1.12204771  0.00211624]]\n",
      "best candidates so far: [3 2 0 9 2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1039326d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/Quantum/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorcircuit as tc\n",
    "import tensorflow as tf\n",
    "from DQAS_tool import  DQASAnsatz_from_result\n",
    "K = tc.set_backend(\"tensorflow\")\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.06, 100, 0.5)\n",
    "structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.05))\n",
    "network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))\n",
    "verbose = True\n",
    "# 设置超参数\n",
    "epochs = 100\n",
    "batch_size=50\n",
    "shape_nnp = (num_layer, shape_parametized)\n",
    "shape_stp = (num_layer, shape_parametized)\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp).astype(rtype)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp).astype(rtype)\n",
    "#print(stp.shape)\n",
    "avcost1 = 0\n",
    "\n",
    "ops_onehot = ops.OneHot(axis=-1)\n",
    "\n",
    "batch_loss_history=[] # 记录每个epoch的batch_size损失值\n",
    "structure_distribution_history=[] # 记录每个epoch的结构参数\n",
    "ansatz_params_history=[] # 记录每个epoch的网络参数\n",
    "best_candidates_history=[] # 记录每个epoch的最佳候选\n",
    "\n",
    "\n",
    "for epoch in range(epochs):  # 更新结构参数的迭代\n",
    "    avcost2 = avcost1\n",
    "    costl = []\n",
    "    tmp = np.stack([sampling_from_structure(stp,num_layer,shape_parametized) for _ in range(batch_size)])\n",
    "    batch_structure = ops_onehot(ms.Tensor(tmp),shape_parametized,ms.Tensor(1),ms.Tensor(0))\n",
    "    #print(batch_structure.shape)\n",
    "    # print(tmp,batch_structure)\n",
    "    loss_value = []\n",
    "    grad_nnps = []\n",
    "    grad_stps = []\n",
    "    \n",
    "    for i in batch_structure:          \n",
    "        infd, grad_nnp = vag_nnp(Structure_params=i,Ansatz_params=nnp,paramerterized_pool=unbound_opeartor_pool,num_layer=num_layer,n_qbits=8)(ms.Tensor(X_train),ms.Tensor(y_train))\n",
    "        gs = nmf_gradient(structures=stp,oh=i,num_layer=num_layer,size_pool=shape_parametized)\n",
    "        #print(infd,grad_nnp)\n",
    "        loss_value.append(infd)\n",
    "        grad_nnps.append(grad_nnp[0])\n",
    "        grad_stps.append(gs)\n",
    "    \n",
    "    infd = ops.stack(loss_value)\n",
    "    gnnp = ops.addn(grad_nnps)\n",
    "    gstp = [(infd[i] - avcost2) * grad_stps[i] for i in range(infd.shape[0])]\n",
    "    gstp_averge = ops.addn(gstp) / infd.shape[0]\n",
    "    avcost1 = sum(infd) / infd.shape[0]\n",
    "    \n",
    "    gnnp_tf = tf.convert_to_tensor(gnnp.reshape(nnp.shape).asnumpy(),dtype=tf.float64)\n",
    "    nnp_tf = tf.convert_to_tensor(nnp,dtype=tf.float64)\n",
    "    gstp_averge_tf = tf.convert_to_tensor(gstp_averge.reshape(stp.shape).asnumpy(),dtype=tf.float64)\n",
    "    stp_tf = tf.convert_to_tensor(stp,dtype=tf.float64)\n",
    "    # 更新参数\n",
    "    nnp_tf = network_opt.update(gnnp_tf, nnp_tf)\n",
    "    stp_tf = structure_opt.update(gstp_averge_tf, stp_tf) \n",
    "    \n",
    "    nnp = nnp_tf.numpy()\n",
    "    stp = stp_tf.numpy()\n",
    "    \n",
    "    batch_loss_history.append(avcost1)\n",
    "    structure_distribution_history.append(stp)\n",
    "    ansatz_params_history.append(nnp)\n",
    "    best_candidates_history.append(best_from_structure(cand_preset.asnumpy()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if epoch % 2 == 0 or epoch == epochs - 1:\n",
    "        print(\"----------epoch %s-----------\" % epoch)\n",
    "        print(\n",
    "            \"batched average loss: \",\n",
    "            avcost1,\n",
    "        )\n",
    "    \n",
    "        if verbose:\n",
    "            print(\n",
    "                \"strcuture parameter: \\n\",\n",
    "                stp,\n",
    "                \"\\n network parameter: \\n\",\n",
    "                nnp,\n",
    "            )\n",
    "        \n",
    "        cand_preset = best_from_structure(stp)\n",
    "        print(\"best candidates so far:\",cand_preset)\n",
    "        # print(\n",
    "        #     \"corresponding weights for each gate:\",\n",
    "        #     [K.numpy(nnp[j, i]) if i < 6 else 0.0 for j, i in enumerate(cand_preset)],\n",
    "        # )\n",
    "    # if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "    #     _,acc = DQASAnsatz_from_result(best_candidate=cand_preset.asnumpy(),parameterized_pool=unbound_opeartor_pool,num_layer=num_layer,n_qbits=8)\n",
    "    #     print(\"目前的二分类精度是:\",np.max(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
