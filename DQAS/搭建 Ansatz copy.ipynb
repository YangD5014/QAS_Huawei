{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindquantum/simulator/__init__.py:17: UserWarning: Unable import mqvector gpu backend due to: cannot import name '_mq_vector_gpu' from partially initialized module 'mindquantum' (most likely due to a circular import) (/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindquantum/__init__.py)\n",
      "  from .available_simulator import SUPPORTED_SIMULATOR\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mindquantum.core.gates import RX, RY, RZ, H, X, Y, Z, CNOT\n",
    "from mindquantum.core.circuit import Circuit\n",
    "import mindspore as ms\n",
    "from mindquantum.simulator import  Simulator\n",
    "from mindquantum.core.gates import GroupedPauli\n",
    "from mindquantum.core.operators import TimeEvolution,QubitOperator\n",
    "from mindquantum.core.parameterresolver import PRConvertible,PRGenerator,ParameterResolver\n",
    "from DQAS_tool import generate_pauli_string,one_hot,unbound_opeartor_pool\n",
    "from mindquantum.core.gates import RotPauliString\n",
    "from mindquantum.core.gates import UnivMathGate\n",
    "from mindspore import Tensor, ops\n",
    "from mindquantum.core.circuit import UN\n",
    "from mindquantum.core.operators import Hamiltonian             # 导入Hamiltonian模块，用于构建哈密顿量\n",
    "from mindquantum.framework import MQLayer\n",
    "from mindspore.nn import  TrainOneStepCell\n",
    "from mindspore.nn import SoftmaxCrossEntropyWithLogits                         # 导入SoftmaxCrossEntropyWithLogits模块，用于定义损失函数\n",
    "from mindspore.nn import Adam                                                  # 导入Adam模块用于定义优化参数\n",
    "from mindspore.train import Accuracy, Model, LossMonitor                       # 导入Accuracy模块，用于评估预测准确率\n",
    "import mindspore as ms\n",
    "from mindspore import Parameter, Tensor\n",
    "from mindspore.dataset import NumpySlicesDataset\n",
    "from torch.utils.data import DataLoader# 导入NumpySlicesDataset模块，用于创建模型可以识别的数据集\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from data_processing import X_train,X_test,y_train,y_test\n",
    "from mindquantum.algorithm.nisq import HardwareEfficientAnsatz     \n",
    "num_layer = 6\n",
    "# 定义标准差和形状\n",
    "stddev = 0.02\n",
    "shape_parametized = 12\n",
    "shape_unparametized = 4\n",
    "shape_nnp = (num_layer, shape_parametized)\n",
    "shape_stp = (num_layer, shape_parametized+shape_unparametized)\n",
    "\n",
    "shape_stp = (num_layer, shape_parametized)\n",
    "\n",
    "rtype = np.float64\n",
    "ctype = np.complex128\n",
    "# 使用 numpy 生成随机数矩阵\n",
    "np.random.seed(10)\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp).astype(rtype)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp).astype(rtype)\n",
    "# #Operator Pool\n",
    "unbound_opeartor_pool = [generate_pauli_string(n=8,seed=i)[0] for i in range(shape_parametized)]\n",
    "bound_opeartor_pool = [generate_pauli_string(n=8,seed=i)[1] for i in range(shape_parametized,shape_parametized+shape_unparametized)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DQAS_tool import Mindspore_ansatz,loss_fn,vag_nnp,sampling_from_structure,vag_nnp,vag_nnp_function,sampling_from_structure\n",
    "from mindquantum.framework import MQOps\n",
    "import mindspore.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.numpy as mnp\n",
    "def nmf_gradient(structures:np.array, oh:ms.Tensor,num_layer: int,size_pool:int):\n",
    "    \"\"\"\n",
    "    使用 MindSpore 实现蒙特卡洛梯度计算。\n",
    "    \"\"\"\n",
    "      # Step 1: 获取选择的索引\n",
    "    choice = ops.Argmax(axis=-1)(oh)\n",
    "    # Step 2: 计算概率\n",
    "    softmax = ops.Softmax(axis=-1)\n",
    "    prob = softmax(ms.Tensor(structures))\n",
    "    # Step 3: 获取概率矩阵中的值\n",
    "    indices = mnp.stack((mnp.arange(num_layer, dtype=ms.int64), choice), axis=1)\n",
    "    prob = ops.GatherNd()(prob, indices)\n",
    "    # Step 4: 变换概率矩阵\n",
    "    prob = prob.reshape(-1, 1)\n",
    "    prob = ops.Tile()(prob, (1, size_pool))\n",
    "    \n",
    "    # Step 5: 生成蒙特卡洛梯度\n",
    "    gradient = ops.TensorScatterAdd()(Tensor(-prob, ms.float64), indices, mnp.ones((num_layer,), dtype=ms.float64))\n",
    "    return gradient\n",
    "    \n",
    "    \n",
    "# 对向量化版本的封装\n",
    "# nmf_gradient_vmap = ops.vmap(nmf_gradient, in_axes=(None, 0, None, None))\n",
    "\n",
    "def best_from_structure(structures: np.array):\n",
    "    return ops.Argmax(axis=-1)(ms.Tensor(structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for dimension with size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m grad_stps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_structure:          \n\u001b[0;32m---> 39\u001b[0m     infd, grad_nnp \u001b[38;5;241m=\u001b[39m \u001b[43mvag_nnp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStructure_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAnsatz_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnnp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_qbits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m(ms\u001b[38;5;241m.\u001b[39mTensor(X_train),ms\u001b[38;5;241m.\u001b[39mTensor(y_train))\n\u001b[1;32m     40\u001b[0m     gs \u001b[38;5;241m=\u001b[39m nmf_gradient(structures\u001b[38;5;241m=\u001b[39mstp,oh\u001b[38;5;241m=\u001b[39mi,num_layer\u001b[38;5;241m=\u001b[39mnum_layer,size_pool\u001b[38;5;241m=\u001b[39mshape_parametized)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m#print(infd,grad_nnp)\u001b[39;00m\n",
      "File \u001b[0;32m~/mac_vscode/华为 QAS 实习/DQAS/DQAS_tool.py:93\u001b[0m, in \u001b[0;36mvag_nnp\u001b[0;34m(Structure_params, Ansatz_params, n_layer, n_qbits)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvag_nnp\u001b[39m(Structure_params: np\u001b[38;5;241m.\u001b[39marray, Ansatz_params: np\u001b[38;5;241m.\u001b[39marray,n_layer:\u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,n_qbits:\u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    用于计算梯度 Ansatz_params关于 loss 的梯度\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    value,grad_ansatz_params = vag(训练数据,标签数据)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     ansatz \u001b[38;5;241m=\u001b[39m \u001b[43mMindspore_ansatz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStructure_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_qbits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_qbits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     sim \u001b[38;5;241m=\u001b[39m Simulator(backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmqvector\u001b[39m\u001b[38;5;124m'\u001b[39m, n_qubits\u001b[38;5;241m=\u001b[39mn_qbits)\n\u001b[1;32m     95\u001b[0m     hams \u001b[38;5;241m=\u001b[39m [Hamiltonian(QubitOperator(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]]\n",
      "File \u001b[0;32m~/mac_vscode/华为 QAS 实习/DQAS/DQAS_tool.py:73\u001b[0m, in \u001b[0;36mMindspore_ansatz\u001b[0;34m(Structure_p, n_layer, n_qbits)\u001b[0m\n\u001b[1;32m     71\u001b[0m paramertized_part_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index_op,each_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unbound_opeartor_pool):\n\u001b[0;32m---> 73\u001b[0m     ansatz \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m TimeEvolution(QubitOperator(terms\u001b[38;5;241m=\u001b[39meach_op,coefficient\u001b[38;5;241m=\u001b[39mpr_gen\u001b[38;5;241m.\u001b[39mnew()),time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[43mmy_stp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex_op\u001b[49m\u001b[43m]\u001b[49m))\u001b[38;5;241m.\u001b[39mcircuit\n\u001b[1;32m     74\u001b[0m     paramertized_part_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# for index_op,each_op in enumerate(bound_opeartor_pool):\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m#     op = GroupedPauli(each_op)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#     tmp_cir = Circuit([GroupedPauli(each_op).on(range(n_qbits))])\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#     matrix = tmp_cir.matrix()\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#     ansatz += UnivMathGate(matrix_value=matrix*float(my_stp[i,index_op+paramertized_part_count]),name=f'{my_stp[i,index_op+paramertized_part_count]}*{op.pauli_string}').on(range(n_qbits))  \u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/common/tensor.py:456\u001b[0m, in \u001b[0;36mTensor.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 456\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_operator_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__getitem__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    458\u001b[0m         out\u001b[38;5;241m.\u001b[39mparent_tensor_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/ops/composite/multitype_ops/_compile_utils.py:202\u001b[0m, in \u001b[0;36m_tensor_getitem\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Handle tensor getitem\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m new_index, tensor_update_types, tensor_update_args \u001b[38;5;241m=\u001b[39m getitem_tensor_index_info(\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m, index)\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_update_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_update_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/ops/composite/multitype_ops/_compile_utils.py:123\u001b[0m, in \u001b[0;36mdata_update\u001b[0;34m(transfer_types, args, data, new_index, value)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_setitem_by_bool_tensor(data, new_index, value)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transfer_type \u001b[38;5;241m==\u001b[39m ValueTransferType\u001b[38;5;241m.\u001b[39mkRaiseIndexError:\n\u001b[0;32m--> 123\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of bounds for dimension with size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for dimension with size 8"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorcircuit as tc\n",
    "import tensorflow as tf\n",
    "K = tc.set_backend(\"tensorflow\")\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.06, 100, 0.5)\n",
    "structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.12))\n",
    "network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))\n",
    "verbose = True\n",
    "# 设置超参数\n",
    "epochs = 100\n",
    "batch_size=50\n",
    "shape_nnp = (num_layer, shape_parametized)\n",
    "shape_stp = (num_layer, shape_parametized)\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp).astype(rtype)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp).astype(rtype)\n",
    "# 定义学习率衰减\n",
    "# lr = nn.ExponentialDecayLR(learning_rate=0.06, decay_rate=0.5, decay_steps=100)\n",
    "# # 定义优化器\n",
    "# structure_params = Parameter(Tensor(stp), name='Structure_params')\n",
    "# ansatz_params    = Parameter(Tensor(nnp), name='Ansatz_params')\n",
    "\n",
    "# structure_opt = nn.Adam(params=[structure_params], learning_rate=0.12)\n",
    "# network_opt = nn.Adam(params=[ansatz_params], learning_rate=lr)\n",
    "avcost1 = 0\n",
    "\n",
    "ops_onehot = ops.OneHot(axis=-1)\n",
    "\n",
    "for epoch in range(epochs):  # 更新结构参数的迭代\n",
    "    avcost2 = avcost1\n",
    "    costl = []\n",
    "    tmp = np.stack([sampling_from_structure(stp,num_layer,shape_parametized) for _ in range(batch_size)])\n",
    "    batch_structure = ops_onehot(ms.Tensor(tmp),8,ms.Tensor(1),ms.Tensor(0))\n",
    "    # print(tmp,batch_structure)\n",
    "    loss_value = []\n",
    "    grad_nnps = []\n",
    "    grad_stps = []\n",
    "    \n",
    "    for i in batch_structure:          \n",
    "        infd, grad_nnp = vag_nnp(Structure_params=i,Ansatz_params=nnp,n_layer=num_layer,n_qbits=8)(ms.Tensor(X_train),ms.Tensor(y_train))\n",
    "        gs = nmf_gradient(structures=stp,oh=i,num_layer=num_layer,size_pool=shape_parametized)\n",
    "        \n",
    "        #print(infd,grad_nnp)\n",
    "        loss_value.append(infd)\n",
    "        grad_nnps.append(grad_nnp[0])\n",
    "        grad_stps.append(gs)\n",
    "    \n",
    "    infd = ops.stack(loss_value)\n",
    "    gnnp = ops.addn(grad_nnps)\n",
    "    gstp = [(infd[i] - avcost2) * grad_stps[i] for i in range(infd.shape[0])]\n",
    "    gstp_averge = ops.addn(gstp) / infd.shape[0]\n",
    "    avcost1 = sum(infd) / infd.shape[0]\n",
    "    \n",
    "    gnnp_tf = tf.convert_to_tensor(gnnp.reshape(nnp.shape).asnumpy(),dtype=tf.float64)\n",
    "    nnp_tf = tf.convert_to_tensor(nnp,dtype=tf.float64)\n",
    "    gstp_averge_tf = tf.convert_to_tensor(gstp_averge.reshape(stp.shape).asnumpy(),dtype=tf.float64)\n",
    "    stp_tf = tf.convert_to_tensor(stp,dtype=tf.float64)\n",
    "    # 更新参数\n",
    "    nnp_tf = network_opt.update(gnnp_tf, nnp_tf)\n",
    "    stp_tf = structure_opt.update(gstp_averge_tf, stp_tf) \n",
    "    \n",
    "    nnp = nnp_tf.numpy()\n",
    "    stp = stp_tf.numpy()\n",
    "    \n",
    "    \n",
    "    if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "        print(\"----------epoch %s-----------\" % epoch)\n",
    "        print(\n",
    "            \"batched average loss: \",\n",
    "            avcost1,\n",
    "        )\n",
    "    \n",
    "        if verbose:\n",
    "            print(\n",
    "                \"strcuture parameter: \\n\",\n",
    "                stp,\n",
    "                \"\\n network parameter: \\n\",\n",
    "                nnp,\n",
    "            )\n",
    "        \n",
    "        cand_preset = best_from_structure(stp)\n",
    "        print(\"best candidates so far:\",cand_preset)\n",
    "        # print(\n",
    "        #     \"corresponding weights for each gate:\",\n",
    "        #     [K.numpy(nnp[j, i]) if i < 6 else 0.0 for j, i in enumerate(cand_preset)],\n",
    "        # )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
