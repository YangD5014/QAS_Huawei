{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mindquantum.algorithm.nisq import HardwareEfficientAnsatz,RYFull\n",
    "from mindquantum.core.parameterresolver import  PRGenerator\n",
    "import numpy as np\n",
    "from mindquantum.core.gates import RX, RY, RZ, H, X, Y, Z, I,CNOT\n",
    "from mindquantum.core.circuit import Circuit,UN\n",
    "import mindspore as ms\n",
    "import pickle\n",
    "from mindquantum.core.parameterresolver import PRGenerator\n",
    "import random\n",
    "from mindspore import Tensor,ops\n",
    "import tensorcircuit as tc\n",
    "import tensorflow as tf\n",
    "import mindspore.numpy as mnp\n",
    "from DQAS_tool import wash_pr,Mindspore_ansatz_micro,best_from_structure\n",
    "from DQAS_tool import  sampling_from_structure,zeroslike_grad_nnp_micro_minipool,nmf_gradient,vag_nnp_micro,DQAS_accuracy,Washing_namemap\n",
    "import sys\n",
    "from typing import Union\n",
    "sys.path.append('..')\n",
    "from Test_tool import Test_ansatz\n",
    "from data_processing import X_train,X_test,y_train,y_test\n",
    "from mindquantum.core.circuit import change_param_name,apply\n",
    "\n",
    "pr_pool = PRGenerator('pool')\n",
    "parameterized_circuit= \\\n",
    "[\n",
    " UN(RZ(pr_pool.new()),maps_obj=[0])+\\\n",
    " UN(RY(pr_pool.new()),maps_obj=[0])+\\\n",
    " UN(RZ(pr_pool.new()),maps_obj=[0])+I.on(1),\n",
    " UN(RZ(pr_pool.new()),maps_obj=[1])+\\\n",
    " UN(RY(pr_pool.new()),maps_obj=[1])+\\\n",
    " UN(RZ(pr_pool.new()),maps_obj=[1])+I.on(0),]\n",
    "\n",
    "\n",
    "unparameterized_circuit = \\\n",
    "[UN(X,maps_obj=[0],maps_ctrl=[1]),\n",
    " UN(X,maps_obj=[1],maps_ctrl=[0]),\n",
    " ]\n",
    "ansatz_pr = PRGenerator('ansatz')\n",
    "shape_parametized = len(parameterized_circuit)\n",
    "shape_unparameterized = len(unparameterized_circuit)\n",
    "num_layer=4\n",
    "shape_nnp = (7,num_layer,shape_parametized,3)\n",
    "shape_stp = (num_layer,shape_unparameterized+shape_parametized)\n",
    "stddev = 0.03\n",
    "np.random.seed(2)\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp)\n",
    "ops_onehot = ops.OneHot(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------epoch 0-----------\n",
      "batched平均损失:  0.8989891\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 1-----------\n",
      "batched平均损失:  0.8224533\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 2-----------\n",
      "batched平均损失:  0.806179\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 3-----------\n",
      "batched平均损失:  0.7906138\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 4-----------\n",
      "batched平均损失:  0.7756768\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 5-----------\n",
      "batched平均损失:  0.74004054\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 6-----------\n",
      "batched平均损失:  0.7490204\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 7-----------\n",
      "batched平均损失:  0.7249916\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 8-----------\n",
      "batched平均损失:  0.66899323\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 9-----------\n",
      "batched平均损失:  0.67487985\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 10-----------\n",
      "batched平均损失:  0.65214455\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 11-----------\n",
      "batched平均损失:  0.65023583\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 12-----------\n",
      "batched平均损失:  0.6769554\n",
      "最好的候选结构: [2 2 3 0]\n",
      "二分类准确率 Acc =49.94807892004154% \n",
      "----------epoch 13-----------\n",
      "batched平均损失:  0.6446177\n",
      "最好的候选结构: [2 2 3 0]\n",
      "二分类准确率 Acc =54.932502596054% \n",
      "----------epoch 14-----------\n",
      "batched平均损失:  0.6354785\n",
      "最好的候选结构: [2 2 3 0]\n",
      "二分类准确率 Acc =58.87850467289719% \n",
      "----------epoch 15-----------\n",
      "batched平均损失:  0.63315606\n",
      "最好的候选结构: [2 2 3 0]\n",
      "二分类准确率 Acc =62.824506749740394% \n",
      "----------epoch 16-----------\n",
      "batched平均损失:  0.6196716\n",
      "最好的候选结构: [2 2 3 0]\n",
      "二分类准确率 Acc =67.70508826583594% \n",
      "----------epoch 17-----------\n",
      "batched平均损失:  0.6107262\n",
      "最好的候选结构: [2 2 3 0]\n",
      "二分类准确率 Acc =70.61266874350987% \n",
      "----------epoch 18-----------\n",
      "batched平均损失:  0.60315937\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =45.482866043613704% \n",
      "----------epoch 19-----------\n",
      "batched平均损失:  0.60575795\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =46.41744548286604% \n",
      "----------epoch 20-----------\n",
      "batched平均损失:  0.5870655\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =46.93665628245067% \n",
      "----------epoch 21-----------\n",
      "batched平均损失:  0.58878344\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =47.559709241952234% \n",
      "----------epoch 22-----------\n",
      "batched平均损失:  0.59623164\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.28660436137071% \n",
      "----------epoch 23-----------\n",
      "batched平均损失:  0.5968465\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.598130841121495% \n",
      "----------epoch 24-----------\n",
      "batched平均损失:  0.59191793\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.701973001038425% \n",
      "----------epoch 25-----------\n",
      "batched平均损失:  0.5823809\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.701973001038425% \n",
      "----------epoch 26-----------\n",
      "batched平均损失:  0.56249255\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.80581516095535% \n",
      "----------epoch 27-----------\n",
      "batched平均损失:  0.57614136\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.80581516095535% \n",
      "----------epoch 28-----------\n",
      "batched平均损失:  0.56942743\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.90965732087228% \n",
      "----------epoch 29-----------\n",
      "batched平均损失:  0.55205745\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.90965732087228% \n",
      "----------epoch 30-----------\n",
      "batched平均损失:  0.549401\n",
      "最好的候选结构: [1 3 3 0]\n",
      "二分类准确率 Acc =73.41640706126688% \n",
      "----------epoch 31-----------\n",
      "batched平均损失:  0.528928\n",
      "最好的候选结构: [1 3 3 0]\n",
      "二分类准确率 Acc =74.55867082035307% \n",
      "----------epoch 32-----------\n",
      "batched平均损失:  0.53440493\n",
      "最好的候选结构: [1 3 3 0]\n",
      "二分类准确率 Acc =76.01246105919003% \n",
      "----------epoch 33-----------\n",
      "batched平均损失:  0.5276684\n",
      "最好的候选结构: [1 3 3 0]\n",
      "二分类准确率 Acc =76.73935617860852% \n",
      "----------epoch 34-----------\n",
      "batched平均损失:  0.52608156\n",
      "最好的候选结构: [1 3 3 0]\n",
      "二分类准确率 Acc =77.36240913811008% \n",
      "----------epoch 35-----------\n",
      "batched平均损失:  0.5127308\n",
      "最好的候选结构: [1 3 3 0]\n",
      "二分类准确率 Acc =77.67393561786086% \n",
      "----------epoch 36-----------\n",
      "batched平均损失:  0.5135563\n",
      "最好的候选结构: [1 3 3 0]\n",
      "二分类准确率 Acc =77.0508826583593% \n",
      "----------epoch 37-----------\n",
      "batched平均损失:  0.5056298\n",
      "最好的候选结构: [1 3 3 0]\n",
      "二分类准确率 Acc =76.84319833852544% \n",
      "----------epoch 38-----------\n",
      "batched平均损失:  0.48572415\n",
      "最好的候选结构: [1 3 3 0]\n",
      "二分类准确率 Acc =77.15472481827622% \n",
      "----------epoch 39-----------\n",
      "batched平均损失:  0.4541275\n",
      "最好的候选结构: [1 3 3 0]\n",
      "二分类准确率 Acc =78.08930425752855% \n",
      "----------epoch 40-----------\n",
      "batched平均损失:  0.48059854\n",
      "最好的候选结构: [1 3 3 0]\n",
      "二分类准确率 Acc =79.95846313603323% \n",
      "----------epoch 41-----------\n",
      "batched平均损失:  0.4860246\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =96.98857736240913% \n",
      "----------epoch 42-----------\n",
      "batched平均损失:  0.46879575\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =96.88473520249221% \n",
      "----------epoch 43-----------\n",
      "batched平均损失:  0.45210102\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =96.88473520249221% \n",
      "----------epoch 44-----------\n",
      "batched平均损失:  0.45505208\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =96.88473520249221% \n",
      "----------epoch 45-----------\n",
      "batched平均损失:  0.45150435\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =96.98857736240913% \n",
      "----------epoch 46-----------\n",
      "batched平均损失:  0.46219885\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =97.09241952232607% \n",
      "----------epoch 47-----------\n",
      "batched平均损失:  0.4381718\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =97.09241952232607% \n",
      "----------epoch 48-----------\n",
      "batched平均损失:  0.4336208\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =97.09241952232607% \n",
      "----------epoch 49-----------\n",
      "batched平均损失:  0.42444083\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =97.09241952232607% \n",
      "----------epoch 50-----------\n",
      "batched平均损失:  0.43229344\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 51-----------\n",
      "batched平均损失:  0.40899345\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 52-----------\n",
      "batched平均损失:  0.42156738\n",
      "最好的候选结构: [1 2 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 53-----------\n",
      "batched平均损失:  0.40480477\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.40394600207685% \n",
      "----------epoch 54-----------\n",
      "batched平均损失:  0.41233054\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.50778816199377% \n",
      "----------epoch 55-----------\n",
      "batched平均损失:  0.40406427\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.50778816199377% \n",
      "----------epoch 56-----------\n",
      "batched平均损失:  0.39956987\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.40394600207685% \n",
      "----------epoch 57-----------\n",
      "batched平均损失:  0.3967803\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.40394600207685% \n",
      "----------epoch 58-----------\n",
      "batched平均损失:  0.37740463\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 59-----------\n",
      "batched平均损失:  0.3837555\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 60-----------\n",
      "batched平均损失:  0.37403172\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 61-----------\n",
      "batched平均损失:  0.37002903\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 62-----------\n",
      "batched平均损失:  0.36337706\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 63-----------\n",
      "batched平均损失:  0.35133225\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 64-----------\n",
      "batched平均损失:  0.35416853\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 65-----------\n",
      "batched平均损失:  0.34662855\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 66-----------\n",
      "batched平均损失:  0.34361473\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 67-----------\n",
      "batched平均损失:  0.34068817\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 68-----------\n",
      "batched平均损失:  0.33131063\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 69-----------\n",
      "batched平均损失:  0.33419266\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 70-----------\n",
      "batched平均损失:  0.32736462\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 71-----------\n",
      "batched平均损失:  0.31757772\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 72-----------\n",
      "batched平均损失:  0.3149585\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 73-----------\n",
      "batched平均损失:  0.32431167\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 74-----------\n",
      "batched平均损失:  0.31776738\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 75-----------\n",
      "batched平均损失:  0.31257033\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 76-----------\n",
      "batched平均损失:  0.3121289\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 77-----------\n",
      "batched平均损失:  0.31186843\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 78-----------\n",
      "batched平均损失:  0.30509618\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 79-----------\n",
      "batched平均损失:  0.30206388\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 80-----------\n",
      "batched平均损失:  0.30429018\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 81-----------\n",
      "batched平均损失:  0.298535\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 82-----------\n",
      "batched平均损失:  0.30435538\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 83-----------\n",
      "batched平均损失:  0.29582202\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 84-----------\n",
      "batched平均损失:  0.3004291\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 85-----------\n",
      "batched平均损失:  0.30158272\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 86-----------\n",
      "batched平均损失:  0.3040378\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 87-----------\n",
      "batched平均损失:  0.3006797\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 88-----------\n",
      "batched平均损失:  0.29584134\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 89-----------\n",
      "batched平均损失:  0.30811128\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 90-----------\n",
      "batched平均损失:  0.29552457\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 91-----------\n",
      "batched平均损失:  0.29686636\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.09241952232607% \n",
      "----------epoch 92-----------\n",
      "batched平均损失:  0.2963736\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.09241952232607% \n",
      "----------epoch 93-----------\n",
      "batched平均损失:  0.29896373\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.09241952232607% \n",
      "----------epoch 94-----------\n",
      "batched平均损失:  0.29355428\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.09241952232607% \n",
      "----------epoch 95-----------\n",
      "batched平均损失:  0.29803047\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 96-----------\n",
      "batched平均损失:  0.29336917\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "----------epoch 97-----------\n",
      "batched平均损失:  0.29609728\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.40394600207685% \n",
      "----------epoch 98-----------\n",
      "batched平均损失:  0.29471272\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.40394600207685% \n",
      "----------epoch 99-----------\n",
      "batched平均损失:  0.2931403\n",
      "最好的候选结构: [1 0 3 0]\n",
      "二分类准确率 Acc =97.40394600207685% \n"
     ]
    }
   ],
   "source": [
    "from DQAS_tool import  sampling_from_structure,vag_nnp_micro_minipool,zeroslike_grad_nnp_micro_minipool,nmf_gradient,DQAS_accuracy,Mindspore_ansatz_micro_minipool,nnp_dealwith\n",
    "#from DQAS_tool import  DQASAnsatz_from_result,DQAS_accuracy\n",
    "K = tc.set_backend(\"tensorflow\")\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.06, 100, 0.5)\n",
    "structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.1))\n",
    "network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))\n",
    "verbose = False\n",
    "# 设置超参数\n",
    "epochs = 100\n",
    "batch_size=100\n",
    "avcost1 = 0\n",
    "ops_onehot = ops.OneHot(axis=-1)\n",
    "batch_loss_history=[] # 记录每个epoch的batch_size损失值\n",
    "structure_distribution_history=[] # 记录每个epoch的结构参数\n",
    "ansatz_params_history=[] # 记录每个epoch的网络参数\n",
    "best_candidates_history=[] # 记录每个epoch的最佳候选\n",
    "acc_history = [] #记录每个epoch的准确率\n",
    "\n",
    " \n",
    "for epoch in range(epochs):  # 更新结构参数的迭代\n",
    "    avcost2 = avcost1\n",
    "    costl = []\n",
    "    tmp = np.stack([sampling_from_structure(stp,num_layer,shape_parametized) for _ in range(batch_size)])\n",
    "    batch_structure = ops_onehot(ms.Tensor(tmp),shape_parametized+shape_unparameterized,ms.Tensor(1),ms.Tensor(0))\n",
    "    #print(batch_structure.shape)\n",
    "    # print(tmp,batch_structure)\n",
    "    loss_value = []\n",
    "    grad_nnps = []\n",
    "    grad_stps = []\n",
    "    \n",
    "    for i in batch_structure:\n",
    "        #print(ops.Argmax()(i))          \n",
    "        infd, grad_nnp = vag_nnp_micro_minipool(Structure_params=i,\n",
    "                                    Ansatz_params=nnp,\n",
    "                                    paramerterized_pool=parameterized_circuit,  unparamerterized_pool=unparameterized_circuit,\n",
    "                                    num_layer=num_layer,n_qbits=8)(ms.Tensor(X_train),ms.Tensor(y_train))\n",
    "        \n",
    "        grad_nnp_zeroslike = zeroslike_grad_nnp_micro_minipool(batch_sturcture=i,grad_nnp=grad_nnp[0],shape_parametized=shape_parametized,ansatz_parameters=nnp)\n",
    "        gs = nmf_gradient(structures=stp,oh=i,num_layer=num_layer,size_pool=stp.shape[1])\n",
    "        #print(infd,grad_nnp)\n",
    "        loss_value.append(infd)\n",
    "        grad_nnps.append(ms.Tensor(grad_nnp_zeroslike,dtype=ms.float64))\n",
    "        grad_stps.append(gs)\n",
    "\n",
    "      \n",
    "    infd = ops.stack(loss_value)\n",
    "    gnnp = ops.addn(grad_nnps)\n",
    "    gstp = [(infd[i] - avcost2) * grad_stps[i] for i in range(infd.shape[0])]\n",
    "    gstp_averge = ops.addn(gstp) / infd.shape[0]\n",
    "    avcost1 = sum(infd) / infd.shape[0]\n",
    "    # print(f'loss={infd}\\ngrad_nnp={gnnp}\\ngrandient_stp={gstp_averge}')\n",
    "    \n",
    "    gnnp_tf = tf.convert_to_tensor(gnnp.asnumpy(),dtype=tf.float64)\n",
    "    nnp_tf = tf.convert_to_tensor(nnp,dtype=tf.float64)\n",
    "    gstp_averge_tf = tf.convert_to_tensor(gstp_averge.reshape(stp.shape).asnumpy(),dtype=tf.float64)\n",
    "    stp_tf = tf.convert_to_tensor(stp,dtype=tf.float64)\n",
    "     # 更新参数\n",
    "    nnp_tf = network_opt.update(gnnp_tf, nnp_tf)\n",
    "    stp_tf = structure_opt.update(gstp_averge_tf, stp_tf) \n",
    "    \n",
    "    nnp = nnp_tf.numpy()\n",
    "    stp = stp_tf.numpy()\n",
    "\n",
    "    batch_loss_history.append(avcost1)\n",
    "    structure_distribution_history.append(stp)\n",
    "    ansatz_params_history.append(nnp)\n",
    "    #best_candidates_history.append(best_from_structure(cand_preset.asnumpy()))\n",
    "    cand_preset = best_from_structure(stp)\n",
    "    best_candidates_history.append(cand_preset.asnumpy())\n",
    "    \n",
    "\n",
    "    if epoch % 1 == 0 or epoch == epochs - 1:\n",
    "        print(\"----------epoch %s-----------\" % epoch)\n",
    "        print(\n",
    "            \"batched平均损失: \",\n",
    "            avcost1,\n",
    "        )\n",
    "    \n",
    "        if verbose:\n",
    "            print(\n",
    "                \"strcuture parameter: \\n\",\n",
    "                stp,\n",
    "                \"\\n network parameter: \\n\",\n",
    "                nnp,\n",
    "            )\n",
    "        \n",
    "        print(\"最好的候选结构:\",cand_preset)\n",
    "        stp_for_test = ops_onehot(ms.Tensor(cand_preset),shape_parametized+shape_unparameterized,ms.Tensor(1),ms.Tensor(0))\n",
    "\n",
    "        \n",
    "        if cand_preset.min() <shape_parametized:\n",
    "            ansatz_parameters = nnp_dealwith(Structure_params=stp_for_test,Network_params=nnp)\n",
    "            test_ansatz = Mindspore_ansatz_micro_minipool(Structure_p=stp_for_test,\n",
    "                                            parameterized_pool=parameterized_circuit,unparameterized_pool=unparameterized_circuit,\n",
    "                                            num_layer=num_layer,\n",
    "                                            n_qbits=8)\n",
    "            acc = DQAS_accuracy(ansatz=test_ansatz,Network_params=ansatz_parameters,n_qbits=8)\n",
    "            acc_history.append(acc)\n",
    "            print(f'二分类准确率 Acc ={acc*100}% ')\n",
    "        \n",
    "        #我想每一轮结束 保存batch_loss_history、structure_distribution_history、ansatz_params_history、best_candidates_history、acc_history\n",
    "                # 保存数据\n",
    "        with open('training_history-minipool-k4.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'batch_loss_history': batch_loss_history,\n",
    "                'structure_distribution_history': structure_distribution_history,\n",
    "                'ansatz_params_history': ansatz_params_history,\n",
    "                'best_candidates_history': best_candidates_history,\n",
    "                'acc_history': acc_history\n",
    "            }, f)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 DQAS 在量子结构搜索的过程中进行结构、参数的双优化.  \n",
    "我们可以在结构基本不再变动时停止搜索过程转为单优化 效率更高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
