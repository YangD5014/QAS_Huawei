{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindquantum/simulator/__init__.py:17: UserWarning: Unable import mqvector gpu backend due to: cannot import name '_mq_vector_gpu' from partially initialized module 'mindquantum' (most likely due to a circular import) (/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindquantum/__init__.py)\n",
      "  from .available_simulator import SUPPORTED_SIMULATOR\n",
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mindquantum.algorithm.nisq import HardwareEfficientAnsatz,RYFull\n",
    "from mindquantum.core.parameterresolver import  PRGenerator\n",
    "import numpy as np\n",
    "from mindquantum.core.gates import RX, RY, RZ, H, X, Y, Z, I,CNOT\n",
    "from mindquantum.core.circuit import Circuit,UN\n",
    "import mindspore as ms\n",
    "import pickle\n",
    "from mindquantum.core.parameterresolver import PRGenerator\n",
    "import random\n",
    "from mindspore import Tensor,ops\n",
    "import tensorcircuit as tc\n",
    "import tensorflow as tf\n",
    "import mindspore.numpy as mnp\n",
    "from DQAS_tool import wash_pr,Mindspore_ansatz_micro,best_from_structure\n",
    "from DQAS_tool import  sampling_from_structure,zeroslike_grad_nnp_micro_minipool,nmf_gradient,vag_nnp_micro,DQAS_accuracy,Washing_namemap\n",
    "import sys\n",
    "from typing import Union\n",
    "sys.path.append('..')\n",
    "from Test_tool import Test_ansatz\n",
    "from data_processing import X_train,X_test,y_train,y_test\n",
    "from mindquantum.core.circuit import change_param_name,apply\n",
    "\n",
    "pr_pool = PRGenerator('pool')\n",
    "parameterized_circuit= \\\n",
    "[\n",
    " UN(RZ(pr_pool.new()),maps_obj=[0])+\\\n",
    " UN(RY(pr_pool.new()),maps_obj=[0])+\\\n",
    " UN(RZ(pr_pool.new()),maps_obj=[0])+I.on(1),\n",
    " UN(RZ(pr_pool.new()),maps_obj=[1])+\\\n",
    " UN(RY(pr_pool.new()),maps_obj=[1])+\\\n",
    " UN(RZ(pr_pool.new()),maps_obj=[1])+I.on(0),]\n",
    "\n",
    "\n",
    "unparameterized_circuit = \\\n",
    "[UN(X,maps_obj=[0],maps_ctrl=[1]),\n",
    " UN(X,maps_obj=[1],maps_ctrl=[0]),\n",
    " ]\n",
    "ansatz_pr = PRGenerator('ansatz')\n",
    "shape_parametized = len(parameterized_circuit)\n",
    "shape_unparameterized = len(unparameterized_circuit)\n",
    "num_layer=4\n",
    "shape_nnp = (7,num_layer,shape_parametized,3)\n",
    "shape_stp = (num_layer,shape_unparameterized+shape_parametized)\n",
    "stddev = 0.03\n",
    "np.random.seed(2)\n",
    "nnp = np.random.normal(loc=0.0, scale=stddev, size=shape_nnp)\n",
    "stp = np.random.normal(loc=0.0, scale=stddev, size=shape_stp)\n",
    "ops_onehot = ops.OneHot(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[4], dtype=Int32, value= [2, 0, 0, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_from_structure(stp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------epoch 0-----------\n",
      "batched平均损失:  0.8067878\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 1-----------\n",
      "batched平均损失:  0.82131666\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 2-----------\n",
      "batched平均损失:  0.8372654\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 3-----------\n",
      "batched平均损失:  0.78644365\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 4-----------\n",
      "batched平均损失:  0.77053946\n",
      "最好的候选结构: [2 3 3 3]\n",
      "----------epoch 5-----------\n",
      "batched平均损失:  0.75701445\n",
      "最好的候选结构: [2 3 3 3]\n",
      "----------epoch 6-----------\n",
      "batched平均损失:  0.72529596\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 7-----------\n",
      "batched平均损失:  0.7474107\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 8-----------\n",
      "batched平均损失:  0.7423351\n",
      "最好的候选结构: [2 2 3 3]\n",
      "----------epoch 9-----------\n",
      "batched平均损失:  0.69960845\n",
      "最好的候选结构: [2 3 3 3]\n",
      "----------epoch 10-----------\n",
      "batched平均损失:  0.6944242\n",
      "最好的候选结构: [2 3 3 3]\n",
      "----------epoch 11-----------\n",
      "batched平均损失:  0.69341713\n",
      "最好的候选结构: [2 3 3 3]\n",
      "----------epoch 12-----------\n",
      "batched平均损失:  0.6781132\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =43.613707165109034% \n",
      "----------epoch 13-----------\n",
      "batched平均损失:  0.6426016\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =44.44444444444444% \n",
      "----------epoch 14-----------\n",
      "batched平均损失:  0.6436561\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =45.17133956386293% \n",
      "----------epoch 15-----------\n",
      "batched平均损失:  0.62656176\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =45.067497403946% \n",
      "----------epoch 16-----------\n",
      "batched平均损失:  0.59403586\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =45.482866043613704% \n",
      "----------epoch 17-----------\n",
      "batched平均损失:  0.60813624\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =46.10591900311526% \n",
      "----------epoch 18-----------\n",
      "batched平均损失:  0.5944304\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =46.41744548286604% \n",
      "----------epoch 19-----------\n",
      "batched平均损失:  0.5905344\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =47.0404984423676% \n",
      "----------epoch 20-----------\n",
      "batched平均损失:  0.58475477\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =47.87123572170301% \n",
      "----------epoch 21-----------\n",
      "batched平均损失:  0.5665396\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.39044652128764% \n",
      "----------epoch 22-----------\n",
      "batched平均损失:  0.5806726\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.39044652128764% \n",
      "----------epoch 23-----------\n",
      "batched平均损失:  0.5779985\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.28660436137071% \n",
      "----------epoch 24-----------\n",
      "batched平均损失:  0.59424436\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.28660436137071% \n",
      "----------epoch 25-----------\n",
      "batched平均损失:  0.55909395\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.39044652128764% \n",
      "----------epoch 26-----------\n",
      "batched平均损失:  0.5643701\n",
      "最好的候选结构: [2 3 3 0]\n",
      "二分类准确率 Acc =48.39044652128764% \n",
      "----------epoch 27-----------\n",
      "batched平均损失:  0.5569771\n",
      "最好的候选结构: [2 3 0 0]\n",
      "二分类准确率 Acc =96.98857736240913% \n",
      "----------epoch 28-----------\n",
      "batched平均损失:  0.53902406\n",
      "最好的候选结构: [2 3 0 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 29-----------\n",
      "batched平均损失:  0.5352089\n",
      "最好的候选结构: [2 3 0 0]\n",
      "二分类准确率 Acc =97.09241952232607% \n",
      "----------epoch 30-----------\n",
      "batched平均损失:  0.52668995\n",
      "最好的候选结构: [2 3 0 0]\n",
      "二分类准确率 Acc =97.09241952232607% \n",
      "----------epoch 31-----------\n",
      "batched平均损失:  0.496559\n",
      "最好的候选结构: [2 3 0 0]\n",
      "二分类准确率 Acc =97.19626168224299% \n",
      "----------epoch 32-----------\n",
      "batched平均损失:  0.5130627\n",
      "最好的候选结构: [2 3 0 0]\n",
      "二分类准确率 Acc =97.30010384215991% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] PYNATIVE(63092,0x202993240,python):2024-11-25-16:42:48.652.233 [mindspore/ccsrc/pipeline/pynative/pynative_execute.cc:60] operator()] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m grad_stps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_structure:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#print(ops.Argmax()(i))          \u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     infd, grad_nnp \u001b[38;5;241m=\u001b[39m \u001b[43mvag_nnp_micro_minipool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStructure_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mAnsatz_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnnp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mparamerterized_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameterized_circuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43munparamerterized_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munparameterized_circuit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mnum_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_qbits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     grad_nnp_zeroslike \u001b[38;5;241m=\u001b[39m zeroslike_grad_nnp_micro_minipool(batch_sturcture\u001b[38;5;241m=\u001b[39mi,grad_nnp\u001b[38;5;241m=\u001b[39mgrad_nnp[\u001b[38;5;241m0\u001b[39m],shape_parametized\u001b[38;5;241m=\u001b[39mshape_parametized,ansatz_parameters\u001b[38;5;241m=\u001b[39mnnp)\n\u001b[1;32m     39\u001b[0m     gs \u001b[38;5;241m=\u001b[39m nmf_gradient(structures\u001b[38;5;241m=\u001b[39mstp,oh\u001b[38;5;241m=\u001b[39mi,num_layer\u001b[38;5;241m=\u001b[39mnum_layer,size_pool\u001b[38;5;241m=\u001b[39mstp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/ops/composite/base.py:625\u001b[0m, in \u001b[0;36m_Grad.__call__.<locals>.after_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_grad\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/common/api.py:121\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.wrapper\u001b[0;34m(*arg, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 121\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_python_data(results)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/ops/composite/base.py:602\u001b[0m, in \u001b[0;36m_Grad.__call__.<locals>.after_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pynative_forward_run(fn, grad_, weights, args, kwargs)\n\u001b[1;32m    601\u001b[0m _pynative_executor\u001b[38;5;241m.\u001b[39mgrad(fn, grad_, weights, grad_position, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 602\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43m_pynative_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m out \u001b[38;5;241m=\u001b[39m _grads_divided_by_device_num_if_recomputation(out)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ids \u001b[38;5;129;01mand\u001b[39;00m out:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/common/api.py:1147\u001b[0m, in \u001b[0;36m_PyNativeExecutor.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;124;03m    PyNative executor run grad graph.\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m    Return:\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03m        The return object after running grad graph.\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/_extends/parse/parser.py:600\u001b[0m, in \u001b[0;36mconvert_to_ms_tensor\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_ms_tensor\u001b[39m(data):\n\u001b[1;32m    601\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert C++ tensor to mindspore tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tensor(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from DQAS_tool import  sampling_from_structure,vag_nnp_micro_minipool,zeroslike_grad_nnp_micro_minipool,nmf_gradient,DQAS_accuracy,Mindspore_ansatz_micro_minipool,nnp_dealwith\n",
    "#from DQAS_tool import  DQASAnsatz_from_result,DQAS_accuracy\n",
    "K = tc.set_backend(\"tensorflow\")\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.06, 100, 0.5)\n",
    "structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.1))\n",
    "network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))\n",
    "verbose = False\n",
    "# 设置超参数\n",
    "epochs = 800\n",
    "batch_size=100\n",
    "avcost1 = 0\n",
    "ops_onehot = ops.OneHot(axis=-1)\n",
    "batch_loss_history=[] # 记录每个epoch的batch_size损失值\n",
    "structure_distribution_history=[] # 记录每个epoch的结构参数\n",
    "ansatz_params_history=[] # 记录每个epoch的网络参数\n",
    "best_candidates_history=[] # 记录每个epoch的最佳候选\n",
    "acc_history = [] #记录每个epoch的准确率\n",
    "\n",
    " \n",
    "for epoch in range(epochs):  # 更新结构参数的迭代\n",
    "    avcost2 = avcost1\n",
    "    costl = []\n",
    "    tmp = np.stack([sampling_from_structure(stp,num_layer,shape_parametized) for _ in range(batch_size)])\n",
    "    batch_structure = ops_onehot(ms.Tensor(tmp),shape_parametized+shape_unparameterized,ms.Tensor(1),ms.Tensor(0))\n",
    "    #print(batch_structure.shape)\n",
    "    # print(tmp,batch_structure)\n",
    "    loss_value = []\n",
    "    grad_nnps = []\n",
    "    grad_stps = []\n",
    "    \n",
    "    for i in batch_structure:\n",
    "        #print(ops.Argmax()(i))          \n",
    "        infd, grad_nnp = vag_nnp_micro_minipool(Structure_params=i,\n",
    "                                    Ansatz_params=nnp,\n",
    "                                    paramerterized_pool=parameterized_circuit,  unparamerterized_pool=unparameterized_circuit,\n",
    "                                    num_layer=num_layer,n_qbits=8)(ms.Tensor(X_train),ms.Tensor(y_train))\n",
    "        \n",
    "        grad_nnp_zeroslike = zeroslike_grad_nnp_micro_minipool(batch_sturcture=i,grad_nnp=grad_nnp[0],shape_parametized=shape_parametized,ansatz_parameters=nnp)\n",
    "        gs = nmf_gradient(structures=stp,oh=i,num_layer=num_layer,size_pool=stp.shape[1])\n",
    "        #print(infd,grad_nnp)\n",
    "        loss_value.append(infd)\n",
    "        grad_nnps.append(ms.Tensor(grad_nnp_zeroslike,dtype=ms.float64))\n",
    "        grad_stps.append(gs)\n",
    "\n",
    "      \n",
    "    infd = ops.stack(loss_value)\n",
    "    gnnp = ops.addn(grad_nnps)\n",
    "    gstp = [(infd[i] - avcost2) * grad_stps[i] for i in range(infd.shape[0])]\n",
    "    gstp_averge = ops.addn(gstp) / infd.shape[0]\n",
    "    avcost1 = sum(infd) / infd.shape[0]\n",
    "    # print(f'loss={infd}\\ngrad_nnp={gnnp}\\ngrandient_stp={gstp_averge}')\n",
    "    \n",
    "    gnnp_tf = tf.convert_to_tensor(gnnp.asnumpy(),dtype=tf.float64)\n",
    "    nnp_tf = tf.convert_to_tensor(nnp,dtype=tf.float64)\n",
    "    gstp_averge_tf = tf.convert_to_tensor(gstp_averge.reshape(stp.shape).asnumpy(),dtype=tf.float64)\n",
    "    stp_tf = tf.convert_to_tensor(stp,dtype=tf.float64)\n",
    "     # 更新参数\n",
    "    nnp_tf = network_opt.update(gnnp_tf, nnp_tf)\n",
    "    stp_tf = structure_opt.update(gstp_averge_tf, stp_tf) \n",
    "    \n",
    "    nnp = nnp_tf.numpy()\n",
    "    stp = stp_tf.numpy()\n",
    "\n",
    "    batch_loss_history.append(avcost1)\n",
    "    structure_distribution_history.append(stp)\n",
    "    ansatz_params_history.append(nnp)\n",
    "    #best_candidates_history.append(best_from_structure(cand_preset.asnumpy()))\n",
    "    cand_preset = best_from_structure(stp)\n",
    "    best_candidates_history.append(cand_preset.asnumpy())\n",
    "    \n",
    "\n",
    "    if epoch % 1 == 0 or epoch == epochs - 1:\n",
    "        print(\"----------epoch %s-----------\" % epoch)\n",
    "        print(\n",
    "            \"batched平均损失: \",\n",
    "            avcost1,\n",
    "        )\n",
    "    \n",
    "        if verbose:\n",
    "            print(\n",
    "                \"strcuture parameter: \\n\",\n",
    "                stp,\n",
    "                \"\\n network parameter: \\n\",\n",
    "                nnp,\n",
    "            )\n",
    "        \n",
    "        print(\"最好的候选结构:\",cand_preset)\n",
    "        stp_for_test = ops_onehot(ms.Tensor(cand_preset),shape_parametized+shape_unparameterized,ms.Tensor(1),ms.Tensor(0))\n",
    "\n",
    "        \n",
    "        if cand_preset.min() <shape_parametized:\n",
    "            ansatz_parameters = nnp_dealwith(Structure_params=stp_for_test,Network_params=nnp)\n",
    "            test_ansatz = Mindspore_ansatz_micro_minipool(Structure_p=stp_for_test,\n",
    "                                            parameterized_pool=parameterized_circuit,unparameterized_pool=unparameterized_circuit,\n",
    "                                            num_layer=num_layer,\n",
    "                                            n_qbits=8)\n",
    "            acc = DQAS_accuracy(ansatz=test_ansatz,Network_params=ansatz_parameters,n_qbits=8)\n",
    "            acc_history.append(acc)\n",
    "            print(f'二分类准确率 Acc ={acc*100}% ')\n",
    "        \n",
    "        #我想每一轮结束 保存batch_loss_history、structure_distribution_history、ansatz_params_history、best_candidates_history、acc_history\n",
    "                # 保存数据\n",
    "        with open('training_history-minipool-k4-1125.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'batch_loss_history': batch_loss_history,\n",
    "                'structure_distribution_history': structure_distribution_history,\n",
    "                'ansatz_params_history': ansatz_params_history,\n",
    "                'best_candidates_history': best_candidates_history,\n",
    "                'acc_history': acc_history\n",
    "            }, f)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_history.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ansatz_params_history'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
